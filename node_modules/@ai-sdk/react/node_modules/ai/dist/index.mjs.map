{"version":3,"sources":["../src/index.ts","../src/generate-text/generate-text.ts","../src/error/no-output-specified-error.ts","../src/logger/log-warnings.ts","../src/model/resolve-model.ts","../src/error/index.ts","../src/error/invalid-argument-error.ts","../src/error/invalid-stream-part-error.ts","../src/error/invalid-tool-input-error.ts","../src/error/no-image-generated-error.ts","../src/error/no-object-generated-error.ts","../src/error/no-output-generated-error.ts","../src/error/no-speech-generated-error.ts","../src/error/no-such-tool-error.ts","../src/error/tool-call-repair-error.ts","../src/error/unsupported-model-version-error.ts","../src/prompt/invalid-data-content-error.ts","../src/prompt/invalid-message-role-error.ts","../src/prompt/message-conversion-error.ts","../src/util/download/download-error.ts","../src/util/retry-error.ts","../src/prompt/convert-to-language-model-prompt.ts","../src/util/detect-media-type.ts","../src/util/download/download.ts","../src/version.ts","../src/util/download/download-function.ts","../src/prompt/data-content.ts","../src/prompt/split-data-url.ts","../src/prompt/prepare-call-settings.ts","../src/prompt/prepare-tools-and-tool-choice.ts","../src/util/is-non-empty-object.ts","../src/prompt/standardize-prompt.ts","../src/prompt/message.ts","../src/types/provider-metadata.ts","../src/types/json-value.ts","../src/prompt/content-part.ts","../src/prompt/wrap-gateway-error.ts","../src/telemetry/assemble-operation-name.ts","../src/telemetry/get-base-telemetry-attributes.ts","../src/telemetry/get-tracer.ts","../src/telemetry/noop-tracer.ts","../src/telemetry/record-span.ts","../src/telemetry/select-telemetry-attributes.ts","../src/telemetry/stringify-for-telemetry.ts","../src/types/usage.ts","../src/util/as-array.ts","../src/util/retry-with-exponential-backoff.ts","../src/util/prepare-retries.ts","../src/generate-text/extract-text-content.ts","../src/generate-text/generated-file.ts","../src/generate-text/parse-tool-call.ts","../src/generate-text/step-result.ts","../src/generate-text/stop-condition.ts","../src/prompt/create-tool-model-output.ts","../src/generate-text/to-response-messages.ts","../src/generate-text/stream-text.ts","../src/util/prepare-headers.ts","../src/text-stream/create-text-stream-response.ts","../src/util/write-to-server-response.ts","../src/text-stream/pipe-text-stream-to-response.ts","../src/ui-message-stream/json-to-sse-transform-stream.ts","../src/ui-message-stream/ui-message-stream-headers.ts","../src/ui-message-stream/create-ui-message-stream-response.ts","../src/ui-message-stream/get-response-ui-message-id.ts","../src/ui/process-ui-message-stream.ts","../src/ui-message-stream/ui-message-chunks.ts","../src/util/merge-objects.ts","../src/util/parse-partial-json.ts","../src/util/fix-json.ts","../src/ui/ui-messages.ts","../src/ui-message-stream/handle-ui-message-stream-finish.ts","../src/ui-message-stream/pipe-ui-message-stream-to-response.ts","../src/util/async-iterable-stream.ts","../src/util/consume-stream.ts","../src/util/create-resolvable-promise.ts","../src/util/create-stitchable-stream.ts","../src/util/now.ts","../src/generate-text/run-tools-transformation.ts","../src/ui/convert-to-model-messages.ts","../src/agent/agent.ts","../src/embed/embed.ts","../src/embed/embed-many.ts","../src/util/split-array.ts","../src/generate-image/generate-image.ts","../src/generate-object/generate-object.ts","../src/generate-text/extract-reasoning-content.ts","../src/generate-object/output-strategy.ts","../src/generate-object/parse-and-validate-object-result.ts","../src/generate-object/validate-object-generation-input.ts","../src/generate-object/stream-object.ts","../src/util/cosine-similarity.ts","../src/util/data-url.ts","../src/util/is-deep-equal-data.ts","../src/util/serial-job-executor.ts","../src/util/simulate-readable-stream.ts","../src/generate-speech/generate-speech.ts","../src/generate-speech/generated-audio-file.ts","../src/generate-text/output.ts","../src/generate-text/prune-messages.ts","../src/generate-text/smooth-stream.ts","../src/middleware/default-settings-middleware.ts","../src/util/get-potential-start-index.ts","../src/middleware/extract-reasoning-middleware.ts","../src/middleware/simulate-streaming-middleware.ts","../src/middleware/wrap-language-model.ts","../src/middleware/wrap-provider.ts","../src/registry/custom-provider.ts","../src/registry/no-such-provider-error.ts","../src/registry/provider-registry.ts","../src/transcribe/transcribe.ts","../src/error/no-transcript-generated-error.ts","../src/ui/call-completion-api.ts","../src/ui/process-text-stream.ts","../src/ui/chat.ts","../src/ui/convert-file-list-to-file-ui-parts.ts","../src/ui/default-chat-transport.ts","../src/ui/http-chat-transport.ts","../src/ui/last-assistant-message-is-complete-with-tool-calls.ts","../src/ui/transform-text-to-ui-message-stream.ts","../src/ui/text-stream-chat-transport.ts","../src/ui/validate-ui-messages.ts","../src/ui-message-stream/create-ui-message-stream.ts","../src/ui-message-stream/read-ui-message-stream.ts"],"sourcesContent":["// re-exports:\nexport { createGateway, gateway, type GatewayModelId } from '@ai-sdk/gateway';\nexport {\n  asSchema,\n  createIdGenerator,\n  dynamicTool,\n  generateId,\n  jsonSchema,\n  parseJsonEventStream,\n  tool,\n  zodSchema,\n  type IdGenerator,\n  type InferToolInput,\n  type InferToolOutput,\n  type Schema,\n  type Tool,\n  type ToolCallOptions,\n  type ToolExecuteFunction,\n} from '@ai-sdk/provider-utils';\n\n// directory exports\nexport * from './agent';\nexport * from './embed';\nexport * from './error';\nexport * from './generate-image';\nexport * from './generate-object';\nexport * from './generate-speech';\nexport * from './generate-text';\nexport * from './logger';\nexport * from './middleware';\nexport * from './prompt';\nexport * from './registry';\nexport * from './text-stream';\nexport * from './transcribe';\nexport * from './types';\nexport * from './ui';\nexport * from './ui-message-stream';\nexport * from './util';\n\n// telemetry types:\nexport type { TelemetrySettings } from './telemetry/telemetry-settings';\n\n// import globals\nimport './global';\n","import {\n  LanguageModelV2,\n  LanguageModelV2Content,\n  LanguageModelV2ToolCall,\n} from '@ai-sdk/provider';\nimport {\n  createIdGenerator,\n  executeTool,\n  getErrorMessage,\n  IdGenerator,\n  ProviderOptions,\n  withUserAgentSuffix,\n} from '@ai-sdk/provider-utils';\nimport { Tracer } from '@opentelemetry/api';\nimport { NoOutputSpecifiedError } from '../error/no-output-specified-error';\nimport { logWarnings } from '../logger/log-warnings';\nimport { resolveLanguageModel } from '../model/resolve-model';\nimport { ModelMessage } from '../prompt';\nimport { CallSettings } from '../prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../prompt/convert-to-language-model-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { prepareToolsAndToolChoice } from '../prompt/prepare-tools-and-tool-choice';\nimport { Prompt } from '../prompt/prompt';\nimport { standardizePrompt } from '../prompt/standardize-prompt';\nimport { wrapGatewayError } from '../prompt/wrap-gateway-error';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordErrorOnSpan, recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { stringifyForTelemetry } from '../telemetry/stringify-for-telemetry';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { LanguageModel, ToolChoice } from '../types';\nimport { addLanguageModelUsage, LanguageModelUsage } from '../types/usage';\nimport { asArray } from '../util/as-array';\nimport { DownloadFunction } from '../util/download/download-function';\nimport { prepareRetries } from '../util/prepare-retries';\nimport { ContentPart } from './content-part';\nimport { extractTextContent } from './extract-text-content';\nimport { GenerateTextResult } from './generate-text-result';\nimport { DefaultGeneratedFile } from './generated-file';\nimport { Output } from './output';\nimport { parseToolCall } from './parse-tool-call';\nimport { PrepareStepFunction } from './prepare-step';\nimport { ResponseMessage } from './response-message';\nimport { DefaultStepResult, StepResult } from './step-result';\nimport {\n  isStopConditionMet,\n  stepCountIs,\n  StopCondition,\n} from './stop-condition';\nimport { toResponseMessages } from './to-response-messages';\nimport { TypedToolCall } from './tool-call';\nimport { ToolCallRepairFunction } from './tool-call-repair-function';\nimport { TypedToolError } from './tool-error';\nimport { ToolOutput } from './tool-output';\nimport { TypedToolResult } from './tool-result';\nimport { ToolSet } from './tool-set';\nimport { VERSION } from '../version';\n\nconst originalGenerateId = createIdGenerator({\n  prefix: 'aitxt',\n  size: 24,\n});\n\n/**\nCallback that is set using the `onStepFinish` option.\n\n@param stepResult - The result of the step.\n */\nexport type GenerateTextOnStepFinishCallback<TOOLS extends ToolSet> = (\n  stepResult: StepResult<TOOLS>,\n) => Promise<void> | void;\n\n/**\nGenerate a text and call tools for a given prompt using a language model.\n\nThis function does not stream the output. If you want to stream the output, use `streamText` instead.\n\n@param model - The language model to use.\n\n@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.\n@param toolChoice - The tool choice strategy. Default: 'auto'.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxOutputTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topK - Only sample from the top K options for each subsequent token.\nUsed to remove \"long tail\" low probability responses.\nRecommended for advanced use cases only. You usually only need to use temperature.\n@param presencePenalty - Presence penalty setting.\nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param stopSequences - Stop sequences.\nIf set, the model will stop generating text when one of the stop sequences is generated.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@param experimental_generateMessageId - Generate a unique ID for each message.\n\n@param onStepFinish - Callback that is called when each step (LLM call) is finished, including intermediate steps.\n\n@returns\nA result object that contains the generated text, the results of the tool calls, and additional information.\n */\nexport async function generateText<\n  TOOLS extends ToolSet,\n  OUTPUT = never,\n  OUTPUT_PARTIAL = never,\n>({\n  model: modelArg,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries: maxRetriesArg,\n  abortSignal,\n  headers,\n  stopWhen = stepCountIs(1),\n  experimental_output: output,\n  experimental_telemetry: telemetry,\n  providerOptions,\n  experimental_activeTools,\n  activeTools = experimental_activeTools,\n  experimental_prepareStep,\n  prepareStep = experimental_prepareStep,\n  experimental_repairToolCall: repairToolCall,\n  experimental_download: download,\n  experimental_context,\n  _internal: {\n    generateId = originalGenerateId,\n    currentDate = () => new Date(),\n  } = {},\n  onStepFinish,\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe tools that the model can call. The model needs to support calling tools.\n*/\n    tools?: TOOLS;\n\n    /**\nThe tool choice strategy. Default: 'auto'.\n     */\n    toolChoice?: ToolChoice<NoInfer<TOOLS>>;\n\n    /**\nCondition for stopping the generation when there are tool results in the last step.\nWhen the condition is an array, any of the conditions can be met to stop the generation.\n\n@default stepCountIs(1)\n     */\n    stopWhen?:\n      | StopCondition<NoInfer<TOOLS>>\n      | Array<StopCondition<NoInfer<TOOLS>>>;\n\n    /**\nOptional telemetry configuration (experimental).\n     */\n    experimental_telemetry?: TelemetrySettings;\n\n    /**\nAdditional provider-specific options. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n    providerOptions?: ProviderOptions;\n\n    /**\n     * @deprecated Use `activeTools` instead.\n     */\n    experimental_activeTools?: Array<keyof NoInfer<TOOLS>>;\n\n    /**\nLimits the tools that are available for the model to call without\nchanging the tool call and result types in the result.\n     */\n    activeTools?: Array<keyof NoInfer<TOOLS>>;\n\n    /**\nOptional specification for parsing structured outputs from the LLM response.\n     */\n    experimental_output?: Output<OUTPUT, OUTPUT_PARTIAL>;\n\n    /**\nCustom download function to use for URLs.\n\nBy default, files are downloaded if the model does not support the URL for the given media type.\n     */\n    experimental_download?: DownloadFunction | undefined;\n\n    /**\n     * @deprecated Use `prepareStep` instead.\n     */\n    experimental_prepareStep?: PrepareStepFunction<NoInfer<TOOLS>>;\n\n    /**\nOptional function that you can use to provide different settings for a step.\n    */\n    prepareStep?: PrepareStepFunction<NoInfer<TOOLS>>;\n\n    /**\nA function that attempts to repair a tool call that failed to parse.\n     */\n    experimental_repairToolCall?: ToolCallRepairFunction<NoInfer<TOOLS>>;\n\n    /**\n    Callback that is called when each step (LLM call) is finished, including intermediate steps.\n    */\n    onStepFinish?: GenerateTextOnStepFinishCallback<NoInfer<TOOLS>>;\n\n    /**\n     * Context that is passed into tool execution.\n     *\n     * Experimental (can break in patch releases).\n     *\n     * @default undefined\n     */\n    experimental_context?: unknown;\n\n    /**\n     * Internal. For test use only. May change without notice.\n     */\n    _internal?: {\n      generateId?: IdGenerator;\n      currentDate?: () => Date;\n    };\n  }): Promise<GenerateTextResult<TOOLS, OUTPUT>> {\n  const model = resolveLanguageModel(modelArg);\n  const stopConditions = asArray(stopWhen);\n  const { maxRetries, retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal,\n  });\n\n  const callSettings = prepareCallSettings(settings);\n\n  const headersWithUserAgent = withUserAgentSuffix(\n    headers ?? {},\n    `ai/${VERSION}`,\n  );\n\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers: headersWithUserAgent,\n    settings: { ...callSettings, maxRetries },\n  });\n\n  const initialPrompt = await standardizePrompt({\n    system,\n    prompt,\n    messages,\n  } as Prompt);\n\n  const tracer = getTracer(telemetry);\n\n  try {\n    return await recordSpan({\n      name: 'ai.generateText',\n      attributes: selectTelemetryAttributes({\n        telemetry,\n        attributes: {\n          ...assembleOperationName({\n            operationId: 'ai.generateText',\n            telemetry,\n          }),\n          ...baseTelemetryAttributes,\n          // model:\n          'ai.model.provider': model.provider,\n          'ai.model.id': model.modelId,\n          // specific settings that only make sense on the outer level:\n          'ai.prompt': {\n            input: () => JSON.stringify({ system, prompt, messages }),\n          },\n        },\n      }),\n      tracer,\n      fn: async span => {\n        const callSettings = prepareCallSettings(settings);\n\n        let currentModelResponse: Awaited<\n          ReturnType<LanguageModelV2['doGenerate']>\n        > & { response: { id: string; timestamp: Date; modelId: string } };\n        let clientToolCalls: Array<TypedToolCall<TOOLS>> = [];\n        let clientToolOutputs: Array<ToolOutput<TOOLS>> = [];\n        const responseMessages: Array<ResponseMessage> = [];\n        const steps: GenerateTextResult<TOOLS, OUTPUT>['steps'] = [];\n\n        do {\n          const stepInputMessages = [\n            ...initialPrompt.messages,\n            ...responseMessages,\n          ];\n\n          const prepareStepResult = await prepareStep?.({\n            model,\n            steps,\n            stepNumber: steps.length,\n            messages: stepInputMessages,\n          });\n\n          const stepModel = resolveLanguageModel(\n            prepareStepResult?.model ?? model,\n          );\n\n          const promptMessages = await convertToLanguageModelPrompt({\n            prompt: {\n              system: prepareStepResult?.system ?? initialPrompt.system,\n              messages: prepareStepResult?.messages ?? stepInputMessages,\n            },\n            supportedUrls: await stepModel.supportedUrls,\n            download,\n          });\n\n          const { toolChoice: stepToolChoice, tools: stepTools } =\n            prepareToolsAndToolChoice({\n              tools,\n              toolChoice: prepareStepResult?.toolChoice ?? toolChoice,\n              activeTools: prepareStepResult?.activeTools ?? activeTools,\n            });\n\n          currentModelResponse = await retry(() =>\n            recordSpan({\n              name: 'ai.generateText.doGenerate',\n              attributes: selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  ...assembleOperationName({\n                    operationId: 'ai.generateText.doGenerate',\n                    telemetry,\n                  }),\n                  ...baseTelemetryAttributes,\n                  // model:\n                  'ai.model.provider': stepModel.provider,\n                  'ai.model.id': stepModel.modelId,\n                  // prompt:\n                  'ai.prompt.messages': {\n                    input: () => stringifyForTelemetry(promptMessages),\n                  },\n                  'ai.prompt.tools': {\n                    // convert the language model level tools:\n                    input: () => stepTools?.map(tool => JSON.stringify(tool)),\n                  },\n                  'ai.prompt.toolChoice': {\n                    input: () =>\n                      stepToolChoice != null\n                        ? JSON.stringify(stepToolChoice)\n                        : undefined,\n                  },\n\n                  // standardized gen-ai llm span attributes:\n                  'gen_ai.system': stepModel.provider,\n                  'gen_ai.request.model': stepModel.modelId,\n                  'gen_ai.request.frequency_penalty': settings.frequencyPenalty,\n                  'gen_ai.request.max_tokens': settings.maxOutputTokens,\n                  'gen_ai.request.presence_penalty': settings.presencePenalty,\n                  'gen_ai.request.stop_sequences': settings.stopSequences,\n                  'gen_ai.request.temperature':\n                    settings.temperature ?? undefined,\n                  'gen_ai.request.top_k': settings.topK,\n                  'gen_ai.request.top_p': settings.topP,\n                },\n              }),\n              tracer,\n              fn: async span => {\n                const result = await stepModel.doGenerate({\n                  ...callSettings,\n                  tools: stepTools,\n                  toolChoice: stepToolChoice,\n                  responseFormat: output?.responseFormat,\n                  prompt: promptMessages,\n                  providerOptions,\n                  abortSignal,\n                  headers: headersWithUserAgent,\n                });\n\n                // Fill in default values:\n                const responseData = {\n                  id: result.response?.id ?? generateId(),\n                  timestamp: result.response?.timestamp ?? currentDate(),\n                  modelId: result.response?.modelId ?? stepModel.modelId,\n                  headers: result.response?.headers,\n                  body: result.response?.body,\n                };\n\n                // Add response information to the span:\n                span.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      'ai.response.finishReason': result.finishReason,\n                      'ai.response.text': {\n                        output: () => extractTextContent(result.content),\n                      },\n                      'ai.response.toolCalls': {\n                        output: () => {\n                          const toolCalls = asToolCalls(result.content);\n                          return toolCalls == null\n                            ? undefined\n                            : JSON.stringify(toolCalls);\n                        },\n                      },\n                      'ai.response.id': responseData.id,\n                      'ai.response.model': responseData.modelId,\n                      'ai.response.timestamp':\n                        responseData.timestamp.toISOString(),\n                      'ai.response.providerMetadata': JSON.stringify(\n                        result.providerMetadata,\n                      ),\n\n                      // TODO rename telemetry attributes to inputTokens and outputTokens\n                      'ai.usage.promptTokens': result.usage.inputTokens,\n                      'ai.usage.completionTokens': result.usage.outputTokens,\n\n                      // standardized gen-ai llm span attributes:\n                      'gen_ai.response.finish_reasons': [result.finishReason],\n                      'gen_ai.response.id': responseData.id,\n                      'gen_ai.response.model': responseData.modelId,\n                      'gen_ai.usage.input_tokens': result.usage.inputTokens,\n                      'gen_ai.usage.output_tokens': result.usage.outputTokens,\n                    },\n                  }),\n                );\n\n                return { ...result, response: responseData };\n              },\n            }),\n          );\n\n          // parse tool calls:\n          const stepToolCalls: TypedToolCall<TOOLS>[] = await Promise.all(\n            currentModelResponse.content\n              .filter(\n                (part): part is LanguageModelV2ToolCall =>\n                  part.type === 'tool-call',\n              )\n              .map(toolCall =>\n                parseToolCall({\n                  toolCall,\n                  tools,\n                  repairToolCall,\n                  system,\n                  messages: stepInputMessages,\n                }),\n              ),\n          );\n\n          // notify the tools that the tool calls are available:\n          for (const toolCall of stepToolCalls) {\n            if (toolCall.invalid) {\n              continue; // ignore invalid tool calls\n            }\n\n            const tool = tools![toolCall.toolName];\n            if (tool?.onInputAvailable != null) {\n              await tool.onInputAvailable({\n                input: toolCall.input,\n                toolCallId: toolCall.toolCallId,\n                messages: stepInputMessages,\n                abortSignal,\n                experimental_context,\n              });\n            }\n          }\n\n          // insert error tool outputs for invalid tool calls:\n          // TODO AI SDK 6: invalid inputs should not require output parts\n          const invalidToolCalls = stepToolCalls.filter(\n            toolCall => toolCall.invalid && toolCall.dynamic,\n          );\n\n          clientToolOutputs = [];\n\n          for (const toolCall of invalidToolCalls) {\n            clientToolOutputs.push({\n              type: 'tool-error',\n              toolCallId: toolCall.toolCallId,\n              toolName: toolCall.toolName,\n              input: toolCall.input,\n              error: getErrorMessage(toolCall.error!),\n              dynamic: true,\n            });\n          }\n\n          // execute client tool calls:\n          clientToolCalls = stepToolCalls.filter(\n            toolCall => !toolCall.providerExecuted,\n          );\n\n          if (tools != null) {\n            clientToolOutputs.push(\n              ...(await executeTools({\n                toolCalls: clientToolCalls.filter(\n                  toolCall => !toolCall.invalid,\n                ),\n                tools,\n                tracer,\n                telemetry,\n                messages: stepInputMessages,\n                abortSignal,\n                experimental_context,\n              })),\n            );\n          }\n\n          // content:\n          const stepContent = asContent({\n            content: currentModelResponse.content,\n            toolCalls: stepToolCalls,\n            toolOutputs: clientToolOutputs,\n          });\n\n          // append to messages for potential next step:\n          responseMessages.push(\n            ...toResponseMessages({\n              content: stepContent,\n              tools,\n            }),\n          );\n\n          // Add step information (after response messages are updated):\n          const currentStepResult: StepResult<TOOLS> = new DefaultStepResult({\n            content: stepContent,\n            finishReason: currentModelResponse.finishReason,\n            usage: currentModelResponse.usage,\n            warnings: currentModelResponse.warnings,\n            providerMetadata: currentModelResponse.providerMetadata,\n            request: currentModelResponse.request ?? {},\n            response: {\n              ...currentModelResponse.response,\n              // deep clone msgs to avoid mutating past messages in multi-step:\n              messages: structuredClone(responseMessages),\n            },\n          });\n\n          logWarnings(currentModelResponse.warnings ?? []);\n\n          steps.push(currentStepResult);\n          await onStepFinish?.(currentStepResult);\n        } while (\n          // there are tool calls:\n          clientToolCalls.length > 0 &&\n          // all current tool calls have outputs (incl. execution errors):\n          clientToolOutputs.length === clientToolCalls.length &&\n          // continue until a stop condition is met:\n          !(await isStopConditionMet({ stopConditions, steps }))\n        );\n\n        // Add response information to the span:\n        span.setAttributes(\n          selectTelemetryAttributes({\n            telemetry,\n            attributes: {\n              'ai.response.finishReason': currentModelResponse.finishReason,\n              'ai.response.text': {\n                output: () => extractTextContent(currentModelResponse.content),\n              },\n              'ai.response.toolCalls': {\n                output: () => {\n                  const toolCalls = asToolCalls(currentModelResponse.content);\n                  return toolCalls == null\n                    ? undefined\n                    : JSON.stringify(toolCalls);\n                },\n              },\n              'ai.response.providerMetadata': JSON.stringify(\n                currentModelResponse.providerMetadata,\n              ),\n\n              // TODO rename telemetry attributes to inputTokens and outputTokens\n              'ai.usage.promptTokens': currentModelResponse.usage.inputTokens,\n              'ai.usage.completionTokens':\n                currentModelResponse.usage.outputTokens,\n            },\n          }),\n        );\n\n        const lastStep = steps[steps.length - 1];\n\n        // parse output only if the last step was finished with \"stop\":\n        let resolvedOutput;\n        if (lastStep.finishReason === 'stop') {\n          resolvedOutput = await output?.parseOutput(\n            { text: lastStep.text },\n            {\n              response: lastStep.response,\n              usage: lastStep.usage,\n              finishReason: lastStep.finishReason,\n            },\n          );\n        }\n\n        return new DefaultGenerateTextResult({\n          steps,\n          resolvedOutput,\n        });\n      },\n    });\n  } catch (error) {\n    throw wrapGatewayError(error);\n  }\n}\n\nasync function executeTools<TOOLS extends ToolSet>({\n  toolCalls,\n  tools,\n  tracer,\n  telemetry,\n  messages,\n  abortSignal,\n  experimental_context,\n}: {\n  toolCalls: Array<TypedToolCall<TOOLS>>;\n  tools: TOOLS;\n  tracer: Tracer;\n  telemetry: TelemetrySettings | undefined;\n  messages: ModelMessage[];\n  abortSignal: AbortSignal | undefined;\n  experimental_context: unknown;\n}): Promise<Array<ToolOutput<TOOLS>>> {\n  const toolOutputs = await Promise.all(\n    toolCalls.map(async ({ toolCallId, toolName, input }) => {\n      const tool = tools[toolName];\n\n      if (tool?.execute == null) {\n        return undefined;\n      }\n\n      return recordSpan({\n        name: 'ai.toolCall',\n        attributes: selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            ...assembleOperationName({\n              operationId: 'ai.toolCall',\n              telemetry,\n            }),\n            'ai.toolCall.name': toolName,\n            'ai.toolCall.id': toolCallId,\n            'ai.toolCall.args': {\n              output: () => JSON.stringify(input),\n            },\n          },\n        }),\n        tracer,\n        fn: async span => {\n          try {\n            const stream = executeTool({\n              execute: tool.execute!.bind(tool),\n              input,\n              options: {\n                toolCallId,\n                messages,\n                abortSignal,\n                experimental_context,\n              },\n            });\n\n            let output: unknown;\n            for await (const part of stream) {\n              if (part.type === 'final') {\n                output = part.output;\n              }\n            }\n            try {\n              span.setAttributes(\n                selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    'ai.toolCall.result': {\n                      output: () => JSON.stringify(output),\n                    },\n                  },\n                }),\n              );\n            } catch (ignored) {\n              // JSON stringify might fail if the result is not serializable,\n              // in which case we just ignore it. In the future we might want to\n              // add an optional serialize method to the tool interface and warn\n              // if the result is not serializable.\n            }\n\n            return {\n              type: 'tool-result',\n              toolCallId,\n              toolName,\n              input,\n              output,\n              dynamic: tool.type === 'dynamic',\n            } as TypedToolResult<TOOLS>;\n          } catch (error) {\n            recordErrorOnSpan(span, error);\n            return {\n              type: 'tool-error',\n              toolCallId,\n              toolName,\n              input,\n              error,\n              dynamic: tool.type === 'dynamic',\n            } as TypedToolError<TOOLS>;\n          }\n        },\n      });\n    }),\n  );\n\n  return toolOutputs.filter(\n    (output): output is NonNullable<typeof output> => output != null,\n  );\n}\n\nclass DefaultGenerateTextResult<TOOLS extends ToolSet, OUTPUT>\n  implements GenerateTextResult<TOOLS, OUTPUT>\n{\n  readonly steps: GenerateTextResult<TOOLS, OUTPUT>['steps'];\n\n  private readonly resolvedOutput: OUTPUT;\n\n  constructor(options: {\n    steps: GenerateTextResult<TOOLS, OUTPUT>['steps'];\n    resolvedOutput: OUTPUT;\n  }) {\n    this.steps = options.steps;\n    this.resolvedOutput = options.resolvedOutput;\n  }\n\n  private get finalStep() {\n    return this.steps[this.steps.length - 1];\n  }\n\n  get content() {\n    return this.finalStep.content;\n  }\n\n  get text() {\n    return this.finalStep.text;\n  }\n\n  get files() {\n    return this.finalStep.files;\n  }\n\n  get reasoningText() {\n    return this.finalStep.reasoningText;\n  }\n\n  get reasoning() {\n    return this.finalStep.reasoning;\n  }\n\n  get toolCalls() {\n    return this.finalStep.toolCalls;\n  }\n\n  get staticToolCalls() {\n    return this.finalStep.staticToolCalls;\n  }\n\n  get dynamicToolCalls() {\n    return this.finalStep.dynamicToolCalls;\n  }\n\n  get toolResults() {\n    return this.finalStep.toolResults;\n  }\n\n  get staticToolResults() {\n    return this.finalStep.staticToolResults;\n  }\n\n  get dynamicToolResults() {\n    return this.finalStep.dynamicToolResults;\n  }\n\n  get sources() {\n    return this.finalStep.sources;\n  }\n\n  get finishReason() {\n    return this.finalStep.finishReason;\n  }\n\n  get warnings() {\n    return this.finalStep.warnings;\n  }\n\n  get providerMetadata() {\n    return this.finalStep.providerMetadata;\n  }\n\n  get response() {\n    return this.finalStep.response;\n  }\n\n  get request() {\n    return this.finalStep.request;\n  }\n\n  get usage() {\n    return this.finalStep.usage;\n  }\n\n  get totalUsage() {\n    return this.steps.reduce(\n      (totalUsage, step) => {\n        return addLanguageModelUsage(totalUsage, step.usage);\n      },\n      {\n        inputTokens: undefined,\n        outputTokens: undefined,\n        totalTokens: undefined,\n        reasoningTokens: undefined,\n        cachedInputTokens: undefined,\n      } as LanguageModelUsage,\n    );\n  }\n\n  get experimental_output() {\n    if (this.resolvedOutput == null) {\n      throw new NoOutputSpecifiedError();\n    }\n\n    return this.resolvedOutput;\n  }\n}\n\nfunction asToolCalls(content: Array<LanguageModelV2Content>) {\n  const parts = content.filter(\n    (part): part is LanguageModelV2ToolCall => part.type === 'tool-call',\n  );\n\n  if (parts.length === 0) {\n    return undefined;\n  }\n\n  return parts.map(toolCall => ({\n    toolCallId: toolCall.toolCallId,\n    toolName: toolCall.toolName,\n    input: toolCall.input,\n  }));\n}\n\nfunction asContent<TOOLS extends ToolSet>({\n  content,\n  toolCalls,\n  toolOutputs,\n}: {\n  content: Array<LanguageModelV2Content>;\n  toolCalls: Array<TypedToolCall<TOOLS>>;\n  toolOutputs: Array<ToolOutput<TOOLS>>;\n}): Array<ContentPart<TOOLS>> {\n  return [\n    ...content.map(part => {\n      switch (part.type) {\n        case 'text':\n        case 'reasoning':\n        case 'source':\n          return part;\n\n        case 'file': {\n          return {\n            type: 'file' as const,\n            file: new DefaultGeneratedFile(part),\n          };\n        }\n\n        case 'tool-call': {\n          return toolCalls.find(\n            toolCall => toolCall.toolCallId === part.toolCallId,\n          )!;\n        }\n\n        case 'tool-result': {\n          const toolCall = toolCalls.find(\n            toolCall => toolCall.toolCallId === part.toolCallId,\n          )!;\n\n          if (toolCall == null) {\n            throw new Error(`Tool call ${part.toolCallId} not found.`);\n          }\n\n          if (part.isError) {\n            return {\n              type: 'tool-error' as const,\n              toolCallId: part.toolCallId,\n              toolName: part.toolName as keyof TOOLS & string,\n              input: toolCall.input,\n              error: part.result,\n              providerExecuted: true,\n              dynamic: toolCall.dynamic,\n            } as TypedToolError<TOOLS>;\n          }\n\n          return {\n            type: 'tool-result' as const,\n            toolCallId: part.toolCallId,\n            toolName: part.toolName as keyof TOOLS & string,\n            input: toolCall.input,\n            output: part.result,\n            providerExecuted: true,\n            dynamic: toolCall.dynamic,\n          } as TypedToolResult<TOOLS>;\n        }\n      }\n    }),\n    ...toolOutputs,\n  ];\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_NoOutputSpecifiedError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\n/**\nThrown when no output type is specified and output-related methods are called.\n */\nexport class NoOutputSpecifiedError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  constructor({ message = 'No output specified.' }: { message?: string } = {}) {\n    super({ name, message });\n  }\n\n  static isInstance(error: unknown): error is NoOutputSpecifiedError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import {\n  ImageModelV2CallWarning,\n  LanguageModelV2CallWarning,\n  SpeechModelV2CallWarning,\n  TranscriptionModelV2CallWarning,\n} from '@ai-sdk/provider';\n\nexport type Warning =\n  | LanguageModelV2CallWarning\n  | ImageModelV2CallWarning\n  | SpeechModelV2CallWarning\n  | TranscriptionModelV2CallWarning;\n\nexport type LogWarningsFunction = (warnings: Warning[]) => void;\n\n/**\n * Formats a warning object into a human-readable string with clear AI SDK branding\n */\nfunction formatWarning(warning: Warning): string {\n  const prefix = 'AI SDK Warning:';\n\n  switch (warning.type) {\n    case 'unsupported-setting': {\n      let message = `${prefix} The \"${warning.setting}\" setting is not supported by this model`;\n      if (warning.details) {\n        message += ` - ${warning.details}`;\n      }\n      return message;\n    }\n\n    case 'unsupported-tool': {\n      const toolName =\n        'name' in warning.tool ? warning.tool.name : 'unknown tool';\n      let message = `${prefix} The tool \"${toolName}\" is not supported by this model`;\n      if (warning.details) {\n        message += ` - ${warning.details}`;\n      }\n      return message;\n    }\n\n    case 'other': {\n      return `${prefix} ${warning.message}`;\n    }\n\n    default: {\n      // Fallback for any unknown warning types\n      return `${prefix} ${JSON.stringify(warning, null, 2)}`;\n    }\n  }\n}\n\nexport const FIRST_WARNING_INFO_MESSAGE =\n  'AI SDK Warning System: To turn off warning logging, set the AI_SDK_LOG_WARNINGS global to false.';\n\nlet hasLoggedBefore = false;\n\nexport const logWarnings: LogWarningsFunction = warnings => {\n  // if the warnings array is empty, do nothing\n  if (warnings.length === 0) {\n    return;\n  }\n\n  const logger = globalThis.AI_SDK_LOG_WARNINGS;\n\n  // if the logger is set to false, do nothing\n  if (logger === false) {\n    return;\n  }\n\n  // use the provided logger if it is a function\n  if (typeof logger === 'function') {\n    logger(warnings);\n    return;\n  }\n\n  // display information note on first call\n  if (!hasLoggedBefore) {\n    hasLoggedBefore = true;\n    console.info(FIRST_WARNING_INFO_MESSAGE);\n  }\n\n  // default behavior: log warnings to the console\n  for (const warning of warnings) {\n    console.warn(formatWarning(warning));\n  }\n};\n\n// Reset function for testing purposes\nexport const resetLogWarningsState = () => {\n  hasLoggedBefore = false;\n};\n","import { gateway } from '@ai-sdk/gateway';\nimport {\n  EmbeddingModelV2,\n  ImageModelV2,\n  LanguageModelV2,\n  ProviderV2,\n} from '@ai-sdk/provider';\nimport { UnsupportedModelVersionError } from '../error';\nimport { EmbeddingModel } from '../types/embedding-model';\nimport { LanguageModel } from '../types/language-model';\nimport { ImageModel } from '../types/image-model';\n\nexport function resolveLanguageModel(model: LanguageModel): LanguageModelV2 {\n  if (typeof model !== 'string') {\n    if (model.specificationVersion !== 'v2') {\n      throw new UnsupportedModelVersionError({\n        version: model.specificationVersion,\n        provider: model.provider,\n        modelId: model.modelId,\n      });\n    }\n\n    return model;\n  }\n\n  return getGlobalProvider().languageModel(model);\n}\n\nexport function resolveEmbeddingModel<VALUE = string>(\n  model: EmbeddingModel<VALUE>,\n): EmbeddingModelV2<VALUE> {\n  if (typeof model !== 'string') {\n    if (model.specificationVersion !== 'v2') {\n      throw new UnsupportedModelVersionError({\n        version: model.specificationVersion,\n        provider: model.provider,\n        modelId: model.modelId,\n      });\n    }\n\n    return model;\n  }\n\n  // TODO AI SDK 6: figure out how to cleanly support different generic types\n  return getGlobalProvider().textEmbeddingModel(\n    model,\n  ) as EmbeddingModelV2<VALUE>;\n}\n\nexport function resolveImageModel(model: ImageModel): ImageModelV2 {\n  if (typeof model !== 'string') {\n    if (model.specificationVersion !== 'v2') {\n      throw new UnsupportedModelVersionError({\n        version: model.specificationVersion,\n        provider: model.provider,\n        modelId: model.modelId,\n      });\n    }\n\n    return model;\n  }\n\n  return getGlobalProvider().imageModel(model);\n}\n\nfunction getGlobalProvider(): ProviderV2 {\n  return globalThis.AI_SDK_DEFAULT_PROVIDER ?? gateway;\n}\n","export {\n  AISDKError,\n  APICallError,\n  EmptyResponseBodyError,\n  InvalidPromptError,\n  InvalidResponseDataError,\n  JSONParseError,\n  LoadAPIKeyError,\n  LoadSettingError,\n  NoContentGeneratedError,\n  NoSuchModelError,\n  TooManyEmbeddingValuesForCallError,\n  TypeValidationError,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport { InvalidArgumentError } from './invalid-argument-error';\nexport { InvalidStreamPartError } from './invalid-stream-part-error';\nexport { InvalidToolInputError } from './invalid-tool-input-error';\nexport { NoImageGeneratedError } from './no-image-generated-error';\nexport { NoObjectGeneratedError } from './no-object-generated-error';\nexport { NoOutputGeneratedError } from './no-output-generated-error';\nexport { NoOutputSpecifiedError } from './no-output-specified-error';\nexport { NoSpeechGeneratedError } from './no-speech-generated-error';\nexport { NoSuchToolError } from './no-such-tool-error';\nexport { ToolCallRepairError } from './tool-call-repair-error';\nexport { UnsupportedModelVersionError } from './unsupported-model-version-error';\n\nexport { InvalidDataContentError } from '../prompt/invalid-data-content-error';\nexport { InvalidMessageRoleError } from '../prompt/invalid-message-role-error';\nexport { MessageConversionError } from '../prompt/message-conversion-error';\nexport { DownloadError } from '../util/download/download-error';\nexport { RetryError } from '../util/retry-error';\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidArgumentError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidArgumentError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly parameter: string;\n  readonly value: unknown;\n\n  constructor({\n    parameter,\n    value,\n    message,\n  }: {\n    parameter: string;\n    value: unknown;\n    message: string;\n  }) {\n    super({\n      name,\n      message: `Invalid argument for parameter ${parameter}: ${message}`,\n    });\n\n    this.parameter = parameter;\n    this.value = value;\n  }\n\n  static isInstance(error: unknown): error is InvalidArgumentError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\nimport { SingleRequestTextStreamPart } from '../generate-text/run-tools-transformation';\n\nconst name = 'AI_InvalidStreamPartError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidStreamPartError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly chunk: SingleRequestTextStreamPart<any>;\n\n  constructor({\n    chunk,\n    message,\n  }: {\n    chunk: SingleRequestTextStreamPart<any>;\n    message: string;\n  }) {\n    super({ name, message });\n\n    this.chunk = chunk;\n  }\n\n  static isInstance(error: unknown): error is InvalidStreamPartError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError, getErrorMessage } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidToolInputError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidToolInputError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly toolName: string;\n  readonly toolInput: string;\n\n  constructor({\n    toolInput,\n    toolName,\n    cause,\n    message = `Invalid input for tool ${toolName}: ${getErrorMessage(cause)}`,\n  }: {\n    message?: string;\n    toolInput: string;\n    toolName: string;\n    cause: unknown;\n  }) {\n    super({ name, message, cause });\n\n    this.toolInput = toolInput;\n    this.toolName = toolName;\n  }\n\n  static isInstance(error: unknown): error is InvalidToolInputError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\nimport { ImageModelResponseMetadata } from '../types/image-model-response-metadata';\n\nconst name = 'AI_NoImageGeneratedError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\n/**\nThrown when no image could be generated. This can have multiple causes:\n\n- The model failed to generate a response.\n- The model generated a response that could not be parsed.\n */\nexport class NoImageGeneratedError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  /**\nThe response metadata for each call.\n   */\n  readonly responses: Array<ImageModelResponseMetadata> | undefined;\n\n  constructor({\n    message = 'No image generated.',\n    cause,\n    responses,\n  }: {\n    message?: string;\n    cause?: Error;\n    responses?: Array<ImageModelResponseMetadata>;\n  }) {\n    super({ name, message, cause });\n\n    this.responses = responses;\n  }\n\n  static isInstance(error: unknown): error is NoImageGeneratedError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\nimport { FinishReason } from '../types/language-model';\nimport { LanguageModelResponseMetadata } from '../types/language-model-response-metadata';\nimport { LanguageModelUsage } from '../types/usage';\n\nconst name = 'AI_NoObjectGeneratedError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\n/**\nThrown when no object could be generated. This can have several causes:\n\n- The model failed to generate a response.\n- The model generated a response that could not be parsed.\n- The model generated a response that could not be validated against the schema.\n\nThe error contains the following properties:\n\n- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the model.\n */\nexport class NoObjectGeneratedError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  /**\n  The text that was generated by the model. This can be the raw text or the tool call text, depending on the model.\n   */\n  readonly text: string | undefined;\n\n  /**\n  The response metadata.\n   */\n  readonly response: LanguageModelResponseMetadata | undefined;\n\n  /**\n  The usage of the model.\n   */\n  readonly usage: LanguageModelUsage | undefined;\n\n  /**\n  Reason why the model finished generating a response.\n   */\n  readonly finishReason: FinishReason | undefined;\n\n  constructor({\n    message = 'No object generated.',\n    cause,\n    text,\n    response,\n    usage,\n    finishReason,\n  }: {\n    message?: string;\n    cause?: Error;\n    text?: string;\n    response: LanguageModelResponseMetadata;\n    usage: LanguageModelUsage;\n    finishReason: FinishReason;\n  }) {\n    super({ name, message, cause });\n\n    this.text = text;\n    this.response = response;\n    this.usage = usage;\n    this.finishReason = finishReason;\n  }\n\n  static isInstance(error: unknown): error is NoObjectGeneratedError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_NoOutputGeneratedError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\n/**\nThrown when no LLM output was generated, e.g. because of errors.\n */\nexport class NoOutputGeneratedError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  constructor({\n    message = 'No output generated.',\n    cause,\n  }: {\n    message?: string;\n    cause?: Error;\n  } = {}) {\n    super({ name, message, cause });\n  }\n\n  static isInstance(error: unknown): error is NoOutputGeneratedError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\nimport { SpeechModelResponseMetadata } from '../types/speech-model-response-metadata';\n\n/**\nError that is thrown when no speech audio was generated.\n */\nexport class NoSpeechGeneratedError extends AISDKError {\n  readonly responses: Array<SpeechModelResponseMetadata>;\n\n  constructor(options: { responses: Array<SpeechModelResponseMetadata> }) {\n    super({\n      name: 'AI_NoSpeechGeneratedError',\n      message: 'No speech audio generated.',\n    });\n\n    this.responses = options.responses;\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_NoSuchToolError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class NoSuchToolError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly toolName: string;\n  readonly availableTools: string[] | undefined;\n\n  constructor({\n    toolName,\n    availableTools = undefined,\n    message = `Model tried to call unavailable tool '${toolName}'. ${\n      availableTools === undefined\n        ? 'No tools are available.'\n        : `Available tools: ${availableTools.join(', ')}.`\n    }`,\n  }: {\n    toolName: string;\n    availableTools?: string[] | undefined;\n    message?: string;\n  }) {\n    super({ name, message });\n\n    this.toolName = toolName;\n    this.availableTools = availableTools;\n  }\n\n  static isInstance(error: unknown): error is NoSuchToolError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError, getErrorMessage } from '@ai-sdk/provider';\nimport { InvalidToolInputError } from './invalid-tool-input-error';\nimport { NoSuchToolError } from './no-such-tool-error';\n\nconst name = 'AI_ToolCallRepairError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class ToolCallRepairError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly originalError: NoSuchToolError | InvalidToolInputError;\n\n  constructor({\n    cause,\n    originalError,\n    message = `Error repairing tool call: ${getErrorMessage(cause)}`,\n  }: {\n    message?: string;\n    cause: unknown;\n    originalError: NoSuchToolError | InvalidToolInputError;\n  }) {\n    super({ name, message, cause });\n    this.originalError = originalError;\n  }\n\n  static isInstance(error: unknown): error is ToolCallRepairError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\n/**\nError that is thrown when a model with an unsupported version is used.\n */\nexport class UnsupportedModelVersionError extends AISDKError {\n  readonly version: string;\n  readonly provider: string;\n  readonly modelId: string;\n\n  constructor(options: { version: string; provider: string; modelId: string }) {\n    super({\n      name: 'AI_UnsupportedModelVersionError',\n      message:\n        `Unsupported model version ${options.version} for provider \"${options.provider}\" and model \"${options.modelId}\". ` +\n        `AI SDK 5 only supports models that implement specification version \"v2\".`,\n    });\n\n    this.version = options.version;\n    this.provider = options.provider;\n    this.modelId = options.modelId;\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidDataContentError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidDataContentError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly content: unknown;\n\n  constructor({\n    content,\n    cause,\n    message = `Invalid data content. Expected a base64 string, Uint8Array, ArrayBuffer, or Buffer, but got ${typeof content}.`,\n  }: {\n    content: unknown;\n    cause?: unknown;\n    message?: string;\n  }) {\n    super({ name, message, cause });\n\n    this.content = content;\n  }\n\n  static isInstance(error: unknown): error is InvalidDataContentError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidMessageRoleError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidMessageRoleError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly role: string;\n\n  constructor({\n    role,\n    message = `Invalid message role: '${role}'. Must be one of: \"system\", \"user\", \"assistant\", \"tool\".`,\n  }: {\n    role: string;\n    message?: string;\n  }) {\n    super({ name, message });\n\n    this.role = role;\n  }\n\n  static isInstance(error: unknown): error is InvalidMessageRoleError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\nimport { UIMessage } from '../ui/ui-messages';\n\nconst name = 'AI_MessageConversionError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class MessageConversionError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly originalMessage: Omit<UIMessage, 'id'>;\n\n  constructor({\n    originalMessage,\n    message,\n  }: {\n    originalMessage: Omit<UIMessage, 'id'>;\n    message: string;\n  }) {\n    super({ name, message });\n\n    this.originalMessage = originalMessage;\n  }\n\n  static isInstance(error: unknown): error is MessageConversionError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_DownloadError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class DownloadError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly url: string;\n  readonly statusCode?: number;\n  readonly statusText?: string;\n\n  constructor({\n    url,\n    statusCode,\n    statusText,\n    cause,\n    message = cause == null\n      ? `Failed to download ${url}: ${statusCode} ${statusText}`\n      : `Failed to download ${url}: ${cause}`,\n  }: {\n    url: string;\n    statusCode?: number;\n    statusText?: string;\n    message?: string;\n    cause?: unknown;\n  }) {\n    super({ name, message, cause });\n\n    this.url = url;\n    this.statusCode = statusCode;\n    this.statusText = statusText;\n  }\n\n  static isInstance(error: unknown): error is DownloadError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_RetryError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport type RetryErrorReason =\n  | 'maxRetriesExceeded'\n  | 'errorNotRetryable'\n  | 'abort';\n\nexport class RetryError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  // note: property order determines debugging output\n  readonly reason: RetryErrorReason;\n  readonly lastError: unknown;\n  readonly errors: Array<unknown>;\n\n  constructor({\n    message,\n    reason,\n    errors,\n  }: {\n    message: string;\n    reason: RetryErrorReason;\n    errors: Array<unknown>;\n  }) {\n    super({ name, message });\n\n    this.reason = reason;\n    this.errors = errors;\n\n    // separate our last error to make debugging via log easier:\n    this.lastError = errors[errors.length - 1];\n  }\n\n  static isInstance(error: unknown): error is RetryError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import {\n  LanguageModelV2FilePart,\n  LanguageModelV2Message,\n  LanguageModelV2Prompt,\n  LanguageModelV2TextPart,\n} from '@ai-sdk/provider';\nimport {\n  DataContent,\n  FilePart,\n  ImagePart,\n  isUrlSupported,\n  ModelMessage,\n  TextPart,\n} from '@ai-sdk/provider-utils';\nimport {\n  detectMediaType,\n  imageMediaTypeSignatures,\n} from '../util/detect-media-type';\nimport {\n  createDefaultDownloadFunction,\n  DownloadFunction,\n} from '../util/download/download-function';\nimport { convertToLanguageModelV2DataContent } from './data-content';\nimport { InvalidMessageRoleError } from './invalid-message-role-error';\nimport { StandardizedPrompt } from './standardize-prompt';\n\nexport async function convertToLanguageModelPrompt({\n  prompt,\n  supportedUrls,\n  download = createDefaultDownloadFunction(),\n}: {\n  prompt: StandardizedPrompt;\n  supportedUrls: Record<string, RegExp[]>;\n  download: DownloadFunction | undefined;\n}): Promise<LanguageModelV2Prompt> {\n  const downloadedAssets = await downloadAssets(\n    prompt.messages,\n    download,\n    supportedUrls,\n  );\n\n  return [\n    ...(prompt.system != null\n      ? [{ role: 'system' as const, content: prompt.system }]\n      : []),\n    ...prompt.messages.map(message =>\n      convertToLanguageModelMessage({ message, downloadedAssets }),\n    ),\n  ];\n}\n\n/**\n * Convert a ModelMessage to a LanguageModelV2Message.\n *\n * @param message The ModelMessage to convert.\n * @param downloadedAssets A map of URLs to their downloaded data. Only\n *   available if the model does not support URLs, null otherwise.\n */\nexport function convertToLanguageModelMessage({\n  message,\n  downloadedAssets,\n}: {\n  message: ModelMessage;\n  downloadedAssets: Record<\n    string,\n    { mediaType: string | undefined; data: Uint8Array }\n  >;\n}): LanguageModelV2Message {\n  const role = message.role;\n  switch (role) {\n    case 'system': {\n      return {\n        role: 'system',\n        content: message.content,\n        providerOptions: message.providerOptions,\n      };\n    }\n\n    case 'user': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'user',\n          content: [{ type: 'text', text: message.content }],\n          providerOptions: message.providerOptions,\n        };\n      }\n\n      return {\n        role: 'user',\n        content: message.content\n          .map(part => convertPartToLanguageModelPart(part, downloadedAssets))\n          // remove empty text parts:\n          .filter(part => part.type !== 'text' || part.text !== ''),\n        providerOptions: message.providerOptions,\n      };\n    }\n\n    case 'assistant': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'assistant',\n          content: [{ type: 'text', text: message.content }],\n          providerOptions: message.providerOptions,\n        };\n      }\n\n      return {\n        role: 'assistant',\n        content: message.content\n          .filter(\n            // remove empty text parts (no text, and no provider options):\n            part =>\n              part.type !== 'text' ||\n              part.text !== '' ||\n              part.providerOptions != null,\n          )\n          .map(part => {\n            const providerOptions = part.providerOptions;\n\n            switch (part.type) {\n              case 'file': {\n                const { data, mediaType } = convertToLanguageModelV2DataContent(\n                  part.data,\n                );\n                return {\n                  type: 'file',\n                  data,\n                  filename: part.filename,\n                  mediaType: mediaType ?? part.mediaType,\n                  providerOptions,\n                };\n              }\n              case 'reasoning': {\n                return {\n                  type: 'reasoning',\n                  text: part.text,\n                  providerOptions,\n                };\n              }\n              case 'text': {\n                return {\n                  type: 'text' as const,\n                  text: part.text,\n                  providerOptions,\n                };\n              }\n              case 'tool-call': {\n                return {\n                  type: 'tool-call' as const,\n                  toolCallId: part.toolCallId,\n                  toolName: part.toolName,\n                  input: part.input,\n                  providerExecuted: part.providerExecuted,\n                  providerOptions,\n                };\n              }\n              case 'tool-result': {\n                return {\n                  type: 'tool-result' as const,\n                  toolCallId: part.toolCallId,\n                  toolName: part.toolName,\n                  output: part.output,\n                  providerOptions,\n                };\n              }\n            }\n          }),\n        providerOptions: message.providerOptions,\n      };\n    }\n\n    case 'tool': {\n      return {\n        role: 'tool',\n        content: message.content.map(part => ({\n          type: 'tool-result' as const,\n          toolCallId: part.toolCallId,\n          toolName: part.toolName,\n          output: part.output,\n          providerOptions: part.providerOptions,\n        })),\n        providerOptions: message.providerOptions,\n      };\n    }\n\n    default: {\n      const _exhaustiveCheck: never = role;\n      throw new InvalidMessageRoleError({ role: _exhaustiveCheck });\n    }\n  }\n}\n\n/**\n * Downloads images and files from URLs in the messages.\n */\nasync function downloadAssets(\n  messages: ModelMessage[],\n  download: DownloadFunction,\n  supportedUrls: Record<string, RegExp[]>,\n): Promise<\n  Record<string, { mediaType: string | undefined; data: Uint8Array }>\n> {\n  const plannedDownloads = messages\n    .filter(message => message.role === 'user')\n    .map(message => message.content)\n    .filter((content): content is Array<TextPart | ImagePart | FilePart> =>\n      Array.isArray(content),\n    )\n    .flat()\n    .filter(\n      (part): part is ImagePart | FilePart =>\n        part.type === 'image' || part.type === 'file',\n    )\n    .map(part => {\n      const mediaType =\n        part.mediaType ?? (part.type === 'image' ? 'image/*' : undefined);\n\n      let data = part.type === 'image' ? part.image : part.data;\n      if (typeof data === 'string') {\n        try {\n          data = new URL(data);\n        } catch (ignored) {}\n      }\n\n      return { mediaType, data };\n    })\n\n    .filter(\n      (part): part is { mediaType: string | undefined; data: URL } =>\n        part.data instanceof URL,\n    )\n    .map(part => ({\n      url: part.data,\n      isUrlSupportedByModel:\n        part.mediaType != null &&\n        isUrlSupported({\n          url: part.data.toString(),\n          mediaType: part.mediaType,\n          supportedUrls,\n        }),\n    }));\n\n  // download in parallel:\n  const downloadedFiles = await download(plannedDownloads);\n\n  return Object.fromEntries(\n    downloadedFiles\n      .map((file, index) =>\n        file == null\n          ? null\n          : [\n              plannedDownloads[index].url.toString(),\n              { data: file.data, mediaType: file.mediaType },\n            ],\n      )\n      .filter(file => file != null),\n  );\n}\n\n/**\n * Convert part of a message to a LanguageModelV2Part.\n * @param part The part to convert.\n * @param downloadedAssets A map of URLs to their downloaded data. Only\n *  available if the model does not support URLs, null otherwise.\n *\n * @returns The converted part.\n */\nfunction convertPartToLanguageModelPart(\n  part: TextPart | ImagePart | FilePart,\n  downloadedAssets: Record<\n    string,\n    { mediaType: string | undefined; data: Uint8Array }\n  >,\n): LanguageModelV2TextPart | LanguageModelV2FilePart {\n  if (part.type === 'text') {\n    return {\n      type: 'text',\n      text: part.text,\n      providerOptions: part.providerOptions,\n    };\n  }\n\n  let originalData: DataContent | URL;\n  const type = part.type;\n  switch (type) {\n    case 'image':\n      originalData = part.image;\n      break;\n    case 'file':\n      originalData = part.data;\n\n      break;\n    default:\n      throw new Error(`Unsupported part type: ${type}`);\n  }\n\n  const { data: convertedData, mediaType: convertedMediaType } =\n    convertToLanguageModelV2DataContent(originalData);\n\n  let mediaType: string | undefined = convertedMediaType ?? part.mediaType;\n  let data: Uint8Array | string | URL = convertedData; // binary | base64 | url\n\n  // If the content is a URL, we check if it was downloaded:\n  if (data instanceof URL) {\n    const downloadedFile = downloadedAssets[data.toString()];\n    if (downloadedFile) {\n      data = downloadedFile.data;\n      mediaType ??= downloadedFile.mediaType;\n    }\n  }\n\n  // Now that we have the normalized data either as a URL or a Uint8Array,\n  // we can create the LanguageModelV2Part.\n  switch (type) {\n    case 'image': {\n      // When possible, try to detect the media type automatically\n      // to deal with incorrect media type inputs.\n      // When detection fails, use provided media type.\n      if (data instanceof Uint8Array || typeof data === 'string') {\n        mediaType =\n          detectMediaType({ data, signatures: imageMediaTypeSignatures }) ??\n          mediaType;\n      }\n\n      return {\n        type: 'file',\n        mediaType: mediaType ?? 'image/*', // any image\n        filename: undefined,\n        data,\n        providerOptions: part.providerOptions,\n      };\n    }\n\n    case 'file': {\n      // We must have a mediaType for files, if not, throw an error.\n      if (mediaType == null) {\n        throw new Error(`Media type is missing for file part`);\n      }\n\n      return {\n        type: 'file',\n        mediaType,\n        filename: part.filename,\n        data,\n        providerOptions: part.providerOptions,\n      };\n    }\n  }\n}\n","import { convertBase64ToUint8Array } from '@ai-sdk/provider-utils';\n\nexport const imageMediaTypeSignatures = [\n  {\n    mediaType: 'image/gif' as const,\n    bytesPrefix: [0x47, 0x49, 0x46], // GIF\n  },\n  {\n    mediaType: 'image/png' as const,\n    bytesPrefix: [0x89, 0x50, 0x4e, 0x47], // PNG\n  },\n  {\n    mediaType: 'image/jpeg' as const,\n    bytesPrefix: [0xff, 0xd8], // JPEG\n  },\n  {\n    mediaType: 'image/webp' as const,\n    bytesPrefix: [\n      0x52,\n      0x49,\n      0x46,\n      0x46, // \"RIFF\"\n      null,\n      null,\n      null,\n      null, // file size (variable)\n      0x57,\n      0x45,\n      0x42,\n      0x50, // \"WEBP\"\n    ],\n  },\n  {\n    mediaType: 'image/bmp' as const,\n    bytesPrefix: [0x42, 0x4d],\n  },\n  {\n    mediaType: 'image/tiff' as const,\n    bytesPrefix: [0x49, 0x49, 0x2a, 0x00],\n  },\n  {\n    mediaType: 'image/tiff' as const,\n    bytesPrefix: [0x4d, 0x4d, 0x00, 0x2a],\n  },\n  {\n    mediaType: 'image/avif' as const,\n    bytesPrefix: [\n      0x00, 0x00, 0x00, 0x20, 0x66, 0x74, 0x79, 0x70, 0x61, 0x76, 0x69, 0x66,\n    ],\n  },\n  {\n    mediaType: 'image/heic' as const,\n    bytesPrefix: [\n      0x00, 0x00, 0x00, 0x20, 0x66, 0x74, 0x79, 0x70, 0x68, 0x65, 0x69, 0x63,\n    ],\n  },\n] as const;\n\nexport const audioMediaTypeSignatures = [\n  {\n    mediaType: 'audio/mpeg' as const,\n    bytesPrefix: [0xff, 0xfb],\n  },\n  {\n    mediaType: 'audio/mpeg' as const,\n    bytesPrefix: [0xff, 0xfa],\n  },\n  {\n    mediaType: 'audio/mpeg' as const,\n    bytesPrefix: [0xff, 0xf3],\n  },\n  {\n    mediaType: 'audio/mpeg' as const,\n    bytesPrefix: [0xff, 0xf2],\n  },\n  {\n    mediaType: 'audio/mpeg' as const,\n    bytesPrefix: [0xff, 0xe3],\n  },\n  {\n    mediaType: 'audio/mpeg' as const,\n    bytesPrefix: [0xff, 0xe2],\n  },\n  {\n    mediaType: 'audio/wav' as const,\n    bytesPrefix: [\n      0x52, // R\n      0x49, // I\n      0x46, // F\n      0x46, // F\n      null,\n      null,\n      null,\n      null,\n      0x57, // W\n      0x41, // A\n      0x56, // V\n      0x45, // E\n    ],\n  },\n  {\n    mediaType: 'audio/ogg' as const,\n    bytesPrefix: [0x4f, 0x67, 0x67, 0x53],\n  },\n  {\n    mediaType: 'audio/flac' as const,\n    bytesPrefix: [0x66, 0x4c, 0x61, 0x43],\n  },\n  {\n    mediaType: 'audio/aac' as const,\n    bytesPrefix: [0x40, 0x15, 0x00, 0x00],\n  },\n  {\n    mediaType: 'audio/mp4' as const,\n    bytesPrefix: [0x66, 0x74, 0x79, 0x70],\n  },\n  {\n    mediaType: 'audio/webm',\n    bytesPrefix: [0x1a, 0x45, 0xdf, 0xa3],\n  },\n] as const;\n\nconst stripID3 = (data: Uint8Array | string) => {\n  const bytes =\n    typeof data === 'string' ? convertBase64ToUint8Array(data) : data;\n  const id3Size =\n    ((bytes[6] & 0x7f) << 21) |\n    ((bytes[7] & 0x7f) << 14) |\n    ((bytes[8] & 0x7f) << 7) |\n    (bytes[9] & 0x7f);\n\n  // The raw MP3 starts here\n  return bytes.slice(id3Size + 10);\n};\n\nfunction stripID3TagsIfPresent(data: Uint8Array | string): Uint8Array | string {\n  const hasId3 =\n    (typeof data === 'string' && data.startsWith('SUQz')) ||\n    (typeof data !== 'string' &&\n      data.length > 10 &&\n      data[0] === 0x49 && // 'I'\n      data[1] === 0x44 && // 'D'\n      data[2] === 0x33); // '3'\n\n  return hasId3 ? stripID3(data) : data;\n}\n\n/**\n * Detect the media IANA media type of a file using a list of signatures.\n *\n * @param data - The file data.\n * @param signatures - The signatures to use for detection.\n * @returns The media type of the file.\n */\nexport function detectMediaType({\n  data,\n  signatures,\n}: {\n  data: Uint8Array | string;\n  signatures: typeof audioMediaTypeSignatures | typeof imageMediaTypeSignatures;\n}): (typeof signatures)[number]['mediaType'] | undefined {\n  const processedData = stripID3TagsIfPresent(data);\n\n  // Convert the first ~18 bytes (24 base64 chars) for consistent detection logic:\n  const bytes =\n    typeof processedData === 'string'\n      ? convertBase64ToUint8Array(\n          processedData.substring(0, Math.min(processedData.length, 24)),\n        )\n      : processedData;\n\n  for (const signature of signatures) {\n    if (\n      bytes.length >= signature.bytesPrefix.length &&\n      signature.bytesPrefix.every(\n        (byte, index) => byte === null || bytes[index] === byte,\n      )\n    ) {\n      return signature.mediaType;\n    }\n  }\n\n  return undefined;\n}\n","import { DownloadError } from './download-error';\nimport {\n  withUserAgentSuffix,\n  getRuntimeEnvironmentUserAgent,\n} from '@ai-sdk/provider-utils';\nimport { VERSION } from '../../version';\n\n/**\n * Download a file from a URL.\n *\n * @param url - The URL to download from.\n * @returns The downloaded data and media type.\n *\n * @throws DownloadError if the download fails.\n */\nexport const download = async ({ url }: { url: URL }) => {\n  const urlText = url.toString();\n  try {\n    const response = await fetch(urlText, {\n      headers: withUserAgentSuffix(\n        {},\n        `ai-sdk/${VERSION}`,\n        getRuntimeEnvironmentUserAgent(),\n      ),\n    });\n\n    if (!response.ok) {\n      throw new DownloadError({\n        url: urlText,\n        statusCode: response.status,\n        statusText: response.statusText,\n      });\n    }\n\n    return {\n      data: new Uint8Array(await response.arrayBuffer()),\n      mediaType: response.headers.get('content-type') ?? undefined,\n    };\n  } catch (error) {\n    if (DownloadError.isInstance(error)) {\n      throw error;\n    }\n\n    throw new DownloadError({ url: urlText, cause: error });\n  }\n};\n","declare const __PACKAGE_VERSION__: string | undefined;\nexport const VERSION: string =\n  typeof __PACKAGE_VERSION__ !== 'undefined'\n    ? __PACKAGE_VERSION__\n    : '0.0.0-test';\n","import { download as originalDownload } from './download';\n\n/**\n * Experimental. Can change in patch versions without warning.\n *\n * Download function. Called with the array of URLs and a boolean indicating\n * whether the URL is supported by the model.\n *\n * The download function can decide for each URL:\n * - to return null (which means that the URL should be passed to the model)\n * - to download the asset and return the data (incl. retries, authentication, etc.)\n *\n * Should throw DownloadError if the download fails.\n *\n * Should return an array of objects sorted by the order of the requested downloads.\n * For each object, the data should be a Uint8Array if the URL was downloaded.\n * For each object, the mediaType should be the media type of the downloaded asset.\n * For each object, the data should be null if the URL should be passed through as is.\n */\nexport type DownloadFunction = (\n  options: Array<{\n    url: URL;\n    isUrlSupportedByModel: boolean;\n  }>,\n) => PromiseLike<\n  Array<{\n    data: Uint8Array;\n    mediaType: string | undefined;\n  } | null>\n>;\n\n/**\n * Default download function.\n * Downloads the file if it is not supported by the model.\n */\nexport const createDefaultDownloadFunction =\n  (download: typeof originalDownload = originalDownload): DownloadFunction =>\n  requestedDownloads =>\n    Promise.all(\n      requestedDownloads.map(async requestedDownload =>\n        requestedDownload.isUrlSupportedByModel\n          ? null\n          : download(requestedDownload),\n      ),\n    );\n","import { AISDKError, LanguageModelV2DataContent } from '@ai-sdk/provider';\nimport {\n  convertBase64ToUint8Array,\n  convertUint8ArrayToBase64,\n  DataContent,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport { InvalidDataContentError } from './invalid-data-content-error';\nimport { splitDataUrl } from './split-data-url';\n\n/**\n@internal\n */\nexport const dataContentSchema: z.ZodType<DataContent> = z.union([\n  z.string(),\n  z.instanceof(Uint8Array),\n  z.instanceof(ArrayBuffer),\n  z.custom<Buffer>(\n    // Buffer might not be available in some environments such as CloudFlare:\n    (value: unknown): value is Buffer =>\n      globalThis.Buffer?.isBuffer(value) ?? false,\n    { message: 'Must be a Buffer' },\n  ),\n]);\n\nexport function convertToLanguageModelV2DataContent(\n  content: DataContent | URL,\n): {\n  data: LanguageModelV2DataContent;\n  mediaType: string | undefined;\n} {\n  // Buffer & Uint8Array:\n  if (content instanceof Uint8Array) {\n    return { data: content, mediaType: undefined };\n  }\n\n  // ArrayBuffer needs conversion to Uint8Array (lightweight):\n  if (content instanceof ArrayBuffer) {\n    return { data: new Uint8Array(content), mediaType: undefined };\n  }\n\n  // Attempt to create a URL from the data. If it fails, we can assume the data\n  // is not a URL and likely some other sort of data.\n  if (typeof content === 'string') {\n    try {\n      content = new URL(content);\n    } catch (error) {\n      // ignored\n    }\n  }\n\n  // Extract data from data URL:\n  if (content instanceof URL && content.protocol === 'data:') {\n    const { mediaType: dataUrlMediaType, base64Content } = splitDataUrl(\n      content.toString(),\n    );\n\n    if (dataUrlMediaType == null || base64Content == null) {\n      throw new AISDKError({\n        name: 'InvalidDataContentError',\n        message: `Invalid data URL format in content ${content.toString()}`,\n      });\n    }\n\n    return { data: base64Content, mediaType: dataUrlMediaType };\n  }\n\n  return { data: content, mediaType: undefined };\n}\n\n/**\nConverts data content to a base64-encoded string.\n\n@param content - Data content to convert.\n@returns Base64-encoded string.\n*/\nexport function convertDataContentToBase64String(content: DataContent): string {\n  if (typeof content === 'string') {\n    return content;\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return convertUint8ArrayToBase64(new Uint8Array(content));\n  }\n\n  return convertUint8ArrayToBase64(content);\n}\n\n/**\nConverts data content to a Uint8Array.\n\n@param content - Data content to convert.\n@returns Uint8Array.\n */\nexport function convertDataContentToUint8Array(\n  content: DataContent,\n): Uint8Array {\n  if (content instanceof Uint8Array) {\n    return content;\n  }\n\n  if (typeof content === 'string') {\n    try {\n      return convertBase64ToUint8Array(content);\n    } catch (error) {\n      throw new InvalidDataContentError({\n        message:\n          'Invalid data content. Content string is not a base64-encoded media.',\n        content,\n        cause: error,\n      });\n    }\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return new Uint8Array(content);\n  }\n\n  throw new InvalidDataContentError({ content });\n}\n\n/**\n * Converts a Uint8Array to a string of text.\n *\n * @param uint8Array - The Uint8Array to convert.\n * @returns The converted string.\n */\nexport function convertUint8ArrayToText(uint8Array: Uint8Array): string {\n  try {\n    return new TextDecoder().decode(uint8Array);\n  } catch (error) {\n    throw new Error('Error decoding Uint8Array to text');\n  }\n}\n","export function splitDataUrl(dataUrl: string): {\n  mediaType: string | undefined;\n  base64Content: string | undefined;\n} {\n  try {\n    const [header, base64Content] = dataUrl.split(',');\n    return {\n      mediaType: header.split(';')[0].split(':')[1],\n      base64Content,\n    };\n  } catch (error) {\n    return {\n      mediaType: undefined,\n      base64Content: undefined,\n    };\n  }\n}\n","import { InvalidArgumentError } from '../error/invalid-argument-error';\nimport { CallSettings } from './call-settings';\n\n/**\n * Validates call settings and returns a new object with limited values.\n */\nexport function prepareCallSettings({\n  maxOutputTokens,\n  temperature,\n  topP,\n  topK,\n  presencePenalty,\n  frequencyPenalty,\n  seed,\n  stopSequences,\n}: Omit<CallSettings, 'abortSignal' | 'headers' | 'maxRetries'>): Omit<\n  CallSettings,\n  'abortSignal' | 'headers' | 'maxRetries'\n> {\n  if (maxOutputTokens != null) {\n    if (!Number.isInteger(maxOutputTokens)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxOutputTokens',\n        value: maxOutputTokens,\n        message: 'maxOutputTokens must be an integer',\n      });\n    }\n\n    if (maxOutputTokens < 1) {\n      throw new InvalidArgumentError({\n        parameter: 'maxOutputTokens',\n        value: maxOutputTokens,\n        message: 'maxOutputTokens must be >= 1',\n      });\n    }\n  }\n\n  if (temperature != null) {\n    if (typeof temperature !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'temperature',\n        value: temperature,\n        message: 'temperature must be a number',\n      });\n    }\n  }\n\n  if (topP != null) {\n    if (typeof topP !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'topP',\n        value: topP,\n        message: 'topP must be a number',\n      });\n    }\n  }\n\n  if (topK != null) {\n    if (typeof topK !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'topK',\n        value: topK,\n        message: 'topK must be a number',\n      });\n    }\n  }\n\n  if (presencePenalty != null) {\n    if (typeof presencePenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'presencePenalty',\n        value: presencePenalty,\n        message: 'presencePenalty must be a number',\n      });\n    }\n  }\n\n  if (frequencyPenalty != null) {\n    if (typeof frequencyPenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'frequencyPenalty',\n        value: frequencyPenalty,\n        message: 'frequencyPenalty must be a number',\n      });\n    }\n  }\n\n  if (seed != null) {\n    if (!Number.isInteger(seed)) {\n      throw new InvalidArgumentError({\n        parameter: 'seed',\n        value: seed,\n        message: 'seed must be an integer',\n      });\n    }\n  }\n\n  return {\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    stopSequences,\n    seed,\n  };\n}\n","import {\n  LanguageModelV2FunctionTool,\n  LanguageModelV2ProviderDefinedTool,\n  LanguageModelV2ToolChoice,\n} from '@ai-sdk/provider';\nimport { asSchema } from '@ai-sdk/provider-utils';\nimport { isNonEmptyObject } from '../util/is-non-empty-object';\nimport { ToolSet } from '../generate-text';\nimport { ToolChoice } from '../types/language-model';\n\nexport function prepareToolsAndToolChoice<TOOLS extends ToolSet>({\n  tools,\n  toolChoice,\n  activeTools,\n}: {\n  tools: TOOLS | undefined;\n  toolChoice: ToolChoice<TOOLS> | undefined;\n  activeTools: Array<keyof TOOLS> | undefined;\n}): {\n  tools:\n    | Array<LanguageModelV2FunctionTool | LanguageModelV2ProviderDefinedTool>\n    | undefined;\n  toolChoice: LanguageModelV2ToolChoice | undefined;\n} {\n  if (!isNonEmptyObject(tools)) {\n    return {\n      tools: undefined,\n      toolChoice: undefined,\n    };\n  }\n\n  // when activeTools is provided, we only include the tools that are in the list:\n  const filteredTools =\n    activeTools != null\n      ? Object.entries(tools).filter(([name]) =>\n          activeTools.includes(name as keyof TOOLS),\n        )\n      : Object.entries(tools);\n\n  return {\n    tools: filteredTools.map(([name, tool]) => {\n      const toolType = tool.type;\n      switch (toolType) {\n        case undefined:\n        case 'dynamic':\n        case 'function':\n          return {\n            type: 'function' as const,\n            name,\n            description: tool.description,\n            inputSchema: asSchema(tool.inputSchema).jsonSchema,\n            providerOptions: tool.providerOptions,\n          };\n        case 'provider-defined':\n          return {\n            type: 'provider-defined' as const,\n            name,\n            id: tool.id,\n            args: tool.args,\n          };\n        default: {\n          const exhaustiveCheck: never = toolType;\n          throw new Error(`Unsupported tool type: ${exhaustiveCheck}`);\n        }\n      }\n    }),\n    toolChoice:\n      toolChoice == null\n        ? { type: 'auto' }\n        : typeof toolChoice === 'string'\n          ? { type: toolChoice }\n          : { type: 'tool' as const, toolName: toolChoice.toolName as string },\n  };\n}\n","export function isNonEmptyObject(\n  object: Record<string, unknown> | undefined | null,\n): object is Record<string, unknown> {\n  return object != null && Object.keys(object).length > 0;\n}\n","import { InvalidPromptError } from '@ai-sdk/provider';\nimport { ModelMessage, safeValidateTypes } from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport { modelMessageSchema } from './message';\nimport { Prompt } from './prompt';\n\nexport type StandardizedPrompt = {\n  /**\n   * System message.\n   */\n  system?: string;\n\n  /**\n   * Messages.\n   */\n  messages: ModelMessage[];\n};\n\nexport async function standardizePrompt(\n  prompt: Prompt,\n): Promise<StandardizedPrompt> {\n  if (prompt.prompt == null && prompt.messages == null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt or messages must be defined',\n    });\n  }\n\n  if (prompt.prompt != null && prompt.messages != null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt and messages cannot be defined at the same time',\n    });\n  }\n\n  // validate that system is a string\n  if (prompt.system != null && typeof prompt.system !== 'string') {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'system must be a string',\n    });\n  }\n\n  let messages: ModelMessage[];\n\n  if (prompt.prompt != null && typeof prompt.prompt === 'string') {\n    messages = [{ role: 'user', content: prompt.prompt }];\n  } else if (prompt.prompt != null && Array.isArray(prompt.prompt)) {\n    messages = prompt.prompt;\n  } else if (prompt.messages != null) {\n    messages = prompt.messages;\n  } else {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt or messages must be defined',\n    });\n  }\n\n  if (messages.length === 0) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'messages must not be empty',\n    });\n  }\n\n  const validationResult = await safeValidateTypes({\n    value: messages,\n    schema: z.array(modelMessageSchema),\n  });\n\n  if (!validationResult.success) {\n    throw new InvalidPromptError({\n      prompt,\n      message:\n        'The messages must be a ModelMessage[]. ' +\n        'If you have passed a UIMessage[], you can use convertToModelMessages to convert them.',\n      cause: validationResult.error,\n    });\n  }\n\n  return {\n    messages,\n    system: prompt.system,\n  };\n}\n","import {\n  AssistantModelMessage,\n  ModelMessage,\n  SystemModelMessage,\n  ToolModelMessage,\n  UserModelMessage,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport { providerMetadataSchema } from '../types/provider-metadata';\nimport {\n  filePartSchema,\n  imagePartSchema,\n  reasoningPartSchema,\n  textPartSchema,\n  toolCallPartSchema,\n  toolResultPartSchema,\n} from './content-part';\n\n/**\n@deprecated Use `SystemModelMessage` instead.\n */\n// TODO remove in AI SDK 6\nexport type CoreSystemMessage = SystemModelMessage;\n\nexport const systemModelMessageSchema: z.ZodType<SystemModelMessage> = z.object(\n  {\n    role: z.literal('system'),\n    content: z.string(),\n    providerOptions: providerMetadataSchema.optional(),\n  },\n);\n\n/**\n@deprecated Use `systemModelMessageSchema` instead.\n */\n// TODO remove in AI SDK 6\nexport const coreSystemMessageSchema = systemModelMessageSchema;\n\n/**\n@deprecated Use `UserModelMessage` instead.\n */\n// TODO remove in AI SDK 6\nexport type CoreUserMessage = UserModelMessage;\n\nexport const userModelMessageSchema: z.ZodType<UserModelMessage> = z.object({\n  role: z.literal('user'),\n  content: z.union([\n    z.string(),\n    z.array(z.union([textPartSchema, imagePartSchema, filePartSchema])),\n  ]),\n  providerOptions: providerMetadataSchema.optional(),\n});\n\n/**\n@deprecated Use `userModelMessageSchema` instead.\n */\n// TODO remove in AI SDK 6\nexport const coreUserMessageSchema = userModelMessageSchema;\n\n/**\n@deprecated Use `AssistantModelMessage` instead.\n */\n// TODO remove in AI SDK 6\nexport type CoreAssistantMessage = AssistantModelMessage;\n\nexport const assistantModelMessageSchema: z.ZodType<AssistantModelMessage> =\n  z.object({\n    role: z.literal('assistant'),\n    content: z.union([\n      z.string(),\n      z.array(\n        z.union([\n          textPartSchema,\n          filePartSchema,\n          reasoningPartSchema,\n          toolCallPartSchema,\n          toolResultPartSchema,\n        ]),\n      ),\n    ]),\n    providerOptions: providerMetadataSchema.optional(),\n  });\n\n/**\n@deprecated Use `assistantModelMessageSchema` instead.\n */\n// TODO remove in AI SDK 6\nexport const coreAssistantMessageSchema = assistantModelMessageSchema;\n\n/**\n@deprecated Use `ToolModelMessage` instead.\n */\n// TODO remove in AI SDK 6\nexport type CoreToolMessage = ToolModelMessage;\n\nexport const toolModelMessageSchema: z.ZodType<ToolModelMessage> = z.object({\n  role: z.literal('tool'),\n  content: z.array(toolResultPartSchema),\n  providerOptions: providerMetadataSchema.optional(),\n});\n\n/**\n@deprecated Use `toolModelMessageSchema` instead.\n */\n// TODO remove in AI SDK 6\nexport const coreToolMessageSchema = toolModelMessageSchema;\n\n/**\n@deprecated Use `ModelMessage` instead.\n   */\n// TODO remove in AI SDK 6\nexport type CoreMessage = ModelMessage;\n\nexport const modelMessageSchema: z.ZodType<ModelMessage> = z.union([\n  systemModelMessageSchema,\n  userModelMessageSchema,\n  assistantModelMessageSchema,\n  toolModelMessageSchema,\n]);\n\n/**\n@deprecated Use `modelMessageSchema` instead.\n */\n// TODO remove in AI SDK 6\nexport const coreMessageSchema: z.ZodType<CoreMessage> = modelMessageSchema;\n","import { SharedV2ProviderMetadata } from '@ai-sdk/provider';\nimport { z } from 'zod/v4';\nimport { jsonValueSchema } from './json-value';\n\n/**\nAdditional provider-specific metadata that is returned from the provider.\n\nThis is needed to enable provider-specific functionality that can be\nfully encapsulated in the provider.\n */\nexport type ProviderMetadata = SharedV2ProviderMetadata;\n\nexport const providerMetadataSchema: z.ZodType<ProviderMetadata> = z.record(\n  z.string(),\n  z.record(z.string(), jsonValueSchema),\n);\n","import { JSONValue as OriginalJSONValue } from '@ai-sdk/provider';\nimport { z } from 'zod/v4';\n\nexport const jsonValueSchema: z.ZodType<JSONValue> = z.lazy(() =>\n  z.union([\n    z.null(),\n    z.string(),\n    z.number(),\n    z.boolean(),\n    z.record(z.string(), jsonValueSchema),\n    z.array(jsonValueSchema),\n  ]),\n);\n\nexport type JSONValue = OriginalJSONValue;\n","import { LanguageModelV2ToolResultOutput } from '@ai-sdk/provider';\nimport {\n  FilePart,\n  ImagePart,\n  ProviderOptions,\n  ReasoningPart,\n  TextPart,\n  ToolResultPart,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport { jsonValueSchema } from '../types/json-value';\nimport { providerMetadataSchema } from '../types/provider-metadata';\nimport { dataContentSchema } from './data-content';\n\n/**\n@internal\n */\nexport const textPartSchema: z.ZodType<TextPart> = z.object({\n  type: z.literal('text'),\n  text: z.string(),\n  providerOptions: providerMetadataSchema.optional(),\n});\n\n/**\n@internal\n */\nexport const imagePartSchema: z.ZodType<ImagePart> = z.object({\n  type: z.literal('image'),\n  image: z.union([dataContentSchema, z.instanceof(URL)]),\n  mediaType: z.string().optional(),\n  providerOptions: providerMetadataSchema.optional(),\n});\n\n/**\n@internal\n */\nexport const filePartSchema: z.ZodType<FilePart> = z.object({\n  type: z.literal('file'),\n  data: z.union([dataContentSchema, z.instanceof(URL)]),\n  filename: z.string().optional(),\n  mediaType: z.string(),\n  providerOptions: providerMetadataSchema.optional(),\n});\n\n/**\n@internal\n */\nexport const reasoningPartSchema: z.ZodType<ReasoningPart> = z.object({\n  type: z.literal('reasoning'),\n  text: z.string(),\n  providerOptions: providerMetadataSchema.optional(),\n});\n\n/**\nTool call content part of a prompt. It contains a tool call (usually generated by the AI model).\n */\nexport interface ToolCallPart {\n  type: 'tool-call';\n\n  /**\nID of the tool call. This ID is used to match the tool call with the tool result.\n */\n  toolCallId: string;\n\n  /**\nName of the tool that is being called.\n */\n  toolName: string;\n\n  /**\nArguments of the tool call. This is a JSON-serializable object that matches the tool's input schema.\n   */\n  input: unknown;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  providerOptions?: ProviderOptions;\n}\n\n/**\n@internal\n */\nexport const toolCallPartSchema: z.ZodType<ToolCallPart> = z.object({\n  type: z.literal('tool-call'),\n  toolCallId: z.string(),\n  toolName: z.string(),\n  input: z.unknown(),\n  providerOptions: providerMetadataSchema.optional(),\n  providerExecuted: z.boolean().optional(),\n}) as z.ZodType<ToolCallPart>; // necessary bc input is optional on Zod type\n\n/**\n@internal\n */\nexport const outputSchema: z.ZodType<LanguageModelV2ToolResultOutput> =\n  z.discriminatedUnion('type', [\n    z.object({\n      type: z.literal('text'),\n      value: z.string(),\n    }),\n    z.object({\n      type: z.literal('json'),\n      value: jsonValueSchema,\n    }),\n    z.object({\n      type: z.literal('error-text'),\n      value: z.string(),\n    }),\n    z.object({\n      type: z.literal('error-json'),\n      value: jsonValueSchema,\n    }),\n    z.object({\n      type: z.literal('content'),\n      value: z.array(\n        z.union([\n          z.object({\n            type: z.literal('text'),\n            text: z.string(),\n          }),\n          z.object({\n            type: z.literal('media'),\n            data: z.string(),\n            mediaType: z.string(),\n          }),\n        ]),\n      ),\n    }),\n  ]);\n\n/**\n@internal\n */\nexport const toolResultPartSchema: z.ZodType<ToolResultPart> = z.object({\n  type: z.literal('tool-result'),\n  toolCallId: z.string(),\n  toolName: z.string(),\n  output: outputSchema,\n  providerOptions: providerMetadataSchema.optional(),\n}) as z.ZodType<ToolResultPart>; // necessary bc result is optional on Zod type\n","import { GatewayAuthenticationError } from '@ai-sdk/gateway';\nimport { AISDKError } from '@ai-sdk/provider';\n\nexport function wrapGatewayError(error: unknown): unknown {\n  if (!GatewayAuthenticationError.isInstance(error)) return error;\n\n  const isProductionEnv = process?.env.NODE_ENV === 'production';\n  const moreInfoURL = 'https://v5.ai-sdk.dev/unauthenticated-ai-gateway';\n\n  if (isProductionEnv) {\n    return new AISDKError({\n      name: 'GatewayError',\n      message: `Unauthenticated. Configure AI_GATEWAY_API_KEY or use a provider module. Learn more: ${moreInfoURL}`,\n    });\n  }\n\n  return Object.assign(\n    new Error(`\\u001b[1m\\u001b[31mUnauthenticated request to AI Gateway.\\u001b[0m\n\nTo authenticate, set the \\u001b[33mAI_GATEWAY_API_KEY\\u001b[0m environment variable with your API key.\n\nAlternatively, you can use a provider module instead of the AI Gateway.\n\nLearn more: \\u001b[34m${moreInfoURL}\\u001b[0m\n\n`),\n    { name: 'GatewayAuthenticationError' },\n  );\n}\n","import { TelemetrySettings } from './telemetry-settings';\n\nexport function assembleOperationName({\n  operationId,\n  telemetry,\n}: {\n  operationId: string;\n  telemetry?: TelemetrySettings;\n}) {\n  return {\n    // standardized operation and resource name:\n    'operation.name': `${operationId}${\n      telemetry?.functionId != null ? ` ${telemetry.functionId}` : ''\n    }`,\n    'resource.name': telemetry?.functionId,\n\n    // detailed, AI SDK specific data:\n    'ai.operationId': operationId,\n    'ai.telemetry.functionId': telemetry?.functionId,\n  };\n}\n","import { Attributes } from '@opentelemetry/api';\nimport { CallSettings } from '../prompt/call-settings';\nimport { TelemetrySettings } from './telemetry-settings';\n\nexport function getBaseTelemetryAttributes({\n  model,\n  settings,\n  telemetry,\n  headers,\n}: {\n  model: { modelId: string; provider: string };\n  settings: Omit<CallSettings, 'abortSignal' | 'headers' | 'temperature'>;\n  telemetry: TelemetrySettings | undefined;\n  headers: Record<string, string | undefined> | undefined;\n}): Attributes {\n  return {\n    'ai.model.provider': model.provider,\n    'ai.model.id': model.modelId,\n\n    // settings:\n    ...Object.entries(settings).reduce((attributes, [key, value]) => {\n      attributes[`ai.settings.${key}`] = value;\n      return attributes;\n    }, {} as Attributes),\n\n    // add metadata as attributes:\n    ...Object.entries(telemetry?.metadata ?? {}).reduce(\n      (attributes, [key, value]) => {\n        attributes[`ai.telemetry.metadata.${key}`] = value;\n        return attributes;\n      },\n      {} as Attributes,\n    ),\n\n    // request headers\n    ...Object.entries(headers ?? {}).reduce((attributes, [key, value]) => {\n      if (value !== undefined) {\n        attributes[`ai.request.headers.${key}`] = value;\n      }\n      return attributes;\n    }, {} as Attributes),\n  };\n}\n","import { Tracer, trace } from '@opentelemetry/api';\nimport { noopTracer } from './noop-tracer';\n\nexport function getTracer({\n  isEnabled = false,\n  tracer,\n}: {\n  isEnabled?: boolean;\n  tracer?: Tracer;\n} = {}): Tracer {\n  if (!isEnabled) {\n    return noopTracer;\n  }\n\n  if (tracer) {\n    return tracer;\n  }\n\n  return trace.getTracer('ai');\n}\n","import { Span, SpanContext, Tracer } from '@opentelemetry/api';\n\n/**\n * Tracer implementation that does nothing (null object).\n */\nexport const noopTracer: Tracer = {\n  startSpan(): Span {\n    return noopSpan;\n  },\n\n  startActiveSpan<F extends (span: Span) => unknown>(\n    name: unknown,\n    arg1: unknown,\n    arg2?: unknown,\n    arg3?: F,\n  ): ReturnType<any> {\n    if (typeof arg1 === 'function') {\n      return arg1(noopSpan);\n    }\n    if (typeof arg2 === 'function') {\n      return arg2(noopSpan);\n    }\n    if (typeof arg3 === 'function') {\n      return arg3(noopSpan);\n    }\n  },\n};\n\nconst noopSpan: Span = {\n  spanContext() {\n    return noopSpanContext;\n  },\n  setAttribute() {\n    return this;\n  },\n  setAttributes() {\n    return this;\n  },\n  addEvent() {\n    return this;\n  },\n  addLink() {\n    return this;\n  },\n  addLinks() {\n    return this;\n  },\n  setStatus() {\n    return this;\n  },\n  updateName() {\n    return this;\n  },\n  end() {\n    return this;\n  },\n  isRecording() {\n    return false;\n  },\n  recordException() {\n    return this;\n  },\n};\n\nconst noopSpanContext: SpanContext = {\n  traceId: '',\n  spanId: '',\n  traceFlags: 0,\n};\n","import { Attributes, Span, Tracer, SpanStatusCode } from '@opentelemetry/api';\n\nexport function recordSpan<T>({\n  name,\n  tracer,\n  attributes,\n  fn,\n  endWhenDone = true,\n}: {\n  name: string;\n  tracer: Tracer;\n  attributes: Attributes;\n  fn: (span: Span) => Promise<T>;\n  endWhenDone?: boolean;\n}) {\n  return tracer.startActiveSpan(name, { attributes }, async span => {\n    try {\n      const result = await fn(span);\n\n      if (endWhenDone) {\n        span.end();\n      }\n\n      return result;\n    } catch (error) {\n      try {\n        recordErrorOnSpan(span, error);\n      } finally {\n        // always stop the span when there is an error:\n        span.end();\n      }\n\n      throw error;\n    }\n  });\n}\n\n/**\n * Record an error on a span. If the error is an instance of Error, an exception event will be recorded on the span, otherwise\n * the span will be set to an error status.\n *\n * @param span - The span to record the error on.\n * @param error - The error to record on the span.\n */\nexport function recordErrorOnSpan(span: Span, error: unknown) {\n  if (error instanceof Error) {\n    span.recordException({\n      name: error.name,\n      message: error.message,\n      stack: error.stack,\n    });\n    span.setStatus({\n      code: SpanStatusCode.ERROR,\n      message: error.message,\n    });\n  } else {\n    span.setStatus({ code: SpanStatusCode.ERROR });\n  }\n}\n","import type { Attributes, AttributeValue } from '@opentelemetry/api';\nimport type { TelemetrySettings } from './telemetry-settings';\n\nexport function selectTelemetryAttributes({\n  telemetry,\n  attributes,\n}: {\n  telemetry?: TelemetrySettings;\n  attributes: {\n    [attributeKey: string]:\n      | AttributeValue\n      | { input: () => AttributeValue | undefined }\n      | { output: () => AttributeValue | undefined }\n      | undefined;\n  };\n}): Attributes {\n  // when telemetry is disabled, return an empty object to avoid serialization overhead:\n  if (telemetry?.isEnabled !== true) {\n    return {};\n  }\n\n  return Object.entries(attributes).reduce((attributes, [key, value]) => {\n    if (value == null) {\n      return attributes;\n    }\n\n    // input value, check if it should be recorded:\n    if (\n      typeof value === 'object' &&\n      'input' in value &&\n      typeof value.input === 'function'\n    ) {\n      // default to true:\n      if (telemetry?.recordInputs === false) {\n        return attributes;\n      }\n\n      const result = value.input();\n\n      return result == null ? attributes : { ...attributes, [key]: result };\n    }\n\n    // output value, check if it should be recorded:\n    if (\n      typeof value === 'object' &&\n      'output' in value &&\n      typeof value.output === 'function'\n    ) {\n      // default to true:\n      if (telemetry?.recordOutputs === false) {\n        return attributes;\n      }\n\n      const result = value.output();\n\n      return result == null ? attributes : { ...attributes, [key]: result };\n    }\n\n    // value is an attribute value already:\n    return { ...attributes, [key]: value };\n  }, {});\n}\n","import {\n  LanguageModelV2Message,\n  LanguageModelV2Prompt,\n} from '@ai-sdk/provider';\nimport { convertDataContentToBase64String } from '../prompt/data-content';\n\n/**\n * Helper utility to serialize prompt content for OpenTelemetry tracing.\n * It is initially created because normalized LanguageModelV1Prompt carries\n * images as Uint8Arrays, on which JSON.stringify acts weirdly, converting\n * them to objects with stringified indices as keys, e.g. {\"0\": 42, \"1\": 69 }.\n */\nexport function stringifyForTelemetry(prompt: LanguageModelV2Prompt): string {\n  return JSON.stringify(\n    prompt.map((message: LanguageModelV2Message) => ({\n      ...message,\n      content:\n        typeof message.content === 'string'\n          ? message.content\n          : message.content.map(part =>\n              part.type === 'file'\n                ? {\n                    ...part,\n                    data:\n                      part.data instanceof Uint8Array\n                        ? convertDataContentToBase64String(part.data)\n                        : part.data,\n                  }\n                : part,\n            ),\n    })),\n  );\n}\n","import { LanguageModelV2Usage } from '@ai-sdk/provider';\n\n/**\nRepresents the number of tokens used in a prompt and completion.\n */\nexport type LanguageModelUsage = LanguageModelV2Usage;\n\n/**\nRepresents the number of tokens used in an embedding.\n */\n// TODO replace with EmbeddingModelV2Usage\nexport type EmbeddingModelUsage = {\n  /**\nThe number of tokens used in the embedding.\n   */\n  tokens: number;\n};\n\nexport function addLanguageModelUsage(\n  usage1: LanguageModelUsage,\n  usage2: LanguageModelUsage,\n): LanguageModelUsage {\n  return {\n    inputTokens: addTokenCounts(usage1.inputTokens, usage2.inputTokens),\n    outputTokens: addTokenCounts(usage1.outputTokens, usage2.outputTokens),\n    totalTokens: addTokenCounts(usage1.totalTokens, usage2.totalTokens),\n    reasoningTokens: addTokenCounts(\n      usage1.reasoningTokens,\n      usage2.reasoningTokens,\n    ),\n    cachedInputTokens: addTokenCounts(\n      usage1.cachedInputTokens,\n      usage2.cachedInputTokens,\n    ),\n  };\n}\n\nfunction addTokenCounts(\n  tokenCount1: number | undefined,\n  tokenCount2: number | undefined,\n): number | undefined {\n  return tokenCount1 == null && tokenCount2 == null\n    ? undefined\n    : (tokenCount1 ?? 0) + (tokenCount2 ?? 0);\n}\n","export function asArray<T>(value: T | T[] | undefined): T[] {\n  return value === undefined ? [] : Array.isArray(value) ? value : [value];\n}\n","import { APICallError } from '@ai-sdk/provider';\nimport { delay, getErrorMessage, isAbortError } from '@ai-sdk/provider-utils';\nimport { RetryError } from './retry-error';\n\nexport type RetryFunction = <OUTPUT>(\n  fn: () => PromiseLike<OUTPUT>,\n) => PromiseLike<OUTPUT>;\n\nfunction getRetryDelayInMs({\n  error,\n  exponentialBackoffDelay,\n}: {\n  error: APICallError;\n  exponentialBackoffDelay: number;\n}): number {\n  const headers = error.responseHeaders;\n\n  if (!headers) return exponentialBackoffDelay;\n\n  let ms: number | undefined;\n\n  // retry-ms is more precise than retry-after and used by e.g. OpenAI\n  const retryAfterMs = headers['retry-after-ms'];\n  if (retryAfterMs) {\n    const timeoutMs = parseFloat(retryAfterMs);\n    if (!Number.isNaN(timeoutMs)) {\n      ms = timeoutMs;\n    }\n  }\n\n  // About the Retry-After header: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After\n  const retryAfter = headers['retry-after'];\n  if (retryAfter && ms === undefined) {\n    const timeoutSeconds = parseFloat(retryAfter);\n    if (!Number.isNaN(timeoutSeconds)) {\n      ms = timeoutSeconds * 1000;\n    } else {\n      ms = Date.parse(retryAfter) - Date.now();\n    }\n  }\n\n  // check that the delay is reasonable:\n  if (\n    ms != null &&\n    !Number.isNaN(ms) &&\n    0 <= ms &&\n    (ms < 60 * 1000 || ms < exponentialBackoffDelay)\n  ) {\n    return ms;\n  }\n\n  return exponentialBackoffDelay;\n}\n\n/**\nThe `retryWithExponentialBackoffRespectingRetryHeaders` strategy retries a failed API call with an exponential backoff,\nwhile respecting rate limit headers (retry-after-ms and retry-after) if they are provided and reasonable (0-60 seconds).\nYou can configure the maximum number of retries, the initial delay, and the backoff factor.\n */\nexport const retryWithExponentialBackoffRespectingRetryHeaders =\n  ({\n    maxRetries = 2,\n    initialDelayInMs = 2000,\n    backoffFactor = 2,\n    abortSignal,\n  }: {\n    maxRetries?: number;\n    initialDelayInMs?: number;\n    backoffFactor?: number;\n    abortSignal?: AbortSignal;\n  } = {}): RetryFunction =>\n  async <OUTPUT>(f: () => PromiseLike<OUTPUT>) =>\n    _retryWithExponentialBackoff(f, {\n      maxRetries,\n      delayInMs: initialDelayInMs,\n      backoffFactor,\n      abortSignal,\n    });\n\nasync function _retryWithExponentialBackoff<OUTPUT>(\n  f: () => PromiseLike<OUTPUT>,\n  {\n    maxRetries,\n    delayInMs,\n    backoffFactor,\n    abortSignal,\n  }: {\n    maxRetries: number;\n    delayInMs: number;\n    backoffFactor: number;\n    abortSignal: AbortSignal | undefined;\n  },\n  errors: unknown[] = [],\n): Promise<OUTPUT> {\n  try {\n    return await f();\n  } catch (error) {\n    if (isAbortError(error)) {\n      throw error; // don't retry when the request was aborted\n    }\n\n    if (maxRetries === 0) {\n      throw error; // don't wrap the error when retries are disabled\n    }\n\n    const errorMessage = getErrorMessage(error);\n    const newErrors = [...errors, error];\n    const tryNumber = newErrors.length;\n\n    if (tryNumber > maxRetries) {\n      throw new RetryError({\n        message: `Failed after ${tryNumber} attempts. Last error: ${errorMessage}`,\n        reason: 'maxRetriesExceeded',\n        errors: newErrors,\n      });\n    }\n\n    if (\n      error instanceof Error &&\n      APICallError.isInstance(error) &&\n      error.isRetryable === true &&\n      tryNumber <= maxRetries\n    ) {\n      await delay(\n        getRetryDelayInMs({\n          error,\n          exponentialBackoffDelay: delayInMs,\n        }),\n        { abortSignal },\n      );\n\n      return _retryWithExponentialBackoff(\n        f,\n        {\n          maxRetries,\n          delayInMs: backoffFactor * delayInMs,\n          backoffFactor,\n          abortSignal,\n        },\n        newErrors,\n      );\n    }\n\n    if (tryNumber === 1) {\n      throw error; // don't wrap the error when a non-retryable error occurs on the first try\n    }\n\n    throw new RetryError({\n      message: `Failed after ${tryNumber} attempts with non-retryable error: '${errorMessage}'`,\n      reason: 'errorNotRetryable',\n      errors: newErrors,\n    });\n  }\n}\n","import { InvalidArgumentError } from '../error/invalid-argument-error';\nimport {\n  RetryFunction,\n  retryWithExponentialBackoffRespectingRetryHeaders,\n} from '../util/retry-with-exponential-backoff';\n\n/**\n * Validate and prepare retries.\n */\nexport function prepareRetries({\n  maxRetries,\n  abortSignal,\n}: {\n  maxRetries: number | undefined;\n  abortSignal: AbortSignal | undefined;\n}): {\n  maxRetries: number;\n  retry: RetryFunction;\n} {\n  if (maxRetries != null) {\n    if (!Number.isInteger(maxRetries)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be an integer',\n      });\n    }\n\n    if (maxRetries < 0) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be >= 0',\n      });\n    }\n  }\n\n  const maxRetriesResult = maxRetries ?? 2;\n\n  return {\n    maxRetries: maxRetriesResult,\n    retry: retryWithExponentialBackoffRespectingRetryHeaders({\n      maxRetries: maxRetriesResult,\n      abortSignal,\n    }),\n  };\n}\n","import { LanguageModelV2Content, LanguageModelV2Text } from '@ai-sdk/provider';\n\nexport function extractTextContent(\n  content: LanguageModelV2Content[],\n): string | undefined {\n  const parts = content.filter(\n    (content): content is LanguageModelV2Text => content.type === 'text',\n  );\n\n  if (parts.length === 0) {\n    return undefined;\n  }\n\n  return parts.map(content => content.text).join('');\n}\n","import {\n  convertBase64ToUint8Array,\n  convertUint8ArrayToBase64,\n} from '@ai-sdk/provider-utils';\n\n/**\n * A generated file.\n */\nexport interface GeneratedFile {\n  /**\nFile as a base64 encoded string.\n     */\n  readonly base64: string;\n\n  /**\nFile as a Uint8Array.\n     */\n  readonly uint8Array: Uint8Array;\n\n  /**\nThe IANA media type of the file.\n\n@see https://www.iana.org/assignments/media-types/media-types.xhtml\n   */\n  readonly mediaType: string;\n}\n\nexport class DefaultGeneratedFile implements GeneratedFile {\n  private base64Data: string | undefined;\n  private uint8ArrayData: Uint8Array | undefined;\n\n  readonly mediaType: string;\n\n  constructor({\n    data,\n    mediaType,\n  }: {\n    data: string | Uint8Array;\n    mediaType: string;\n  }) {\n    const isUint8Array = data instanceof Uint8Array;\n    this.base64Data = isUint8Array ? undefined : data;\n    this.uint8ArrayData = isUint8Array ? data : undefined;\n    this.mediaType = mediaType;\n  }\n\n  // lazy conversion with caching to avoid unnecessary conversion overhead:\n  get base64() {\n    if (this.base64Data == null) {\n      this.base64Data = convertUint8ArrayToBase64(this.uint8ArrayData!);\n    }\n    return this.base64Data;\n  }\n\n  // lazy conversion with caching to avoid unnecessary conversion overhead:\n  get uint8Array() {\n    if (this.uint8ArrayData == null) {\n      this.uint8ArrayData = convertBase64ToUint8Array(this.base64Data!);\n    }\n    return this.uint8ArrayData;\n  }\n}\n\nexport class DefaultGeneratedFileWithType extends DefaultGeneratedFile {\n  readonly type = 'file';\n\n  constructor(options: { data: string | Uint8Array; mediaType: string }) {\n    super(options);\n  }\n}\n","import { LanguageModelV2ToolCall } from '@ai-sdk/provider';\nimport {\n  asSchema,\n  ModelMessage,\n  safeParseJSON,\n  safeValidateTypes,\n} from '@ai-sdk/provider-utils';\nimport { InvalidToolInputError } from '../error/invalid-tool-input-error';\nimport { NoSuchToolError } from '../error/no-such-tool-error';\nimport { ToolCallRepairError } from '../error/tool-call-repair-error';\nimport { TypedToolCall } from './tool-call';\nimport { ToolCallRepairFunction } from './tool-call-repair-function';\nimport { ToolSet } from './tool-set';\n\nexport async function parseToolCall<TOOLS extends ToolSet>({\n  toolCall,\n  tools,\n  repairToolCall,\n  system,\n  messages,\n}: {\n  toolCall: LanguageModelV2ToolCall;\n  tools: TOOLS | undefined;\n  repairToolCall: ToolCallRepairFunction<TOOLS> | undefined;\n  system: string | undefined;\n  messages: ModelMessage[];\n}): Promise<TypedToolCall<TOOLS>> {\n  try {\n    if (tools == null) {\n      throw new NoSuchToolError({ toolName: toolCall.toolName });\n    }\n\n    try {\n      return await doParseToolCall({ toolCall, tools });\n    } catch (error) {\n      if (\n        repairToolCall == null ||\n        !(\n          NoSuchToolError.isInstance(error) ||\n          InvalidToolInputError.isInstance(error)\n        )\n      ) {\n        throw error;\n      }\n\n      let repairedToolCall: LanguageModelV2ToolCall | null = null;\n\n      try {\n        repairedToolCall = await repairToolCall({\n          toolCall,\n          tools,\n          inputSchema: ({ toolName }) => {\n            const { inputSchema } = tools[toolName];\n            return asSchema(inputSchema).jsonSchema;\n          },\n          system,\n          messages,\n          error,\n        });\n      } catch (repairError) {\n        throw new ToolCallRepairError({\n          cause: repairError,\n          originalError: error,\n        });\n      }\n\n      // no repaired tool call returned\n      if (repairedToolCall == null) {\n        throw error;\n      }\n\n      return await doParseToolCall({ toolCall: repairedToolCall, tools });\n    }\n  } catch (error) {\n    // use parsed input when possible\n    const parsedInput = await safeParseJSON({ text: toolCall.input });\n    const input = parsedInput.success ? parsedInput.value : toolCall.input;\n\n    // TODO AI SDK 6: special invalid tool call parts\n    return {\n      type: 'tool-call',\n      toolCallId: toolCall.toolCallId,\n      toolName: toolCall.toolName,\n      input,\n      dynamic: true,\n      invalid: true,\n      error,\n      providerMetadata: toolCall.providerMetadata,\n    };\n  }\n}\n\nasync function doParseToolCall<TOOLS extends ToolSet>({\n  toolCall,\n  tools,\n}: {\n  toolCall: LanguageModelV2ToolCall;\n  tools: TOOLS;\n}): Promise<TypedToolCall<TOOLS>> {\n  const toolName = toolCall.toolName as keyof TOOLS & string;\n\n  const tool = tools[toolName];\n\n  if (tool == null) {\n    throw new NoSuchToolError({\n      toolName: toolCall.toolName,\n      availableTools: Object.keys(tools),\n    });\n  }\n\n  const schema = asSchema(tool.inputSchema);\n\n  // when the tool call has no arguments, we try passing an empty object to the schema\n  // (many LLMs generate empty strings for tool calls with no arguments)\n  const parseResult =\n    toolCall.input.trim() === ''\n      ? await safeValidateTypes({ value: {}, schema })\n      : await safeParseJSON({ text: toolCall.input, schema });\n\n  if (parseResult.success === false) {\n    throw new InvalidToolInputError({\n      toolName,\n      toolInput: toolCall.input,\n      cause: parseResult.error,\n    });\n  }\n\n  return tool.type === 'dynamic'\n    ? {\n        type: 'tool-call',\n        toolCallId: toolCall.toolCallId,\n        toolName: toolCall.toolName,\n        input: parseResult.value,\n        providerExecuted: toolCall.providerExecuted,\n        providerMetadata: toolCall.providerMetadata,\n        dynamic: true,\n      }\n    : {\n        type: 'tool-call',\n        toolCallId: toolCall.toolCallId,\n        toolName,\n        input: parseResult.value,\n        providerExecuted: toolCall.providerExecuted,\n        providerMetadata: toolCall.providerMetadata,\n      };\n}\n","import { ReasoningPart } from '@ai-sdk/provider-utils';\nimport {\n  CallWarning,\n  FinishReason,\n  LanguageModelRequestMetadata,\n  LanguageModelResponseMetadata,\n  ProviderMetadata,\n} from '../types';\nimport { Source } from '../types/language-model';\nimport { LanguageModelUsage } from '../types/usage';\nimport { ContentPart } from './content-part';\nimport { GeneratedFile } from './generated-file';\nimport { ResponseMessage } from './response-message';\nimport { DynamicToolCall, StaticToolCall, TypedToolCall } from './tool-call';\nimport {\n  DynamicToolResult,\n  StaticToolResult,\n  TypedToolResult,\n} from './tool-result';\nimport { ToolSet } from './tool-set';\n\n/**\n * The result of a single step in the generation process.\n */\nexport type StepResult<TOOLS extends ToolSet> = {\n  /**\nThe content that was generated in the last step.\n   */\n  readonly content: Array<ContentPart<TOOLS>>;\n\n  /**\nThe generated text.\n*/\n  readonly text: string;\n\n  /**\nThe reasoning that was generated during the generation.\n*/\n  readonly reasoning: Array<ReasoningPart>;\n\n  /**\nThe reasoning text that was generated during the generation.\n*/\n  readonly reasoningText: string | undefined;\n\n  /**\nThe files that were generated during the generation.\n*/\n  readonly files: Array<GeneratedFile>;\n\n  /**\nThe sources that were used to generate the text.\n*/\n  readonly sources: Array<Source>;\n\n  /**\nThe tool calls that were made during the generation.\n*/\n  readonly toolCalls: Array<TypedToolCall<TOOLS>>;\n\n  /**\nThe static tool calls that were made in the last step.\n*/\n  readonly staticToolCalls: Array<StaticToolCall<TOOLS>>;\n\n  /**\nThe dynamic tool calls that were made in the last step.\n*/\n  readonly dynamicToolCalls: Array<DynamicToolCall>;\n\n  /**\nThe results of the tool calls.\n*/\n  readonly toolResults: Array<TypedToolResult<TOOLS>>;\n\n  /**\nThe static tool results that were made in the last step.\n*/\n  readonly staticToolResults: Array<StaticToolResult<TOOLS>>;\n\n  /**\nThe dynamic tool results that were made in the last step.\n*/\n  readonly dynamicToolResults: Array<DynamicToolResult>;\n\n  /**\nThe reason why the generation finished.\n*/\n  readonly finishReason: FinishReason;\n\n  /**\nThe token usage of the generated text.\n*/\n  readonly usage: LanguageModelUsage;\n\n  /**\nWarnings from the model provider (e.g. unsupported settings).\n*/\n  readonly warnings: CallWarning[] | undefined;\n\n  /**\nAdditional request information.\n   */\n  readonly request: LanguageModelRequestMetadata;\n\n  /**\nAdditional response information.\n*/\n  readonly response: LanguageModelResponseMetadata & {\n    /**\nThe response messages that were generated during the call.\nResponse messages can be either assistant messages or tool messages.\nThey contain a generated id.\n*/\n    readonly messages: Array<ResponseMessage>;\n\n    /**\nResponse body (available only for providers that use HTTP requests).\n     */\n    body?: unknown;\n  };\n\n  /**\nAdditional provider-specific metadata. They are passed through\nfrom the provider to the AI SDK and enable provider-specific\nresults that can be fully encapsulated in the provider.\n   */\n  readonly providerMetadata: ProviderMetadata | undefined;\n};\n\nexport class DefaultStepResult<TOOLS extends ToolSet>\n  implements StepResult<TOOLS>\n{\n  readonly content: StepResult<TOOLS>['content'];\n  readonly finishReason: StepResult<TOOLS>['finishReason'];\n  readonly usage: StepResult<TOOLS>['usage'];\n  readonly warnings: StepResult<TOOLS>['warnings'];\n  readonly request: StepResult<TOOLS>['request'];\n  readonly response: StepResult<TOOLS>['response'];\n  readonly providerMetadata: StepResult<TOOLS>['providerMetadata'];\n\n  constructor({\n    content,\n    finishReason,\n    usage,\n    warnings,\n    request,\n    response,\n    providerMetadata,\n  }: {\n    content: StepResult<TOOLS>['content'];\n    finishReason: StepResult<TOOLS>['finishReason'];\n    usage: StepResult<TOOLS>['usage'];\n    warnings: StepResult<TOOLS>['warnings'];\n    request: StepResult<TOOLS>['request'];\n    response: StepResult<TOOLS>['response'];\n    providerMetadata: StepResult<TOOLS>['providerMetadata'];\n  }) {\n    this.content = content;\n    this.finishReason = finishReason;\n    this.usage = usage;\n    this.warnings = warnings;\n    this.request = request;\n    this.response = response;\n    this.providerMetadata = providerMetadata;\n  }\n\n  get text() {\n    return this.content\n      .filter(part => part.type === 'text')\n      .map(part => part.text)\n      .join('');\n  }\n\n  get reasoning() {\n    return this.content.filter(part => part.type === 'reasoning');\n  }\n\n  get reasoningText() {\n    return this.reasoning.length === 0\n      ? undefined\n      : this.reasoning.map(part => part.text).join('');\n  }\n\n  get files() {\n    return this.content\n      .filter(part => part.type === 'file')\n      .map(part => part.file);\n  }\n\n  get sources() {\n    return this.content.filter(part => part.type === 'source');\n  }\n\n  get toolCalls() {\n    return this.content.filter(part => part.type === 'tool-call');\n  }\n\n  get staticToolCalls() {\n    return this.toolCalls.filter(\n      (toolCall): toolCall is StaticToolCall<TOOLS> =>\n        toolCall.dynamic !== true,\n    );\n  }\n\n  get dynamicToolCalls() {\n    return this.toolCalls.filter(\n      (toolCall): toolCall is DynamicToolCall => toolCall.dynamic === true,\n    );\n  }\n\n  get toolResults() {\n    return this.content.filter(part => part.type === 'tool-result');\n  }\n\n  get staticToolResults() {\n    return this.toolResults.filter(\n      (toolResult): toolResult is StaticToolResult<TOOLS> =>\n        toolResult.dynamic !== true,\n    );\n  }\n\n  get dynamicToolResults() {\n    return this.toolResults.filter(\n      (toolResult): toolResult is DynamicToolResult =>\n        toolResult.dynamic === true,\n    );\n  }\n}\n","import { StepResult } from './step-result';\nimport { ToolSet } from './tool-set';\n\nexport type StopCondition<TOOLS extends ToolSet> = (options: {\n  steps: Array<StepResult<TOOLS>>;\n}) => PromiseLike<boolean> | boolean;\n\nexport function stepCountIs(stepCount: number): StopCondition<any> {\n  return ({ steps }) => steps.length === stepCount;\n}\n\nexport function hasToolCall(toolName: string): StopCondition<any> {\n  return ({ steps }) =>\n    steps[steps.length - 1]?.toolCalls?.some(\n      toolCall => toolCall.toolName === toolName,\n    ) ?? false;\n}\n\nexport async function isStopConditionMet<TOOLS extends ToolSet>({\n  stopConditions,\n  steps,\n}: {\n  stopConditions: Array<StopCondition<TOOLS>>;\n  steps: Array<StepResult<TOOLS>>;\n}): Promise<boolean> {\n  return (\n    await Promise.all(stopConditions.map(condition => condition({ steps })))\n  ).some(result => result);\n}\n","import {\n  getErrorMessage,\n  JSONValue,\n  LanguageModelV2ToolResultOutput,\n} from '@ai-sdk/provider';\nimport { Tool } from '@ai-sdk/provider-utils';\n\nexport function createToolModelOutput({\n  output,\n  tool,\n  errorMode,\n}: {\n  output: unknown;\n  tool: Tool | undefined;\n  errorMode: 'none' | 'text' | 'json';\n}): LanguageModelV2ToolResultOutput {\n  if (errorMode === 'text') {\n    return { type: 'error-text', value: getErrorMessage(output) };\n  } else if (errorMode === 'json') {\n    return { type: 'error-json', value: toJSONValue(output) };\n  }\n\n  if (tool?.toModelOutput) {\n    return tool.toModelOutput(output);\n  }\n\n  return typeof output === 'string'\n    ? { type: 'text', value: output }\n    : { type: 'json', value: toJSONValue(output) };\n}\n\nfunction toJSONValue(value: unknown): JSONValue {\n  return value === undefined ? null : (value as JSONValue);\n}\n","import {\n  AssistantContent,\n  AssistantModelMessage,\n  ToolContent,\n  ToolModelMessage,\n} from '../prompt';\nimport { createToolModelOutput } from '../prompt/create-tool-model-output';\nimport { ContentPart } from './content-part';\nimport { ToolSet } from './tool-set';\n\n/**\nConverts the result of a `generateText` or `streamText` call to a list of response messages.\n */\nexport function toResponseMessages<TOOLS extends ToolSet>({\n  content: inputContent,\n  tools,\n}: {\n  content: Array<ContentPart<TOOLS>>;\n  tools: TOOLS | undefined;\n}): Array<AssistantModelMessage | ToolModelMessage> {\n  const responseMessages: Array<AssistantModelMessage | ToolModelMessage> = [];\n\n  const content: AssistantContent = inputContent\n    .filter(part => part.type !== 'source')\n    .filter(\n      part =>\n        (part.type !== 'tool-result' || part.providerExecuted) &&\n        (part.type !== 'tool-error' || part.providerExecuted),\n    )\n    .filter(part => part.type !== 'text' || part.text.length > 0)\n    .map(part => {\n      switch (part.type) {\n        case 'text':\n          return {\n            type: 'text',\n            text: part.text,\n            providerOptions: part.providerMetadata,\n          };\n        case 'reasoning':\n          return {\n            type: 'reasoning',\n            text: part.text,\n            providerOptions: part.providerMetadata,\n          };\n        case 'file':\n          return {\n            type: 'file',\n            data: part.file.base64,\n            mediaType: part.file.mediaType,\n            providerOptions: part.providerMetadata,\n          };\n        case 'tool-call':\n          return {\n            type: 'tool-call',\n            toolCallId: part.toolCallId,\n            toolName: part.toolName,\n            input: part.input,\n            providerExecuted: part.providerExecuted,\n            providerOptions: part.providerMetadata,\n          };\n        case 'tool-result':\n          return {\n            type: 'tool-result',\n            toolCallId: part.toolCallId,\n            toolName: part.toolName,\n            output: createToolModelOutput({\n              tool: tools?.[part.toolName],\n              output: part.output,\n              errorMode: 'none',\n            }),\n            providerExecuted: true,\n            providerOptions: part.providerMetadata,\n          };\n        case 'tool-error':\n          return {\n            type: 'tool-result',\n            toolCallId: part.toolCallId,\n            toolName: part.toolName,\n            output: createToolModelOutput({\n              tool: tools?.[part.toolName],\n              output: part.error,\n              errorMode: 'json',\n            }),\n            providerOptions: part.providerMetadata,\n          };\n      }\n    });\n\n  if (content.length > 0) {\n    responseMessages.push({\n      role: 'assistant',\n      content,\n    });\n  }\n\n  const toolResultContent: ToolContent = inputContent\n    .filter(part => part.type === 'tool-result' || part.type === 'tool-error')\n    .filter(part => !part.providerExecuted)\n    .map(toolResult => ({\n      type: 'tool-result',\n      toolCallId: toolResult.toolCallId,\n      toolName: toolResult.toolName,\n      output: createToolModelOutput({\n        tool: tools?.[toolResult.toolName],\n        output:\n          toolResult.type === 'tool-result'\n            ? toolResult.output\n            : toolResult.error,\n        errorMode: toolResult.type === 'tool-error' ? 'text' : 'none',\n      }),\n      ...(toolResult.providerMetadata != null\n        ? { providerOptions: toolResult.providerMetadata }\n        : {}),\n    }));\n\n  if (toolResultContent.length > 0) {\n    responseMessages.push({\n      role: 'tool',\n      content: toolResultContent,\n    });\n  }\n\n  return responseMessages;\n}\n","import {\n  getErrorMessage,\n  LanguageModelV2,\n  LanguageModelV2CallWarning,\n} from '@ai-sdk/provider';\nimport {\n  createIdGenerator,\n  DelayedPromise,\n  IdGenerator,\n  isAbortError,\n  ProviderOptions,\n} from '@ai-sdk/provider-utils';\nimport { Span } from '@opentelemetry/api';\nimport { ServerResponse } from 'node:http';\nimport { NoOutputGeneratedError } from '../error';\nimport { NoOutputSpecifiedError } from '../error/no-output-specified-error';\nimport { logWarnings } from '../logger/log-warnings';\nimport { resolveLanguageModel } from '../model/resolve-model';\nimport { CallSettings } from '../prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../prompt/convert-to-language-model-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { prepareToolsAndToolChoice } from '../prompt/prepare-tools-and-tool-choice';\nimport { Prompt } from '../prompt/prompt';\nimport { standardizePrompt } from '../prompt/standardize-prompt';\nimport { wrapGatewayError } from '../prompt/wrap-gateway-error';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { stringifyForTelemetry } from '../telemetry/stringify-for-telemetry';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { createTextStreamResponse } from '../text-stream/create-text-stream-response';\nimport { pipeTextStreamToResponse } from '../text-stream/pipe-text-stream-to-response';\nimport { LanguageModelRequestMetadata } from '../types';\nimport {\n  CallWarning,\n  FinishReason,\n  LanguageModel,\n  ToolChoice,\n} from '../types/language-model';\nimport { ProviderMetadata } from '../types/provider-metadata';\nimport { addLanguageModelUsage, LanguageModelUsage } from '../types/usage';\nimport { UIMessage } from '../ui';\nimport { createUIMessageStreamResponse } from '../ui-message-stream/create-ui-message-stream-response';\nimport { getResponseUIMessageId } from '../ui-message-stream/get-response-ui-message-id';\nimport { handleUIMessageStreamFinish } from '../ui-message-stream/handle-ui-message-stream-finish';\nimport { pipeUIMessageStreamToResponse } from '../ui-message-stream/pipe-ui-message-stream-to-response';\nimport {\n  InferUIMessageChunk,\n  UIMessageChunk,\n} from '../ui-message-stream/ui-message-chunks';\nimport { UIMessageStreamResponseInit } from '../ui-message-stream/ui-message-stream-response-init';\nimport { InferUIMessageData, InferUIMessageMetadata } from '../ui/ui-messages';\nimport { asArray } from '../util/as-array';\nimport {\n  AsyncIterableStream,\n  createAsyncIterableStream,\n} from '../util/async-iterable-stream';\nimport { consumeStream } from '../util/consume-stream';\nimport { createStitchableStream } from '../util/create-stitchable-stream';\nimport { DownloadFunction } from '../util/download/download-function';\nimport { now as originalNow } from '../util/now';\nimport { prepareRetries } from '../util/prepare-retries';\nimport { ContentPart } from './content-part';\nimport { Output } from './output';\nimport { PrepareStepFunction } from './prepare-step';\nimport { ResponseMessage } from './response-message';\nimport {\n  runToolsTransformation,\n  SingleRequestTextStreamPart,\n} from './run-tools-transformation';\nimport { DefaultStepResult, StepResult } from './step-result';\nimport {\n  isStopConditionMet,\n  stepCountIs,\n  StopCondition,\n} from './stop-condition';\nimport {\n  ConsumeStreamOptions,\n  StreamTextResult,\n  TextStreamPart,\n  UIMessageStreamOptions,\n} from './stream-text-result';\nimport { toResponseMessages } from './to-response-messages';\nimport { TypedToolCall } from './tool-call';\nimport { ToolCallRepairFunction } from './tool-call-repair-function';\nimport { ToolOutput } from './tool-output';\nimport { ToolSet } from './tool-set';\n\nconst originalGenerateId = createIdGenerator({\n  prefix: 'aitxt',\n  size: 24,\n});\n\n/**\nA transformation that is applied to the stream.\n\n@param stopStream - A function that stops the source stream.\n@param tools - The tools that are accessible to and can be called by the model. The model needs to support calling tools.\n */\nexport type StreamTextTransform<TOOLS extends ToolSet> = (options: {\n  tools: TOOLS; // for type inference\n  stopStream: () => void;\n}) => TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>;\n\n/**\nCallback that is set using the `onError` option.\n\n@param event - The event that is passed to the callback.\n */\nexport type StreamTextOnErrorCallback = (event: {\n  error: unknown;\n}) => PromiseLike<void> | void;\n\n/**\nCallback that is set using the `onStepFinish` option.\n\n@param stepResult - The result of the step.\n */\nexport type StreamTextOnStepFinishCallback<TOOLS extends ToolSet> = (\n  stepResult: StepResult<TOOLS>,\n) => PromiseLike<void> | void;\n\n/**\nCallback that is set using the `onChunk` option.\n\n@param event - The event that is passed to the callback.\n */\nexport type StreamTextOnChunkCallback<TOOLS extends ToolSet> = (event: {\n  chunk: Extract<\n    TextStreamPart<TOOLS>,\n    {\n      type:\n        | 'text-delta'\n        | 'reasoning-delta'\n        | 'source'\n        | 'tool-call'\n        | 'tool-input-start'\n        | 'tool-input-delta'\n        | 'tool-result'\n        | 'raw';\n    }\n  >;\n}) => PromiseLike<void> | void;\n\n/**\nCallback that is set using the `onFinish` option.\n\n@param event - The event that is passed to the callback.\n */\nexport type StreamTextOnFinishCallback<TOOLS extends ToolSet> = (\n  event: StepResult<TOOLS> & {\n    /**\nDetails for all steps.\n   */\n    readonly steps: StepResult<TOOLS>[];\n\n    /**\nTotal usage for all steps. This is the sum of the usage of all steps.\n     */\n    readonly totalUsage: LanguageModelUsage;\n  },\n) => PromiseLike<void> | void;\n\n/**\nCallback that is set using the `onAbort` option.\n\n@param event - The event that is passed to the callback.\n */\nexport type StreamTextOnAbortCallback<TOOLS extends ToolSet> = (event: {\n  /**\nDetails for all previously finished steps.\n   */\n  readonly steps: StepResult<TOOLS>[];\n}) => PromiseLike<void> | void;\n\n/**\nGenerate a text and call tools for a given prompt using a language model.\n\nThis function streams the output. If you do not want to stream the output, use `generateText` instead.\n\n@param model - The language model to use.\n@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxOutputTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topK - Only sample from the top K options for each subsequent token.\nUsed to remove \"long tail\" low probability responses.\nRecommended for advanced use cases only. You usually only need to use temperature.\n@param presencePenalty - Presence penalty setting.\nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param stopSequences - Stop sequences.\nIf set, the model will stop generating text when one of the stop sequences is generated.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@param maxSteps - Maximum number of sequential LLM calls (steps), e.g. when you use tool calls.\n\n@param onChunk - Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.\n@param onError - Callback that is called when an error occurs during streaming. You can use it to log errors.\n@param onStepFinish - Callback that is called when each step (LLM call) is finished, including intermediate steps.\n@param onFinish - Callback that is called when the LLM response and all request tool executions\n(for tools that have an `execute` function) are finished.\n\n@return\nA result object for accessing different stream types and additional information.\n */\nexport function streamText<\n  TOOLS extends ToolSet,\n  OUTPUT = never,\n  PARTIAL_OUTPUT = never,\n>({\n  model,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  stopWhen = stepCountIs(1),\n  experimental_output: output,\n  experimental_telemetry: telemetry,\n  prepareStep,\n  providerOptions,\n  experimental_activeTools,\n  activeTools = experimental_activeTools,\n  experimental_repairToolCall: repairToolCall,\n  experimental_transform: transform,\n  experimental_download: download,\n  includeRawChunks = false,\n  onChunk,\n  onError = ({ error }) => {\n    console.error(error);\n  },\n  onFinish,\n  onAbort,\n  onStepFinish,\n  experimental_context,\n  _internal: {\n    now = originalNow,\n    generateId = originalGenerateId,\n    currentDate = () => new Date(),\n  } = {},\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe tools that the model can call. The model needs to support calling tools.\n    */\n    tools?: TOOLS;\n\n    /**\nThe tool choice strategy. Default: 'auto'.\n     */\n    toolChoice?: ToolChoice<TOOLS>;\n\n    /**\nCondition for stopping the generation when there are tool results in the last step.\nWhen the condition is an array, any of the conditions can be met to stop the generation.\n\n@default stepCountIs(1)\n     */\n    stopWhen?:\n      | StopCondition<NoInfer<TOOLS>>\n      | Array<StopCondition<NoInfer<TOOLS>>>;\n\n    /**\nOptional telemetry configuration (experimental).\n     */\n    experimental_telemetry?: TelemetrySettings;\n\n    /**\nAdditional provider-specific options. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n    providerOptions?: ProviderOptions;\n\n    /**\n     * @deprecated Use `activeTools` instead.\n     */\n    experimental_activeTools?: Array<keyof NoInfer<TOOLS>>;\n\n    /**\n   Limits the tools that are available for the model to call without\n   changing the tool call and result types in the result.\n        */\n    activeTools?: Array<keyof NoInfer<TOOLS>>;\n\n    /**\nOptional specification for parsing structured outputs from the LLM response.\n     */\n    experimental_output?: Output<OUTPUT, PARTIAL_OUTPUT>;\n\n    /**\nOptional function that you can use to provide different settings for a step.\n\n@param options - The options for the step.\n@param options.steps - The steps that have been executed so far.\n@param options.stepNumber - The number of the step that is being executed.\n@param options.model - The model that is being used.\n\n@returns An object that contains the settings for the step.\nIf you return undefined (or for undefined settings), the settings from the outer level will be used.\n    */\n    prepareStep?: PrepareStepFunction<NoInfer<TOOLS>>;\n\n    /**\nA function that attempts to repair a tool call that failed to parse.\n     */\n    experimental_repairToolCall?: ToolCallRepairFunction<TOOLS>;\n\n    /**\nOptional stream transformations.\nThey are applied in the order they are provided.\nThe stream transformations must maintain the stream structure for streamText to work correctly.\n     */\n    experimental_transform?:\n      | StreamTextTransform<TOOLS>\n      | Array<StreamTextTransform<TOOLS>>;\n\n    /**\nCustom download function to use for URLs.\n\nBy default, files are downloaded if the model does not support the URL for the given media type.\n     */\n    experimental_download?: DownloadFunction | undefined;\n\n    /**\nWhether to include raw chunks from the provider in the stream.\nWhen enabled, you will receive raw chunks with type 'raw' that contain the unprocessed data from the provider.\nThis allows access to cutting-edge provider features not yet wrapped by the AI SDK.\nDefaults to false.\n     */\n    includeRawChunks?: boolean;\n\n    /**\nCallback that is called for each chunk of the stream.\nThe stream processing will pause until the callback promise is resolved.\n     */\n    onChunk?: StreamTextOnChunkCallback<TOOLS>;\n\n    /**\nCallback that is invoked when an error occurs during streaming.\nYou can use it to log errors.\nThe stream processing will pause until the callback promise is resolved.\n     */\n    onError?: StreamTextOnErrorCallback;\n\n    /**\nCallback that is called when the LLM response and all request tool executions\n(for tools that have an `execute` function) are finished.\n\nThe usage is the combined usage of all steps.\n     */\n    onFinish?: StreamTextOnFinishCallback<TOOLS>;\n\n    onAbort?: StreamTextOnAbortCallback<TOOLS>;\n\n    /**\nCallback that is called when each step (LLM call) is finished, including intermediate steps.\n    */\n    onStepFinish?: StreamTextOnStepFinishCallback<TOOLS>;\n\n    /**\n     * Context that is passed into tool execution.\n     *\n     * Experimental (can break in patch releases).\n     *\n     * @default undefined\n     */\n    experimental_context?: unknown;\n\n    /**\nInternal. For test use only. May change without notice.\n     */\n    _internal?: {\n      now?: () => number;\n      generateId?: IdGenerator;\n      currentDate?: () => Date;\n    };\n  }): StreamTextResult<TOOLS, PARTIAL_OUTPUT> {\n  return new DefaultStreamTextResult<TOOLS, OUTPUT, PARTIAL_OUTPUT>({\n    model: resolveLanguageModel(model),\n    telemetry,\n    headers,\n    settings,\n    maxRetries,\n    abortSignal,\n    system,\n    prompt,\n    messages,\n    tools,\n    toolChoice,\n    transforms: asArray(transform),\n    activeTools,\n    repairToolCall,\n    stopConditions: asArray(stopWhen),\n    output,\n    providerOptions,\n    prepareStep,\n    includeRawChunks,\n    onChunk,\n    onError,\n    onFinish,\n    onAbort,\n    onStepFinish,\n    now,\n    currentDate,\n    generateId,\n    experimental_context,\n    download,\n  });\n}\n\ntype EnrichedStreamPart<TOOLS extends ToolSet, PARTIAL_OUTPUT> = {\n  part: TextStreamPart<TOOLS>;\n  partialOutput: PARTIAL_OUTPUT | undefined;\n};\n\nfunction createOutputTransformStream<\n  TOOLS extends ToolSet,\n  OUTPUT,\n  PARTIAL_OUTPUT,\n>(\n  output: Output<OUTPUT, PARTIAL_OUTPUT> | undefined,\n): TransformStream<\n  TextStreamPart<TOOLS>,\n  EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>\n> {\n  if (!output) {\n    return new TransformStream<\n      TextStreamPart<TOOLS>,\n      EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>\n    >({\n      transform(chunk, controller) {\n        controller.enqueue({ part: chunk, partialOutput: undefined });\n      },\n    });\n  }\n\n  let firstTextChunkId: string | undefined = undefined;\n  let text = '';\n  let textChunk = '';\n  let lastPublishedJson = '';\n\n  function publishTextChunk({\n    controller,\n    partialOutput = undefined,\n  }: {\n    controller: TransformStreamDefaultController<\n      EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>\n    >;\n    partialOutput?: PARTIAL_OUTPUT;\n  }) {\n    controller.enqueue({\n      part: {\n        type: 'text-delta',\n        id: firstTextChunkId!,\n        text: textChunk,\n      },\n      partialOutput,\n    });\n    textChunk = '';\n  }\n\n  return new TransformStream<\n    TextStreamPart<TOOLS>,\n    EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>\n  >({\n    async transform(chunk, controller) {\n      // ensure that we publish the last text chunk before the step finish:\n      if (chunk.type === 'finish-step' && textChunk.length > 0) {\n        publishTextChunk({ controller });\n      }\n\n      if (\n        chunk.type !== 'text-delta' &&\n        chunk.type !== 'text-start' &&\n        chunk.type !== 'text-end'\n      ) {\n        controller.enqueue({ part: chunk, partialOutput: undefined });\n        return;\n      }\n\n      // we have to pick a text chunk which contains the json text\n      // since we are streaming, we have to pick the first text chunk\n      if (firstTextChunkId == null) {\n        firstTextChunkId = chunk.id;\n      } else if (chunk.id !== firstTextChunkId) {\n        controller.enqueue({ part: chunk, partialOutput: undefined });\n        return;\n      }\n\n      if (chunk.type === 'text-start') {\n        controller.enqueue({ part: chunk, partialOutput: undefined });\n        return;\n      }\n\n      if (chunk.type === 'text-end') {\n        if (textChunk.length > 0) {\n          publishTextChunk({ controller });\n        }\n        controller.enqueue({ part: chunk, partialOutput: undefined });\n        return;\n      }\n\n      text += chunk.text;\n      textChunk += chunk.text;\n\n      // only publish if partial json can be parsed:\n      const result = await output.parsePartial({ text });\n      if (result != null) {\n        // only send new json if it has changed:\n        const currentJson = JSON.stringify(result.partial);\n        if (currentJson !== lastPublishedJson) {\n          publishTextChunk({ controller, partialOutput: result.partial });\n          lastPublishedJson = currentJson;\n        }\n      }\n    },\n  });\n}\n\nclass DefaultStreamTextResult<TOOLS extends ToolSet, OUTPUT, PARTIAL_OUTPUT>\n  implements StreamTextResult<TOOLS, PARTIAL_OUTPUT>\n{\n  private readonly _totalUsage = new DelayedPromise<\n    Awaited<StreamTextResult<TOOLS, PARTIAL_OUTPUT>['usage']>\n  >();\n  private readonly _finishReason = new DelayedPromise<\n    Awaited<StreamTextResult<TOOLS, PARTIAL_OUTPUT>['finishReason']>\n  >();\n  private readonly _steps = new DelayedPromise<\n    Awaited<StreamTextResult<TOOLS, PARTIAL_OUTPUT>['steps']>\n  >();\n\n  private readonly addStream: (\n    stream: ReadableStream<TextStreamPart<TOOLS>>,\n  ) => void;\n\n  private readonly closeStream: () => void;\n\n  private baseStream: ReadableStream<EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>>;\n\n  private output: Output<OUTPUT, PARTIAL_OUTPUT> | undefined;\n\n  private includeRawChunks: boolean;\n\n  private tools: TOOLS | undefined;\n\n  constructor({\n    model,\n    telemetry,\n    headers,\n    settings,\n    maxRetries: maxRetriesArg,\n    abortSignal,\n    system,\n    prompt,\n    messages,\n    tools,\n    toolChoice,\n    transforms,\n    activeTools,\n    repairToolCall,\n    stopConditions,\n    output,\n    providerOptions,\n    prepareStep,\n    includeRawChunks,\n    now,\n    currentDate,\n    generateId,\n    onChunk,\n    onError,\n    onFinish,\n    onAbort,\n    onStepFinish,\n    experimental_context,\n    download,\n  }: {\n    model: LanguageModelV2;\n    telemetry: TelemetrySettings | undefined;\n    headers: Record<string, string | undefined> | undefined;\n    settings: Omit<CallSettings, 'abortSignal' | 'headers'>;\n    maxRetries: number | undefined;\n    abortSignal: AbortSignal | undefined;\n    system: Prompt['system'];\n    prompt: Prompt['prompt'];\n    messages: Prompt['messages'];\n    tools: TOOLS | undefined;\n    toolChoice: ToolChoice<TOOLS> | undefined;\n    transforms: Array<StreamTextTransform<TOOLS>>;\n    activeTools: Array<keyof TOOLS> | undefined;\n    repairToolCall: ToolCallRepairFunction<TOOLS> | undefined;\n    stopConditions: Array<StopCondition<NoInfer<TOOLS>>>;\n    output: Output<OUTPUT, PARTIAL_OUTPUT> | undefined;\n    providerOptions: ProviderOptions | undefined;\n    prepareStep: PrepareStepFunction<NoInfer<TOOLS>> | undefined;\n    includeRawChunks: boolean;\n    now: () => number;\n    currentDate: () => Date;\n    generateId: () => string;\n    experimental_context: unknown;\n    download: DownloadFunction | undefined;\n\n    // callbacks:\n    onChunk: undefined | StreamTextOnChunkCallback<TOOLS>;\n    onError: StreamTextOnErrorCallback;\n    onFinish: undefined | StreamTextOnFinishCallback<TOOLS>;\n    onAbort: undefined | StreamTextOnAbortCallback<TOOLS>;\n    onStepFinish: undefined | StreamTextOnStepFinishCallback<TOOLS>;\n  }) {\n    this.output = output;\n    this.includeRawChunks = includeRawChunks;\n    this.tools = tools;\n\n    // promise to ensure that the step has been fully processed by the event processor\n    // before a new step is started. This is required because the continuation condition\n    // needs the updated steps to determine if another step is needed.\n    let stepFinish!: DelayedPromise<void>;\n\n    let recordedContent: Array<ContentPart<TOOLS>> = [];\n    const recordedResponseMessages: Array<ResponseMessage> = [];\n    let recordedFinishReason: FinishReason | undefined = undefined;\n    let recordedTotalUsage: LanguageModelUsage | undefined = undefined;\n    let recordedRequest: LanguageModelRequestMetadata = {};\n    let recordedWarnings: Array<CallWarning> = [];\n    const recordedSteps: StepResult<TOOLS>[] = [];\n\n    let rootSpan!: Span;\n\n    let activeTextContent: Record<\n      string,\n      {\n        type: 'text';\n        text: string;\n        providerMetadata: ProviderMetadata | undefined;\n      }\n    > = {};\n\n    let activeReasoningContent: Record<\n      string,\n      {\n        type: 'reasoning';\n        text: string;\n        providerMetadata: ProviderMetadata | undefined;\n      }\n    > = {};\n\n    const eventProcessor = new TransformStream<\n      EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>,\n      EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>\n    >({\n      async transform(chunk, controller) {\n        controller.enqueue(chunk); // forward the chunk to the next stream\n\n        const { part } = chunk;\n\n        if (\n          part.type === 'text-delta' ||\n          part.type === 'reasoning-delta' ||\n          part.type === 'source' ||\n          part.type === 'tool-call' ||\n          part.type === 'tool-result' ||\n          part.type === 'tool-input-start' ||\n          part.type === 'tool-input-delta' ||\n          part.type === 'raw'\n        ) {\n          await onChunk?.({ chunk: part });\n        }\n\n        if (part.type === 'error') {\n          await onError({ error: wrapGatewayError(part.error) });\n        }\n\n        if (part.type === 'text-start') {\n          activeTextContent[part.id] = {\n            type: 'text',\n            text: '',\n            providerMetadata: part.providerMetadata,\n          };\n\n          recordedContent.push(activeTextContent[part.id]);\n        }\n\n        if (part.type === 'text-delta') {\n          const activeText = activeTextContent[part.id];\n\n          if (activeText == null) {\n            controller.enqueue({\n              part: {\n                type: 'error',\n                error: `text part ${part.id} not found`,\n              },\n              partialOutput: undefined,\n            });\n            return;\n          }\n\n          activeText.text += part.text;\n          activeText.providerMetadata =\n            part.providerMetadata ?? activeText.providerMetadata;\n        }\n\n        if (part.type === 'text-end') {\n          const activeText = activeTextContent[part.id];\n\n          if (activeText == null) {\n            controller.enqueue({\n              part: {\n                type: 'error',\n                error: `text part ${part.id} not found`,\n              },\n              partialOutput: undefined,\n            });\n            return;\n          }\n\n          activeText.providerMetadata =\n            part.providerMetadata ?? activeText.providerMetadata;\n\n          delete activeTextContent[part.id];\n        }\n\n        if (part.type === 'reasoning-start') {\n          activeReasoningContent[part.id] = {\n            type: 'reasoning',\n            text: '',\n            providerMetadata: part.providerMetadata,\n          };\n\n          recordedContent.push(activeReasoningContent[part.id]);\n        }\n\n        if (part.type === 'reasoning-delta') {\n          const activeReasoning = activeReasoningContent[part.id];\n\n          if (activeReasoning == null) {\n            controller.enqueue({\n              part: {\n                type: 'error',\n                error: `reasoning part ${part.id} not found`,\n              },\n              partialOutput: undefined,\n            });\n            return;\n          }\n\n          activeReasoning.text += part.text;\n          activeReasoning.providerMetadata =\n            part.providerMetadata ?? activeReasoning.providerMetadata;\n        }\n\n        if (part.type === 'reasoning-end') {\n          const activeReasoning = activeReasoningContent[part.id];\n\n          if (activeReasoning == null) {\n            controller.enqueue({\n              part: {\n                type: 'error',\n                error: `reasoning part ${part.id} not found`,\n              },\n              partialOutput: undefined,\n            });\n            return;\n          }\n\n          activeReasoning.providerMetadata =\n            part.providerMetadata ?? activeReasoning.providerMetadata;\n\n          delete activeReasoningContent[part.id];\n        }\n\n        if (part.type === 'file') {\n          recordedContent.push({ type: 'file', file: part.file });\n        }\n\n        if (part.type === 'source') {\n          recordedContent.push(part);\n        }\n\n        if (part.type === 'tool-call') {\n          recordedContent.push(part);\n        }\n\n        if (part.type === 'tool-result' && !part.preliminary) {\n          recordedContent.push(part);\n        }\n\n        if (part.type === 'tool-error') {\n          recordedContent.push(part);\n        }\n\n        if (part.type === 'start-step') {\n          recordedRequest = part.request;\n          recordedWarnings = part.warnings;\n        }\n\n        if (part.type === 'finish-step') {\n          const stepMessages = toResponseMessages({\n            content: recordedContent,\n            tools,\n          });\n\n          // Add step information (after response messages are updated):\n          const currentStepResult: StepResult<TOOLS> = new DefaultStepResult({\n            content: recordedContent,\n            finishReason: part.finishReason,\n            usage: part.usage,\n            warnings: recordedWarnings,\n            request: recordedRequest,\n            response: {\n              ...part.response,\n              messages: [...recordedResponseMessages, ...stepMessages],\n            },\n            providerMetadata: part.providerMetadata,\n          });\n\n          await onStepFinish?.(currentStepResult);\n\n          logWarnings(recordedWarnings);\n\n          recordedSteps.push(currentStepResult);\n\n          recordedContent = [];\n          activeReasoningContent = {};\n          activeTextContent = {};\n\n          recordedResponseMessages.push(...stepMessages);\n\n          // resolve the promise to signal that the step has been fully processed\n          // by the event processor:\n          stepFinish.resolve();\n        }\n\n        if (part.type === 'finish') {\n          recordedTotalUsage = part.totalUsage;\n          recordedFinishReason = part.finishReason;\n        }\n      },\n\n      async flush(controller) {\n        try {\n          if (recordedSteps.length === 0) {\n            const error = new NoOutputGeneratedError({\n              message: 'No output generated. Check the stream for errors.',\n            });\n\n            self._finishReason.reject(error);\n            self._totalUsage.reject(error);\n            self._steps.reject(error);\n\n            return; // no steps recorded (e.g. in error scenario)\n          }\n\n          // derived:\n          const finishReason = recordedFinishReason ?? 'unknown';\n          const totalUsage = recordedTotalUsage ?? {\n            inputTokens: undefined,\n            outputTokens: undefined,\n            totalTokens: undefined,\n          };\n\n          // from finish:\n          self._finishReason.resolve(finishReason);\n          self._totalUsage.resolve(totalUsage);\n\n          // aggregate results:\n          self._steps.resolve(recordedSteps);\n\n          // call onFinish callback:\n          const finalStep = recordedSteps[recordedSteps.length - 1];\n          await onFinish?.({\n            finishReason,\n            totalUsage,\n            usage: finalStep.usage,\n            content: finalStep.content,\n            text: finalStep.text,\n            reasoningText: finalStep.reasoningText,\n            reasoning: finalStep.reasoning,\n            files: finalStep.files,\n            sources: finalStep.sources,\n            toolCalls: finalStep.toolCalls,\n            staticToolCalls: finalStep.staticToolCalls,\n            dynamicToolCalls: finalStep.dynamicToolCalls,\n            toolResults: finalStep.toolResults,\n            staticToolResults: finalStep.staticToolResults,\n            dynamicToolResults: finalStep.dynamicToolResults,\n            request: finalStep.request,\n            response: finalStep.response,\n            warnings: finalStep.warnings,\n            providerMetadata: finalStep.providerMetadata,\n            steps: recordedSteps,\n          });\n\n          // Add response information to the root span:\n          rootSpan.setAttributes(\n            selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                'ai.response.finishReason': finishReason,\n                'ai.response.text': { output: () => finalStep.text },\n                'ai.response.toolCalls': {\n                  output: () =>\n                    finalStep.toolCalls?.length\n                      ? JSON.stringify(finalStep.toolCalls)\n                      : undefined,\n                },\n                'ai.response.providerMetadata': JSON.stringify(\n                  finalStep.providerMetadata,\n                ),\n\n                'ai.usage.inputTokens': totalUsage.inputTokens,\n                'ai.usage.outputTokens': totalUsage.outputTokens,\n                'ai.usage.totalTokens': totalUsage.totalTokens,\n                'ai.usage.reasoningTokens': totalUsage.reasoningTokens,\n                'ai.usage.cachedInputTokens': totalUsage.cachedInputTokens,\n              },\n            }),\n          );\n        } catch (error) {\n          controller.error(error);\n        } finally {\n          rootSpan.end();\n        }\n      },\n    });\n\n    // initialize the stitchable stream and the transformed stream:\n    const stitchableStream = createStitchableStream<TextStreamPart<TOOLS>>();\n    this.addStream = stitchableStream.addStream;\n    this.closeStream = stitchableStream.close;\n\n    // resilient stream that handles abort signals and errors:\n    const reader = stitchableStream.stream.getReader();\n    let stream = new ReadableStream<TextStreamPart<TOOLS>>({\n      async start(controller) {\n        // send start event:\n        controller.enqueue({ type: 'start' });\n      },\n\n      async pull(controller) {\n        // abort handling:\n        function abort() {\n          onAbort?.({ steps: recordedSteps });\n          controller.enqueue({ type: 'abort' });\n          controller.close();\n        }\n\n        try {\n          const { done, value } = await reader.read();\n\n          if (done) {\n            controller.close();\n            return;\n          }\n\n          if (abortSignal?.aborted) {\n            abort();\n            return;\n          }\n\n          controller.enqueue(value);\n        } catch (error) {\n          if (isAbortError(error) && abortSignal?.aborted) {\n            abort();\n          } else {\n            controller.error(error);\n          }\n        }\n      },\n\n      cancel(reason) {\n        return stitchableStream.stream.cancel(reason);\n      },\n    });\n\n    // transform the stream before output parsing\n    // to enable replacement of stream segments:\n    for (const transform of transforms) {\n      stream = stream.pipeThrough(\n        transform({\n          tools: tools as TOOLS,\n          stopStream() {\n            stitchableStream.terminate();\n          },\n        }),\n      );\n    }\n\n    this.baseStream = stream\n      .pipeThrough(createOutputTransformStream(output))\n      .pipeThrough(eventProcessor);\n\n    const { maxRetries, retry } = prepareRetries({\n      maxRetries: maxRetriesArg,\n      abortSignal,\n    });\n\n    const tracer = getTracer(telemetry);\n\n    const callSettings = prepareCallSettings(settings);\n\n    const baseTelemetryAttributes = getBaseTelemetryAttributes({\n      model,\n      telemetry,\n      headers,\n      settings: { ...callSettings, maxRetries },\n    });\n\n    const self = this;\n\n    recordSpan({\n      name: 'ai.streamText',\n      attributes: selectTelemetryAttributes({\n        telemetry,\n        attributes: {\n          ...assembleOperationName({ operationId: 'ai.streamText', telemetry }),\n          ...baseTelemetryAttributes,\n          // specific settings that only make sense on the outer level:\n          'ai.prompt': {\n            input: () => JSON.stringify({ system, prompt, messages }),\n          },\n        },\n      }),\n      tracer,\n      endWhenDone: false,\n      fn: async rootSpanArg => {\n        rootSpan = rootSpanArg;\n\n        async function streamStep({\n          currentStep,\n          responseMessages,\n          usage,\n        }: {\n          currentStep: number;\n          responseMessages: Array<ResponseMessage>;\n          usage: LanguageModelUsage;\n        }) {\n          const includeRawChunks = self.includeRawChunks;\n\n          stepFinish = new DelayedPromise<void>();\n\n          const initialPrompt = await standardizePrompt({\n            system,\n            prompt,\n            messages,\n          } as Prompt);\n\n          const stepInputMessages = [\n            ...initialPrompt.messages,\n            ...responseMessages,\n          ];\n\n          const prepareStepResult = await prepareStep?.({\n            model,\n            steps: recordedSteps,\n            stepNumber: recordedSteps.length,\n            messages: stepInputMessages,\n          });\n\n          const stepModel = resolveLanguageModel(\n            prepareStepResult?.model ?? model,\n          );\n\n          const promptMessages = await convertToLanguageModelPrompt({\n            prompt: {\n              system: prepareStepResult?.system ?? initialPrompt.system,\n              messages: prepareStepResult?.messages ?? stepInputMessages,\n            },\n            supportedUrls: await stepModel.supportedUrls,\n            download,\n          });\n\n          const { toolChoice: stepToolChoice, tools: stepTools } =\n            prepareToolsAndToolChoice({\n              tools,\n              toolChoice: prepareStepResult?.toolChoice ?? toolChoice,\n              activeTools: prepareStepResult?.activeTools ?? activeTools,\n            });\n\n          const {\n            result: { stream, response, request },\n            doStreamSpan,\n            startTimestampMs,\n          } = await retry(() =>\n            recordSpan({\n              name: 'ai.streamText.doStream',\n              attributes: selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  ...assembleOperationName({\n                    operationId: 'ai.streamText.doStream',\n                    telemetry,\n                  }),\n                  ...baseTelemetryAttributes,\n                  // model:\n                  'ai.model.provider': stepModel.provider,\n                  'ai.model.id': stepModel.modelId,\n                  // prompt:\n                  'ai.prompt.messages': {\n                    input: () => stringifyForTelemetry(promptMessages),\n                  },\n                  'ai.prompt.tools': {\n                    // convert the language model level tools:\n                    input: () => stepTools?.map(tool => JSON.stringify(tool)),\n                  },\n                  'ai.prompt.toolChoice': {\n                    input: () =>\n                      stepToolChoice != null\n                        ? JSON.stringify(stepToolChoice)\n                        : undefined,\n                  },\n\n                  // standardized gen-ai llm span attributes:\n                  'gen_ai.system': stepModel.provider,\n                  'gen_ai.request.model': stepModel.modelId,\n                  'gen_ai.request.frequency_penalty':\n                    callSettings.frequencyPenalty,\n                  'gen_ai.request.max_tokens': callSettings.maxOutputTokens,\n                  'gen_ai.request.presence_penalty':\n                    callSettings.presencePenalty,\n                  'gen_ai.request.stop_sequences': callSettings.stopSequences,\n                  'gen_ai.request.temperature': callSettings.temperature,\n                  'gen_ai.request.top_k': callSettings.topK,\n                  'gen_ai.request.top_p': callSettings.topP,\n                },\n              }),\n              tracer,\n              endWhenDone: false,\n              fn: async doStreamSpan => {\n                return {\n                  startTimestampMs: now(), // get before the call\n                  doStreamSpan,\n                  result: await stepModel.doStream({\n                    ...callSettings,\n                    tools: stepTools,\n                    toolChoice: stepToolChoice,\n                    responseFormat: output?.responseFormat,\n                    prompt: promptMessages,\n                    providerOptions,\n                    abortSignal,\n                    headers,\n                    includeRawChunks,\n                  }),\n                };\n              },\n            }),\n          );\n\n          const streamWithToolResults = runToolsTransformation({\n            tools,\n            generatorStream: stream,\n            tracer,\n            telemetry,\n            system,\n            messages: stepInputMessages,\n            repairToolCall,\n            abortSignal,\n            experimental_context,\n          });\n\n          const stepRequest = request ?? {};\n          const stepToolCalls: TypedToolCall<TOOLS>[] = [];\n          const stepToolOutputs: ToolOutput<TOOLS>[] = [];\n          let warnings: LanguageModelV2CallWarning[] | undefined;\n\n          const activeToolCallToolNames: Record<string, string> = {};\n\n          let stepFinishReason: FinishReason = 'unknown';\n          let stepUsage: LanguageModelUsage = {\n            inputTokens: undefined,\n            outputTokens: undefined,\n            totalTokens: undefined,\n          };\n          let stepProviderMetadata: ProviderMetadata | undefined;\n          let stepFirstChunk = true;\n          let stepResponse: { id: string; timestamp: Date; modelId: string } = {\n            id: generateId(),\n            timestamp: currentDate(),\n            modelId: model.modelId,\n          };\n\n          // raw text as it comes from the provider. recorded for telemetry.\n          let activeText = '';\n\n          self.addStream(\n            streamWithToolResults.pipeThrough(\n              new TransformStream<\n                SingleRequestTextStreamPart<TOOLS>,\n                TextStreamPart<TOOLS>\n              >({\n                async transform(chunk, controller): Promise<void> {\n                  if (chunk.type === 'stream-start') {\n                    warnings = chunk.warnings;\n                    return; // stream start chunks are sent immediately and do not count as first chunk\n                  }\n\n                  if (stepFirstChunk) {\n                    // Telemetry for first chunk:\n                    const msToFirstChunk = now() - startTimestampMs;\n\n                    stepFirstChunk = false;\n\n                    doStreamSpan.addEvent('ai.stream.firstChunk', {\n                      'ai.response.msToFirstChunk': msToFirstChunk,\n                    });\n\n                    doStreamSpan.setAttributes({\n                      'ai.response.msToFirstChunk': msToFirstChunk,\n                    });\n\n                    // Step start:\n                    controller.enqueue({\n                      type: 'start-step',\n                      request: stepRequest,\n                      warnings: warnings ?? [],\n                    });\n                  }\n\n                  const chunkType = chunk.type;\n                  switch (chunkType) {\n                    case 'text-start':\n                    case 'text-end': {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n\n                    case 'text-delta': {\n                      if (chunk.delta.length > 0) {\n                        controller.enqueue({\n                          type: 'text-delta',\n                          id: chunk.id,\n                          text: chunk.delta,\n                          providerMetadata: chunk.providerMetadata,\n                        });\n                        activeText += chunk.delta;\n                      }\n                      break;\n                    }\n\n                    case 'reasoning-start':\n                    case 'reasoning-end': {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n\n                    case 'reasoning-delta': {\n                      controller.enqueue({\n                        type: 'reasoning-delta',\n                        id: chunk.id,\n                        text: chunk.delta,\n                        providerMetadata: chunk.providerMetadata,\n                      });\n                      break;\n                    }\n\n                    case 'tool-call': {\n                      controller.enqueue(chunk);\n                      // store tool calls for onFinish callback and toolCalls promise:\n                      stepToolCalls.push(chunk);\n                      break;\n                    }\n\n                    case 'tool-result': {\n                      controller.enqueue(chunk);\n\n                      if (!chunk.preliminary) {\n                        stepToolOutputs.push(chunk);\n                      }\n\n                      break;\n                    }\n\n                    case 'tool-error': {\n                      controller.enqueue(chunk);\n                      stepToolOutputs.push(chunk);\n                      break;\n                    }\n\n                    case 'response-metadata': {\n                      stepResponse = {\n                        id: chunk.id ?? stepResponse.id,\n                        timestamp: chunk.timestamp ?? stepResponse.timestamp,\n                        modelId: chunk.modelId ?? stepResponse.modelId,\n                      };\n                      break;\n                    }\n\n                    case 'finish': {\n                      // Note: tool executions might not be finished yet when the finish event is emitted.\n                      // store usage and finish reason for promises and onFinish callback:\n                      stepUsage = chunk.usage;\n                      stepFinishReason = chunk.finishReason;\n                      stepProviderMetadata = chunk.providerMetadata;\n\n                      // Telemetry for finish event timing\n                      // (since tool executions can take longer and distort calculations)\n                      const msToFinish = now() - startTimestampMs;\n                      doStreamSpan.addEvent('ai.stream.finish');\n                      doStreamSpan.setAttributes({\n                        'ai.response.msToFinish': msToFinish,\n                        'ai.response.avgOutputTokensPerSecond':\n                          (1000 * (stepUsage.outputTokens ?? 0)) / msToFinish,\n                      });\n\n                      break;\n                    }\n\n                    case 'file': {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n\n                    case 'source': {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n\n                    case 'tool-input-start': {\n                      activeToolCallToolNames[chunk.id] = chunk.toolName;\n\n                      const tool = tools?.[chunk.toolName];\n                      if (tool?.onInputStart != null) {\n                        await tool.onInputStart({\n                          toolCallId: chunk.id,\n                          messages: stepInputMessages,\n                          abortSignal,\n                          experimental_context,\n                        });\n                      }\n\n                      controller.enqueue({\n                        ...chunk,\n                        dynamic: tool?.type === 'dynamic',\n                      });\n                      break;\n                    }\n\n                    case 'tool-input-end': {\n                      delete activeToolCallToolNames[chunk.id];\n                      controller.enqueue(chunk);\n                      break;\n                    }\n\n                    case 'tool-input-delta': {\n                      const toolName = activeToolCallToolNames[chunk.id];\n                      const tool = tools?.[toolName];\n\n                      if (tool?.onInputDelta != null) {\n                        await tool.onInputDelta({\n                          inputTextDelta: chunk.delta,\n                          toolCallId: chunk.id,\n                          messages: stepInputMessages,\n                          abortSignal,\n                          experimental_context,\n                        });\n                      }\n\n                      controller.enqueue(chunk);\n                      break;\n                    }\n\n                    case 'error': {\n                      controller.enqueue(chunk);\n                      stepFinishReason = 'error';\n                      break;\n                    }\n\n                    case 'raw': {\n                      if (includeRawChunks) {\n                        controller.enqueue(chunk);\n                      }\n                      break;\n                    }\n\n                    default: {\n                      const exhaustiveCheck: never = chunkType;\n                      throw new Error(`Unknown chunk type: ${exhaustiveCheck}`);\n                    }\n                  }\n                },\n\n                // invoke onFinish callback and resolve toolResults promise when the stream is about to close:\n                async flush(controller) {\n                  const stepToolCallsJson =\n                    stepToolCalls.length > 0\n                      ? JSON.stringify(stepToolCalls)\n                      : undefined;\n\n                  // record telemetry information first to ensure best effort timing\n                  try {\n                    doStreamSpan.setAttributes(\n                      selectTelemetryAttributes({\n                        telemetry,\n                        attributes: {\n                          'ai.response.finishReason': stepFinishReason,\n                          'ai.response.text': {\n                            output: () => activeText,\n                          },\n                          'ai.response.toolCalls': {\n                            output: () => stepToolCallsJson,\n                          },\n                          'ai.response.id': stepResponse.id,\n                          'ai.response.model': stepResponse.modelId,\n                          'ai.response.timestamp':\n                            stepResponse.timestamp.toISOString(),\n                          'ai.response.providerMetadata':\n                            JSON.stringify(stepProviderMetadata),\n\n                          'ai.usage.inputTokens': stepUsage.inputTokens,\n                          'ai.usage.outputTokens': stepUsage.outputTokens,\n                          'ai.usage.totalTokens': stepUsage.totalTokens,\n                          'ai.usage.reasoningTokens': stepUsage.reasoningTokens,\n                          'ai.usage.cachedInputTokens':\n                            stepUsage.cachedInputTokens,\n\n                          // standardized gen-ai llm span attributes:\n                          'gen_ai.response.finish_reasons': [stepFinishReason],\n                          'gen_ai.response.id': stepResponse.id,\n                          'gen_ai.response.model': stepResponse.modelId,\n                          'gen_ai.usage.input_tokens': stepUsage.inputTokens,\n                          'gen_ai.usage.output_tokens': stepUsage.outputTokens,\n                        },\n                      }),\n                    );\n                  } catch (error) {\n                    // ignore error setting telemetry attributes\n                  } finally {\n                    // finish doStreamSpan before other operations for correct timing:\n                    doStreamSpan.end();\n                  }\n\n                  controller.enqueue({\n                    type: 'finish-step',\n                    finishReason: stepFinishReason,\n                    usage: stepUsage,\n                    providerMetadata: stepProviderMetadata,\n                    response: {\n                      ...stepResponse,\n                      headers: response?.headers,\n                    },\n                  });\n\n                  const combinedUsage = addLanguageModelUsage(usage, stepUsage);\n\n                  // wait for the step to be fully processed by the event processor\n                  // to ensure that the recorded steps are complete:\n                  await stepFinish.promise;\n\n                  const clientToolCalls = stepToolCalls.filter(\n                    toolCall => toolCall.providerExecuted !== true,\n                  );\n                  const clientToolOutputs = stepToolOutputs.filter(\n                    toolOutput => toolOutput.providerExecuted !== true,\n                  );\n\n                  if (\n                    clientToolCalls.length > 0 &&\n                    // all current tool calls have outputs (incl. execution errors):\n                    clientToolOutputs.length === clientToolCalls.length &&\n                    // continue until a stop condition is met:\n                    !(await isStopConditionMet({\n                      stopConditions,\n                      steps: recordedSteps,\n                    }))\n                  ) {\n                    // append to messages for the next step:\n                    responseMessages.push(\n                      ...toResponseMessages({\n                        content:\n                          // use transformed content to create the messages for the next step:\n                          recordedSteps[recordedSteps.length - 1].content,\n                        tools,\n                      }),\n                    );\n\n                    try {\n                      await streamStep({\n                        currentStep: currentStep + 1,\n                        responseMessages,\n                        usage: combinedUsage,\n                      });\n                    } catch (error) {\n                      controller.enqueue({\n                        type: 'error',\n                        error,\n                      });\n\n                      self.closeStream();\n                    }\n                  } else {\n                    controller.enqueue({\n                      type: 'finish',\n                      finishReason: stepFinishReason,\n                      totalUsage: combinedUsage,\n                    });\n\n                    self.closeStream(); // close the stitchable stream\n                  }\n                },\n              }),\n            ),\n          );\n        }\n\n        // add the initial stream to the stitchable stream\n        await streamStep({\n          currentStep: 0,\n          responseMessages: [],\n          usage: {\n            inputTokens: undefined,\n            outputTokens: undefined,\n            totalTokens: undefined,\n          },\n        });\n      },\n    }).catch(error => {\n      // add an error stream part and close the streams:\n      self.addStream(\n        new ReadableStream({\n          start(controller) {\n            controller.enqueue({ type: 'error', error });\n            controller.close();\n          },\n        }),\n      );\n      self.closeStream();\n    });\n  }\n\n  get steps() {\n    // when any of the promises are accessed, the stream is consumed\n    // so it resolves without needing to consume the stream separately\n    this.consumeStream();\n\n    return this._steps.promise;\n  }\n\n  private get finalStep() {\n    return this.steps.then(steps => steps[steps.length - 1]);\n  }\n\n  get content() {\n    return this.finalStep.then(step => step.content);\n  }\n\n  get warnings() {\n    return this.finalStep.then(step => step.warnings);\n  }\n\n  get providerMetadata() {\n    return this.finalStep.then(step => step.providerMetadata);\n  }\n\n  get text() {\n    return this.finalStep.then(step => step.text);\n  }\n\n  get reasoningText() {\n    return this.finalStep.then(step => step.reasoningText);\n  }\n\n  get reasoning() {\n    return this.finalStep.then(step => step.reasoning);\n  }\n\n  get sources() {\n    return this.finalStep.then(step => step.sources);\n  }\n\n  get files() {\n    return this.finalStep.then(step => step.files);\n  }\n\n  get toolCalls() {\n    return this.finalStep.then(step => step.toolCalls);\n  }\n\n  get staticToolCalls() {\n    return this.finalStep.then(step => step.staticToolCalls);\n  }\n\n  get dynamicToolCalls() {\n    return this.finalStep.then(step => step.dynamicToolCalls);\n  }\n\n  get toolResults() {\n    return this.finalStep.then(step => step.toolResults);\n  }\n\n  get staticToolResults() {\n    return this.finalStep.then(step => step.staticToolResults);\n  }\n\n  get dynamicToolResults() {\n    return this.finalStep.then(step => step.dynamicToolResults);\n  }\n\n  get usage() {\n    return this.finalStep.then(step => step.usage);\n  }\n\n  get request() {\n    return this.finalStep.then(step => step.request);\n  }\n\n  get response() {\n    return this.finalStep.then(step => step.response);\n  }\n\n  get totalUsage() {\n    // when any of the promises are accessed, the stream is consumed\n    // so it resolves without needing to consume the stream separately\n    this.consumeStream();\n\n    return this._totalUsage.promise;\n  }\n\n  get finishReason() {\n    // when any of the promises are accessed, the stream is consumed\n    // so it resolves without needing to consume the stream separately\n    this.consumeStream();\n\n    return this._finishReason.promise;\n  }\n\n  /**\nSplit out a new stream from the original stream.\nThe original stream is replaced to allow for further splitting,\nsince we do not know how many times the stream will be split.\n\nNote: this leads to buffering the stream content on the server.\nHowever, the LLM results are expected to be small enough to not cause issues.\n   */\n  private teeStream() {\n    const [stream1, stream2] = this.baseStream.tee();\n    this.baseStream = stream2;\n    return stream1;\n  }\n\n  get textStream(): AsyncIterableStream<string> {\n    return createAsyncIterableStream(\n      this.teeStream().pipeThrough(\n        new TransformStream<EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>, string>({\n          transform({ part }, controller) {\n            if (part.type === 'text-delta') {\n              controller.enqueue(part.text);\n            }\n          },\n        }),\n      ),\n    );\n  }\n\n  get fullStream(): AsyncIterableStream<TextStreamPart<TOOLS>> {\n    return createAsyncIterableStream(\n      this.teeStream().pipeThrough(\n        new TransformStream<\n          EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>,\n          TextStreamPart<TOOLS>\n        >({\n          transform({ part }, controller) {\n            controller.enqueue(part);\n          },\n        }),\n      ),\n    );\n  }\n\n  async consumeStream(options?: ConsumeStreamOptions): Promise<void> {\n    try {\n      await consumeStream({\n        stream: this.fullStream,\n        onError: options?.onError,\n      });\n    } catch (error) {\n      options?.onError?.(error);\n    }\n  }\n\n  get experimental_partialOutputStream(): AsyncIterableStream<PARTIAL_OUTPUT> {\n    if (this.output == null) {\n      throw new NoOutputSpecifiedError();\n    }\n\n    return createAsyncIterableStream(\n      this.teeStream().pipeThrough(\n        new TransformStream<\n          EnrichedStreamPart<TOOLS, PARTIAL_OUTPUT>,\n          PARTIAL_OUTPUT\n        >({\n          transform({ partialOutput }, controller) {\n            if (partialOutput != null) {\n              controller.enqueue(partialOutput);\n            }\n          },\n        }),\n      ),\n    );\n  }\n\n  toUIMessageStream<UI_MESSAGE extends UIMessage>({\n    originalMessages,\n    generateMessageId,\n    onFinish,\n    messageMetadata,\n    sendReasoning = true,\n    sendSources = false,\n    sendStart = true,\n    sendFinish = true,\n    onError = getErrorMessage,\n  }: UIMessageStreamOptions<UI_MESSAGE> = {}): AsyncIterableStream<\n    InferUIMessageChunk<UI_MESSAGE>\n  > {\n    const responseMessageId =\n      generateMessageId != null\n        ? getResponseUIMessageId({\n            originalMessages,\n            responseMessageId: generateMessageId,\n          })\n        : undefined;\n\n    const toolNamesByCallId: Record<string, string> = {};\n\n    const isDynamic = (toolCallId: string) => {\n      const toolName = toolNamesByCallId[toolCallId];\n      const dynamic = this.tools?.[toolName]?.type === 'dynamic';\n      return dynamic ? true : undefined; // only send when dynamic to reduce data transfer\n    };\n\n    const baseStream = this.fullStream.pipeThrough(\n      new TransformStream<\n        TextStreamPart<TOOLS>,\n        UIMessageChunk<\n          InferUIMessageMetadata<UI_MESSAGE>,\n          InferUIMessageData<UI_MESSAGE>\n        >\n      >({\n        transform: async (part, controller) => {\n          const messageMetadataValue = messageMetadata?.({ part });\n\n          const partType = part.type;\n          switch (partType) {\n            case 'text-start': {\n              controller.enqueue({\n                type: 'text-start',\n                id: part.id,\n                ...(part.providerMetadata != null\n                  ? { providerMetadata: part.providerMetadata }\n                  : {}),\n              });\n              break;\n            }\n\n            case 'text-delta': {\n              controller.enqueue({\n                type: 'text-delta',\n                id: part.id,\n                delta: part.text,\n                ...(part.providerMetadata != null\n                  ? { providerMetadata: part.providerMetadata }\n                  : {}),\n              });\n              break;\n            }\n\n            case 'text-end': {\n              controller.enqueue({\n                type: 'text-end',\n                id: part.id,\n                ...(part.providerMetadata != null\n                  ? { providerMetadata: part.providerMetadata }\n                  : {}),\n              });\n              break;\n            }\n\n            case 'reasoning-start': {\n              controller.enqueue({\n                type: 'reasoning-start',\n                id: part.id,\n                ...(part.providerMetadata != null\n                  ? { providerMetadata: part.providerMetadata }\n                  : {}),\n              });\n              break;\n            }\n\n            case 'reasoning-delta': {\n              if (sendReasoning) {\n                controller.enqueue({\n                  type: 'reasoning-delta',\n                  id: part.id,\n                  delta: part.text,\n                  ...(part.providerMetadata != null\n                    ? { providerMetadata: part.providerMetadata }\n                    : {}),\n                });\n              }\n              break;\n            }\n\n            case 'reasoning-end': {\n              controller.enqueue({\n                type: 'reasoning-end',\n                id: part.id,\n                ...(part.providerMetadata != null\n                  ? { providerMetadata: part.providerMetadata }\n                  : {}),\n              });\n              break;\n            }\n\n            case 'file': {\n              controller.enqueue({\n                type: 'file',\n                mediaType: part.file.mediaType,\n                url: `data:${part.file.mediaType};base64,${part.file.base64}`,\n              });\n              break;\n            }\n\n            case 'source': {\n              if (sendSources && part.sourceType === 'url') {\n                controller.enqueue({\n                  type: 'source-url',\n                  sourceId: part.id,\n                  url: part.url,\n                  title: part.title,\n                  ...(part.providerMetadata != null\n                    ? { providerMetadata: part.providerMetadata }\n                    : {}),\n                });\n              }\n\n              if (sendSources && part.sourceType === 'document') {\n                controller.enqueue({\n                  type: 'source-document',\n                  sourceId: part.id,\n                  mediaType: part.mediaType,\n                  title: part.title,\n                  filename: part.filename,\n                  ...(part.providerMetadata != null\n                    ? { providerMetadata: part.providerMetadata }\n                    : {}),\n                });\n              }\n              break;\n            }\n\n            case 'tool-input-start': {\n              toolNamesByCallId[part.id] = part.toolName;\n              const dynamic = isDynamic(part.id);\n\n              controller.enqueue({\n                type: 'tool-input-start',\n                toolCallId: part.id,\n                toolName: part.toolName,\n                ...(part.providerExecuted != null\n                  ? { providerExecuted: part.providerExecuted }\n                  : {}),\n                ...(dynamic != null ? { dynamic } : {}),\n              });\n              break;\n            }\n\n            case 'tool-input-delta': {\n              controller.enqueue({\n                type: 'tool-input-delta',\n                toolCallId: part.id,\n                inputTextDelta: part.delta,\n              });\n              break;\n            }\n\n            case 'tool-call': {\n              toolNamesByCallId[part.toolCallId] = part.toolName;\n              const dynamic = isDynamic(part.toolCallId);\n\n              if (part.invalid) {\n                controller.enqueue({\n                  type: 'tool-input-error',\n                  toolCallId: part.toolCallId,\n                  toolName: part.toolName,\n                  input: part.input,\n                  ...(part.providerExecuted != null\n                    ? { providerExecuted: part.providerExecuted }\n                    : {}),\n                  ...(part.providerMetadata != null\n                    ? { providerMetadata: part.providerMetadata }\n                    : {}),\n                  ...(dynamic != null ? { dynamic } : {}),\n                  errorText: onError(part.error),\n                });\n              } else {\n                controller.enqueue({\n                  type: 'tool-input-available',\n                  toolCallId: part.toolCallId,\n                  toolName: part.toolName,\n                  input: part.input,\n                  ...(part.providerExecuted != null\n                    ? { providerExecuted: part.providerExecuted }\n                    : {}),\n                  ...(part.providerMetadata != null\n                    ? { providerMetadata: part.providerMetadata }\n                    : {}),\n                  ...(dynamic != null ? { dynamic } : {}),\n                });\n              }\n\n              break;\n            }\n\n            case 'tool-result': {\n              const dynamic = isDynamic(part.toolCallId);\n\n              controller.enqueue({\n                type: 'tool-output-available',\n                toolCallId: part.toolCallId,\n                output: part.output,\n                ...(part.providerExecuted != null\n                  ? { providerExecuted: part.providerExecuted }\n                  : {}),\n                ...(part.preliminary != null\n                  ? { preliminary: part.preliminary }\n                  : {}),\n                ...(dynamic != null ? { dynamic } : {}),\n              });\n              break;\n            }\n\n            case 'tool-error': {\n              const dynamic = isDynamic(part.toolCallId);\n\n              controller.enqueue({\n                type: 'tool-output-error',\n                toolCallId: part.toolCallId,\n                errorText: onError(part.error),\n                ...(part.providerExecuted != null\n                  ? { providerExecuted: part.providerExecuted }\n                  : {}),\n                ...(dynamic != null ? { dynamic } : {}),\n              });\n              break;\n            }\n\n            case 'error': {\n              controller.enqueue({\n                type: 'error',\n                errorText: onError(part.error),\n              });\n              break;\n            }\n\n            case 'start-step': {\n              controller.enqueue({ type: 'start-step' });\n              break;\n            }\n\n            case 'finish-step': {\n              controller.enqueue({ type: 'finish-step' });\n              break;\n            }\n\n            case 'start': {\n              if (sendStart) {\n                controller.enqueue({\n                  type: 'start',\n                  ...(messageMetadataValue != null\n                    ? { messageMetadata: messageMetadataValue }\n                    : {}),\n                  ...(responseMessageId != null\n                    ? { messageId: responseMessageId }\n                    : {}),\n                });\n              }\n              break;\n            }\n\n            case 'finish': {\n              if (sendFinish) {\n                controller.enqueue({\n                  type: 'finish',\n                  finishReason: part.finishReason,\n                  ...(messageMetadataValue != null\n                    ? { messageMetadata: messageMetadataValue }\n                    : {}),\n                });\n              }\n              break;\n            }\n\n            case 'abort': {\n              controller.enqueue(part);\n              break;\n            }\n\n            case 'tool-input-end': {\n              break;\n            }\n\n            case 'raw': {\n              // Raw chunks are not included in UI message streams\n              // as they contain provider-specific data for developer use\n              break;\n            }\n\n            default: {\n              const exhaustiveCheck: never = partType;\n              throw new Error(`Unknown chunk type: ${exhaustiveCheck}`);\n            }\n          }\n\n          // start and finish events already have metadata\n          // so we only need to send metadata for other parts\n          if (\n            messageMetadataValue != null &&\n            partType !== 'start' &&\n            partType !== 'finish'\n          ) {\n            controller.enqueue({\n              type: 'message-metadata',\n              messageMetadata: messageMetadataValue,\n            });\n          }\n        },\n      }),\n    );\n\n    return createAsyncIterableStream(\n      handleUIMessageStreamFinish<UI_MESSAGE>({\n        stream: baseStream,\n        messageId: responseMessageId ?? generateMessageId?.(),\n        originalMessages,\n        onFinish,\n        onError,\n      }),\n    );\n  }\n\n  pipeUIMessageStreamToResponse<UI_MESSAGE extends UIMessage>(\n    response: ServerResponse,\n    {\n      originalMessages,\n      generateMessageId,\n      onFinish,\n      messageMetadata,\n      sendReasoning,\n      sendSources,\n      sendFinish,\n      sendStart,\n      onError,\n      ...init\n    }: UIMessageStreamResponseInit & UIMessageStreamOptions<UI_MESSAGE> = {},\n  ) {\n    pipeUIMessageStreamToResponse({\n      response,\n      stream: this.toUIMessageStream({\n        originalMessages,\n        generateMessageId,\n        onFinish,\n        messageMetadata,\n        sendReasoning,\n        sendSources,\n        sendFinish,\n        sendStart,\n        onError,\n      }),\n      ...init,\n    });\n  }\n\n  pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit) {\n    pipeTextStreamToResponse({\n      response,\n      textStream: this.textStream,\n      ...init,\n    });\n  }\n\n  toUIMessageStreamResponse<UI_MESSAGE extends UIMessage>({\n    originalMessages,\n    generateMessageId,\n    onFinish,\n    messageMetadata,\n    sendReasoning,\n    sendSources,\n    sendFinish,\n    sendStart,\n    onError,\n    ...init\n  }: UIMessageStreamResponseInit &\n    UIMessageStreamOptions<UI_MESSAGE> = {}): Response {\n    return createUIMessageStreamResponse({\n      stream: this.toUIMessageStream({\n        originalMessages,\n        generateMessageId,\n        onFinish,\n        messageMetadata,\n        sendReasoning,\n        sendSources,\n        sendFinish,\n        sendStart,\n        onError,\n      }),\n      ...init,\n    });\n  }\n\n  toTextStreamResponse(init?: ResponseInit): Response {\n    return createTextStreamResponse({\n      textStream: this.textStream,\n      ...init,\n    });\n  }\n}\n","export function prepareHeaders(\n  headers: HeadersInit | undefined,\n  defaultHeaders: Record<string, string>,\n): Headers {\n  const responseHeaders = new Headers(headers ?? {});\n\n  for (const [key, value] of Object.entries(defaultHeaders)) {\n    if (!responseHeaders.has(key)) {\n      responseHeaders.set(key, value);\n    }\n  }\n\n  return responseHeaders;\n}\n","import { prepareHeaders } from '../util/prepare-headers';\n\nexport function createTextStreamResponse({\n  status,\n  statusText,\n  headers,\n  textStream,\n}: ResponseInit & {\n  textStream: ReadableStream<string>;\n}): Response {\n  return new Response(textStream.pipeThrough(new TextEncoderStream()), {\n    status: status ?? 200,\n    statusText,\n    headers: prepareHeaders(headers, {\n      'content-type': 'text/plain; charset=utf-8',\n    }),\n  });\n}\n","import { ServerResponse } from 'node:http';\n\n/**\n * Writes the content of a stream to a server response.\n */\nexport function writeToServerResponse({\n  response,\n  status,\n  statusText,\n  headers,\n  stream,\n}: {\n  response: ServerResponse;\n  status?: number;\n  statusText?: string;\n  headers?: Record<string, string | number | string[]>;\n  stream: ReadableStream<Uint8Array>;\n}): void {\n  const statusCode = status ?? 200;\n  if (statusText !== undefined) {\n    response.writeHead(statusCode, statusText, headers);\n  } else {\n    response.writeHead(statusCode, headers);\n  }\n\n  const reader = stream.getReader();\n  const read = async () => {\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        // Respect backpressure: if write() returns false, wait for 'drain' event\n        const canContinue = response.write(value);\n        if (!canContinue) {\n          await new Promise<void>(resolve => {\n            response.once('drain', resolve);\n          });\n        }\n      }\n    } catch (error) {\n      throw error;\n    } finally {\n      response.end();\n    }\n  };\n\n  read();\n}\n","import { ServerResponse } from 'node:http';\nimport { prepareHeaders } from '../util/prepare-headers';\nimport { writeToServerResponse } from '../util/write-to-server-response';\n\nexport function pipeTextStreamToResponse({\n  response,\n  status,\n  statusText,\n  headers,\n  textStream,\n}: {\n  response: ServerResponse;\n  textStream: ReadableStream<string>;\n} & ResponseInit): void {\n  writeToServerResponse({\n    response,\n    status,\n    statusText,\n    headers: Object.fromEntries(\n      prepareHeaders(headers, {\n        'content-type': 'text/plain; charset=utf-8',\n      }).entries(),\n    ),\n    stream: textStream.pipeThrough(new TextEncoderStream()),\n  });\n}\n","export class JsonToSseTransformStream extends TransformStream<unknown, string> {\n  constructor() {\n    super({\n      transform(part, controller) {\n        controller.enqueue(`data: ${JSON.stringify(part)}\\n\\n`);\n      },\n      flush(controller) {\n        controller.enqueue('data: [DONE]\\n\\n');\n      },\n    });\n  }\n}\n","export const UI_MESSAGE_STREAM_HEADERS = {\n  'content-type': 'text/event-stream',\n  'cache-control': 'no-cache',\n  connection: 'keep-alive',\n  'x-vercel-ai-ui-message-stream': 'v1',\n  'x-accel-buffering': 'no', // disable nginx buffering\n};\n","import { prepareHeaders } from '../util/prepare-headers';\nimport { JsonToSseTransformStream } from './json-to-sse-transform-stream';\nimport { UI_MESSAGE_STREAM_HEADERS } from './ui-message-stream-headers';\nimport { UIMessageChunk } from './ui-message-chunks';\nimport { UIMessageStreamResponseInit } from './ui-message-stream-response-init';\n\nexport function createUIMessageStreamResponse({\n  status,\n  statusText,\n  headers,\n  stream,\n  consumeSseStream,\n}: UIMessageStreamResponseInit & {\n  stream: ReadableStream<UIMessageChunk>;\n}): Response {\n  let sseStream = stream.pipeThrough(new JsonToSseTransformStream());\n\n  // when the consumeSseStream is provided, we need to tee the stream\n  // and send the second part to the consumeSseStream function\n  // so that it can be consumed by the client independently\n  if (consumeSseStream) {\n    const [stream1, stream2] = sseStream.tee();\n    sseStream = stream1;\n    consumeSseStream({ stream: stream2 }); // no await (do not block the response)\n  }\n\n  return new Response(sseStream.pipeThrough(new TextEncoderStream()), {\n    status,\n    statusText,\n    headers: prepareHeaders(headers, UI_MESSAGE_STREAM_HEADERS),\n  });\n}\n","import { IdGenerator } from '@ai-sdk/provider-utils';\nimport { UIMessage } from '../ui/ui-messages';\n\nexport function getResponseUIMessageId({\n  originalMessages,\n  responseMessageId,\n}: {\n  originalMessages: UIMessage[] | undefined;\n  responseMessageId: string | IdGenerator;\n}) {\n  // when there are no original messages (i.e. no persistence),\n  // the assistant message id generation is handled on the client side.\n  if (originalMessages == null) {\n    return undefined;\n  }\n\n  const lastMessage = originalMessages[originalMessages.length - 1];\n\n  return lastMessage?.role === 'assistant'\n    ? lastMessage.id\n    : typeof responseMessageId === 'function'\n      ? responseMessageId()\n      : responseMessageId;\n}\n","import {\n  StandardSchemaV1,\n  validateTypes,\n  Validator,\n} from '@ai-sdk/provider-utils';\nimport { ProviderMetadata } from '../types';\nimport { FinishReason } from '../types/language-model';\nimport {\n  DataUIMessageChunk,\n  InferUIMessageChunk,\n  isDataUIMessageChunk,\n  UIMessageChunk,\n} from '../ui-message-stream/ui-message-chunks';\nimport { ErrorHandler } from '../util/error-handler';\nimport { mergeObjects } from '../util/merge-objects';\nimport { parsePartialJson } from '../util/parse-partial-json';\nimport { UIDataTypesToSchemas } from './chat';\nimport {\n  DataUIPart,\n  DynamicToolUIPart,\n  getToolName,\n  InferUIMessageData,\n  InferUIMessageMetadata,\n  InferUIMessageToolCall,\n  InferUIMessageTools,\n  isToolUIPart,\n  ReasoningUIPart,\n  TextUIPart,\n  ToolUIPart,\n  UIMessage,\n  UIMessagePart,\n} from './ui-messages';\n\nexport type StreamingUIMessageState<UI_MESSAGE extends UIMessage> = {\n  message: UI_MESSAGE;\n  activeTextParts: Record<string, TextUIPart>;\n  activeReasoningParts: Record<string, ReasoningUIPart>;\n  partialToolCalls: Record<\n    string,\n    { text: string; index: number; toolName: string; dynamic?: boolean }\n  >;\n  finishReason?: FinishReason;\n};\n\nexport function createStreamingUIMessageState<UI_MESSAGE extends UIMessage>({\n  lastMessage,\n  messageId,\n}: {\n  lastMessage: UI_MESSAGE | undefined;\n  messageId: string;\n}): StreamingUIMessageState<UI_MESSAGE> {\n  return {\n    message:\n      lastMessage?.role === 'assistant'\n        ? lastMessage\n        : ({\n            id: messageId,\n            metadata: undefined,\n            role: 'assistant',\n            parts: [] as UIMessagePart<\n              InferUIMessageData<UI_MESSAGE>,\n              InferUIMessageTools<UI_MESSAGE>\n            >[],\n          } as UI_MESSAGE),\n    activeTextParts: {},\n    activeReasoningParts: {},\n    partialToolCalls: {},\n  };\n}\n\nexport function processUIMessageStream<UI_MESSAGE extends UIMessage>({\n  stream,\n  messageMetadataSchema,\n  dataPartSchemas,\n  runUpdateMessageJob,\n  onError,\n  onToolCall,\n  onData,\n}: {\n  // input stream is not fully typed yet:\n  stream: ReadableStream<UIMessageChunk>;\n  messageMetadataSchema?:\n    | Validator<InferUIMessageMetadata<UI_MESSAGE>>\n    | StandardSchemaV1<InferUIMessageMetadata<UI_MESSAGE>>;\n  dataPartSchemas?: UIDataTypesToSchemas<InferUIMessageData<UI_MESSAGE>>;\n  onToolCall?: (options: {\n    toolCall: InferUIMessageToolCall<UI_MESSAGE>;\n  }) => void | PromiseLike<void>;\n  onData?: (dataPart: DataUIPart<InferUIMessageData<UI_MESSAGE>>) => void;\n  runUpdateMessageJob: (\n    job: (options: {\n      state: StreamingUIMessageState<UI_MESSAGE>;\n      write: () => void;\n    }) => Promise<void>,\n  ) => Promise<void>;\n  onError: ErrorHandler;\n}): ReadableStream<InferUIMessageChunk<UI_MESSAGE>> {\n  return stream.pipeThrough(\n    new TransformStream<UIMessageChunk, InferUIMessageChunk<UI_MESSAGE>>({\n      async transform(chunk, controller) {\n        await runUpdateMessageJob(async ({ state, write }) => {\n          function getToolInvocation(toolCallId: string) {\n            const toolInvocations = state.message.parts.filter(isToolUIPart);\n\n            const toolInvocation = toolInvocations.find(\n              invocation => invocation.toolCallId === toolCallId,\n            );\n\n            if (toolInvocation == null) {\n              throw new Error(\n                'tool-output-error must be preceded by a tool-input-available',\n              );\n            }\n\n            return toolInvocation;\n          }\n\n          function getDynamicToolInvocation(toolCallId: string) {\n            const toolInvocations = state.message.parts.filter(\n              part => part.type === 'dynamic-tool',\n            ) as DynamicToolUIPart[];\n\n            const toolInvocation = toolInvocations.find(\n              invocation => invocation.toolCallId === toolCallId,\n            );\n\n            if (toolInvocation == null) {\n              throw new Error(\n                'tool-output-error must be preceded by a tool-input-available',\n              );\n            }\n\n            return toolInvocation;\n          }\n\n          function updateToolPart(\n            options: {\n              toolName: keyof InferUIMessageTools<UI_MESSAGE> & string;\n              toolCallId: string;\n              providerExecuted?: boolean;\n            } & (\n              | {\n                  state: 'input-streaming';\n                  input: unknown;\n                  providerExecuted?: boolean;\n                }\n              | {\n                  state: 'input-available';\n                  input: unknown;\n                  providerExecuted?: boolean;\n                  providerMetadata?: ProviderMetadata;\n                }\n              | {\n                  state: 'output-available';\n                  input: unknown;\n                  output: unknown;\n                  providerExecuted?: boolean;\n                  preliminary?: boolean;\n                }\n              | {\n                  state: 'output-error';\n                  input: unknown;\n                  rawInput?: unknown;\n                  errorText: string;\n                  providerExecuted?: boolean;\n                  providerMetadata?: ProviderMetadata;\n                }\n            ),\n          ) {\n            const part = state.message.parts.find(\n              part =>\n                isToolUIPart(part) && part.toolCallId === options.toolCallId,\n            ) as ToolUIPart<InferUIMessageTools<UI_MESSAGE>> | undefined;\n\n            const anyOptions = options as any;\n            const anyPart = part as any;\n\n            if (part != null) {\n              part.state = options.state;\n              anyPart.input = anyOptions.input;\n              anyPart.output = anyOptions.output;\n              anyPart.errorText = anyOptions.errorText;\n              anyPart.rawInput = anyOptions.rawInput;\n              anyPart.preliminary = anyOptions.preliminary;\n\n              // once providerExecuted is set, it stays for streaming\n              anyPart.providerExecuted =\n                anyOptions.providerExecuted ?? part.providerExecuted;\n\n              if (\n                anyOptions.providerMetadata != null &&\n                part.state === 'input-available'\n              ) {\n                part.callProviderMetadata = anyOptions.providerMetadata;\n              }\n            } else {\n              state.message.parts.push({\n                type: `tool-${options.toolName}`,\n                toolCallId: options.toolCallId,\n                state: options.state,\n                input: anyOptions.input,\n                output: anyOptions.output,\n                rawInput: anyOptions.rawInput,\n                errorText: anyOptions.errorText,\n                providerExecuted: anyOptions.providerExecuted,\n                preliminary: anyOptions.preliminary,\n                ...(anyOptions.providerMetadata != null\n                  ? { callProviderMetadata: anyOptions.providerMetadata }\n                  : {}),\n              } as ToolUIPart<InferUIMessageTools<UI_MESSAGE>>);\n            }\n          }\n\n          function updateDynamicToolPart(\n            options: {\n              toolName: keyof InferUIMessageTools<UI_MESSAGE> & string;\n              toolCallId: string;\n              providerExecuted?: boolean;\n            } & (\n              | {\n                  state: 'input-streaming';\n                  input: unknown;\n                }\n              | {\n                  state: 'input-available';\n                  input: unknown;\n                  providerMetadata?: ProviderMetadata;\n                }\n              | {\n                  state: 'output-available';\n                  input: unknown;\n                  output: unknown;\n                  preliminary: boolean | undefined;\n                }\n              | {\n                  state: 'output-error';\n                  input: unknown;\n                  errorText: string;\n                  providerMetadata?: ProviderMetadata;\n                }\n            ),\n          ) {\n            const part = state.message.parts.find(\n              part =>\n                part.type === 'dynamic-tool' &&\n                part.toolCallId === options.toolCallId,\n            ) as DynamicToolUIPart | undefined;\n\n            const anyOptions = options as any;\n            const anyPart = part as any;\n\n            if (part != null) {\n              part.state = options.state;\n              anyPart.toolName = options.toolName;\n              anyPart.input = anyOptions.input;\n              anyPart.output = anyOptions.output;\n              anyPart.errorText = anyOptions.errorText;\n              anyPart.rawInput = anyOptions.rawInput ?? anyPart.rawInput;\n              anyPart.preliminary = anyOptions.preliminary;\n\n              // once providerExecuted is set, it stays for streaming\n              anyPart.providerExecuted =\n                anyOptions.providerExecuted ?? part.providerExecuted;\n\n              if (\n                anyOptions.providerMetadata != null &&\n                part.state === 'input-available'\n              ) {\n                part.callProviderMetadata = anyOptions.providerMetadata;\n              }\n            } else {\n              state.message.parts.push({\n                type: 'dynamic-tool',\n                toolName: options.toolName,\n                toolCallId: options.toolCallId,\n                state: options.state,\n                input: anyOptions.input,\n                output: anyOptions.output,\n                errorText: anyOptions.errorText,\n                preliminary: anyOptions.preliminary,\n                providerExecuted: anyOptions.providerExecuted,\n                ...(anyOptions.providerMetadata != null\n                  ? { callProviderMetadata: anyOptions.providerMetadata }\n                  : {}),\n              } as DynamicToolUIPart);\n            }\n          }\n\n          async function updateMessageMetadata(metadata: unknown) {\n            if (metadata != null) {\n              const mergedMetadata =\n                state.message.metadata != null\n                  ? mergeObjects(state.message.metadata, metadata)\n                  : metadata;\n\n              if (messageMetadataSchema != null) {\n                await validateTypes({\n                  value: mergedMetadata,\n                  schema: messageMetadataSchema,\n                });\n              }\n\n              state.message.metadata =\n                mergedMetadata as InferUIMessageMetadata<UI_MESSAGE>;\n            }\n          }\n\n          switch (chunk.type) {\n            case 'text-start': {\n              const textPart: TextUIPart = {\n                type: 'text',\n                text: '',\n                providerMetadata: chunk.providerMetadata,\n                state: 'streaming',\n              };\n              state.activeTextParts[chunk.id] = textPart;\n              state.message.parts.push(textPart);\n              write();\n              break;\n            }\n\n            case 'text-delta': {\n              const textPart = state.activeTextParts[chunk.id];\n              textPart.text += chunk.delta;\n              textPart.providerMetadata =\n                chunk.providerMetadata ?? textPart.providerMetadata;\n              write();\n              break;\n            }\n\n            case 'text-end': {\n              const textPart = state.activeTextParts[chunk.id];\n              textPart.state = 'done';\n              textPart.providerMetadata =\n                chunk.providerMetadata ?? textPart.providerMetadata;\n              delete state.activeTextParts[chunk.id];\n              write();\n              break;\n            }\n\n            case 'reasoning-start': {\n              const reasoningPart: ReasoningUIPart = {\n                type: 'reasoning',\n                text: '',\n                providerMetadata: chunk.providerMetadata,\n                state: 'streaming',\n              };\n              state.activeReasoningParts[chunk.id] = reasoningPart;\n              state.message.parts.push(reasoningPart);\n              write();\n              break;\n            }\n\n            case 'reasoning-delta': {\n              const reasoningPart = state.activeReasoningParts[chunk.id];\n              reasoningPart.text += chunk.delta;\n              reasoningPart.providerMetadata =\n                chunk.providerMetadata ?? reasoningPart.providerMetadata;\n              write();\n              break;\n            }\n\n            case 'reasoning-end': {\n              const reasoningPart = state.activeReasoningParts[chunk.id];\n              reasoningPart.providerMetadata =\n                chunk.providerMetadata ?? reasoningPart.providerMetadata;\n              reasoningPart.state = 'done';\n              delete state.activeReasoningParts[chunk.id];\n\n              write();\n              break;\n            }\n\n            case 'file': {\n              state.message.parts.push({\n                type: 'file',\n                mediaType: chunk.mediaType,\n                url: chunk.url,\n              });\n\n              write();\n              break;\n            }\n\n            case 'source-url': {\n              state.message.parts.push({\n                type: 'source-url',\n                sourceId: chunk.sourceId,\n                url: chunk.url,\n                title: chunk.title,\n                providerMetadata: chunk.providerMetadata,\n              });\n\n              write();\n              break;\n            }\n\n            case 'source-document': {\n              state.message.parts.push({\n                type: 'source-document',\n                sourceId: chunk.sourceId,\n                mediaType: chunk.mediaType,\n                title: chunk.title,\n                filename: chunk.filename,\n                providerMetadata: chunk.providerMetadata,\n              });\n\n              write();\n              break;\n            }\n\n            case 'tool-input-start': {\n              const toolInvocations = state.message.parts.filter(isToolUIPart);\n\n              // add the partial tool call to the map\n              state.partialToolCalls[chunk.toolCallId] = {\n                text: '',\n                toolName: chunk.toolName,\n                index: toolInvocations.length,\n                dynamic: chunk.dynamic,\n              };\n\n              if (chunk.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: 'input-streaming',\n                  input: undefined,\n                  providerExecuted: chunk.providerExecuted,\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: 'input-streaming',\n                  input: undefined,\n                  providerExecuted: chunk.providerExecuted,\n                });\n              }\n\n              write();\n              break;\n            }\n\n            case 'tool-input-delta': {\n              const partialToolCall = state.partialToolCalls[chunk.toolCallId];\n\n              partialToolCall.text += chunk.inputTextDelta;\n\n              const { value: partialArgs } = await parsePartialJson(\n                partialToolCall.text,\n              );\n\n              if (partialToolCall.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: partialToolCall.toolName,\n                  state: 'input-streaming',\n                  input: partialArgs,\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: partialToolCall.toolName,\n                  state: 'input-streaming',\n                  input: partialArgs,\n                });\n              }\n\n              write();\n              break;\n            }\n\n            case 'tool-input-available': {\n              if (chunk.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: 'input-available',\n                  input: chunk.input,\n                  providerExecuted: chunk.providerExecuted,\n                  providerMetadata: chunk.providerMetadata,\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: 'input-available',\n                  input: chunk.input,\n                  providerExecuted: chunk.providerExecuted,\n                  providerMetadata: chunk.providerMetadata,\n                });\n              }\n\n              write();\n\n              // invoke the onToolCall callback if it exists. This is blocking.\n              // In the future we should make this non-blocking, which\n              // requires additional state management for error handling etc.\n              // Skip calling onToolCall for provider-executed tools since they are already executed\n              if (onToolCall && !chunk.providerExecuted) {\n                await onToolCall({\n                  toolCall: chunk as InferUIMessageToolCall<UI_MESSAGE>,\n                });\n              }\n              break;\n            }\n\n            case 'tool-input-error': {\n              if (chunk.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: 'output-error',\n                  input: chunk.input,\n                  errorText: chunk.errorText,\n                  providerExecuted: chunk.providerExecuted,\n                  providerMetadata: chunk.providerMetadata,\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: 'output-error',\n                  input: undefined,\n                  rawInput: chunk.input,\n                  errorText: chunk.errorText,\n                  providerExecuted: chunk.providerExecuted,\n                  providerMetadata: chunk.providerMetadata,\n                });\n              }\n\n              write();\n              break;\n            }\n\n            case 'tool-output-available': {\n              if (chunk.dynamic) {\n                const toolInvocation = getDynamicToolInvocation(\n                  chunk.toolCallId,\n                );\n\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: toolInvocation.toolName,\n                  state: 'output-available',\n                  input: (toolInvocation as any).input,\n                  output: chunk.output,\n                  preliminary: chunk.preliminary,\n                });\n              } else {\n                const toolInvocation = getToolInvocation(chunk.toolCallId);\n\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: getToolName(toolInvocation),\n                  state: 'output-available',\n                  input: (toolInvocation as any).input,\n                  output: chunk.output,\n                  providerExecuted: chunk.providerExecuted,\n                  preliminary: chunk.preliminary,\n                });\n              }\n\n              write();\n              break;\n            }\n\n            case 'tool-output-error': {\n              if (chunk.dynamic) {\n                const toolInvocation = getDynamicToolInvocation(\n                  chunk.toolCallId,\n                );\n\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: toolInvocation.toolName,\n                  state: 'output-error',\n                  input: (toolInvocation as any).input,\n                  errorText: chunk.errorText,\n                  providerExecuted: chunk.providerExecuted,\n                });\n              } else {\n                const toolInvocation = getToolInvocation(chunk.toolCallId);\n\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: getToolName(toolInvocation),\n                  state: 'output-error',\n                  input: (toolInvocation as any).input,\n                  rawInput: (toolInvocation as any).rawInput,\n                  errorText: chunk.errorText,\n                  providerExecuted: chunk.providerExecuted,\n                });\n              }\n\n              write();\n              break;\n            }\n\n            case 'start-step': {\n              // add a step boundary part to the message\n              state.message.parts.push({ type: 'step-start' });\n              break;\n            }\n\n            case 'finish-step': {\n              // reset the current text and reasoning parts\n              state.activeTextParts = {};\n              state.activeReasoningParts = {};\n              break;\n            }\n\n            case 'start': {\n              if (chunk.messageId != null) {\n                state.message.id = chunk.messageId;\n              }\n\n              await updateMessageMetadata(chunk.messageMetadata);\n\n              if (chunk.messageId != null || chunk.messageMetadata != null) {\n                write();\n              }\n              break;\n            }\n\n            case 'finish': {\n              if (chunk.finishReason != null) {\n                state.finishReason = chunk.finishReason;\n              }\n              await updateMessageMetadata(chunk.messageMetadata);\n              if (chunk.messageMetadata != null) {\n                write();\n              }\n              break;\n            }\n\n            case 'message-metadata': {\n              await updateMessageMetadata(chunk.messageMetadata);\n              if (chunk.messageMetadata != null) {\n                write();\n              }\n              break;\n            }\n\n            case 'error': {\n              onError?.(new Error(chunk.errorText));\n              break;\n            }\n\n            default: {\n              if (isDataUIMessageChunk(chunk)) {\n                // validate data chunk if dataPartSchemas is provided\n                if (dataPartSchemas?.[chunk.type] != null) {\n                  await validateTypes({\n                    value: chunk.data,\n                    schema: dataPartSchemas[chunk.type],\n                  });\n                }\n\n                // cast, validation is done above\n                const dataChunk = chunk as DataUIMessageChunk<\n                  InferUIMessageData<UI_MESSAGE>\n                >;\n\n                // transient parts are not added to the message state\n                if (dataChunk.transient) {\n                  onData?.(dataChunk);\n                  break;\n                }\n\n                const existingUIPart =\n                  dataChunk.id != null\n                    ? (state.message.parts.find(\n                        chunkArg =>\n                          dataChunk.type === chunkArg.type &&\n                          dataChunk.id === chunkArg.id,\n                      ) as\n                        | DataUIPart<InferUIMessageData<UI_MESSAGE>>\n                        | undefined)\n                    : undefined;\n\n                if (existingUIPart != null) {\n                  existingUIPart.data = dataChunk.data;\n                } else {\n                  state.message.parts.push(dataChunk);\n                }\n\n                onData?.(dataChunk);\n\n                write();\n              }\n            }\n          }\n\n          controller.enqueue(chunk as InferUIMessageChunk<UI_MESSAGE>);\n        });\n      },\n    }),\n  );\n}\n","import { z } from 'zod/v4';\nimport {\n  ProviderMetadata,\n  providerMetadataSchema,\n} from '../types/provider-metadata';\nimport { FinishReason } from '../types/language-model';\nimport {\n  InferUIMessageData,\n  InferUIMessageMetadata,\n  UIDataTypes,\n  UIMessage,\n} from '../ui/ui-messages';\nimport { ValueOf } from '../util/value-of';\nimport { lazyValidator, zodSchema } from '@ai-sdk/provider-utils';\n\nexport const uiMessageChunkSchema = lazyValidator(() =>\n  zodSchema(\n    z.union([\n      z.strictObject({\n        type: z.literal('text-start'),\n        id: z.string(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('text-delta'),\n        id: z.string(),\n        delta: z.string(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('text-end'),\n        id: z.string(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('error'),\n        errorText: z.string(),\n      }),\n      z.strictObject({\n        type: z.literal('tool-input-start'),\n        toolCallId: z.string(),\n        toolName: z.string(),\n        providerExecuted: z.boolean().optional(),\n        dynamic: z.boolean().optional(),\n      }),\n      z.strictObject({\n        type: z.literal('tool-input-delta'),\n        toolCallId: z.string(),\n        inputTextDelta: z.string(),\n      }),\n      z.strictObject({\n        type: z.literal('tool-input-available'),\n        toolCallId: z.string(),\n        toolName: z.string(),\n        input: z.unknown(),\n        providerExecuted: z.boolean().optional(),\n        providerMetadata: providerMetadataSchema.optional(),\n        dynamic: z.boolean().optional(),\n      }),\n      z.strictObject({\n        type: z.literal('tool-input-error'),\n        toolCallId: z.string(),\n        toolName: z.string(),\n        input: z.unknown(),\n        providerExecuted: z.boolean().optional(),\n        providerMetadata: providerMetadataSchema.optional(),\n        dynamic: z.boolean().optional(),\n        errorText: z.string(),\n      }),\n      z.strictObject({\n        type: z.literal('tool-output-available'),\n        toolCallId: z.string(),\n        output: z.unknown(),\n        providerExecuted: z.boolean().optional(),\n        dynamic: z.boolean().optional(),\n        preliminary: z.boolean().optional(),\n      }),\n      z.strictObject({\n        type: z.literal('tool-output-error'),\n        toolCallId: z.string(),\n        errorText: z.string(),\n        providerExecuted: z.boolean().optional(),\n        dynamic: z.boolean().optional(),\n      }),\n      z.strictObject({\n        type: z.literal('reasoning-start'),\n        id: z.string(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('reasoning-delta'),\n        id: z.string(),\n        delta: z.string(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('reasoning-end'),\n        id: z.string(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('source-url'),\n        sourceId: z.string(),\n        url: z.string(),\n        title: z.string().optional(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('source-document'),\n        sourceId: z.string(),\n        mediaType: z.string(),\n        title: z.string(),\n        filename: z.string().optional(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.literal('file'),\n        url: z.string(),\n        mediaType: z.string(),\n        providerMetadata: providerMetadataSchema.optional(),\n      }),\n      z.strictObject({\n        type: z.custom<`data-${string}`>(\n          (value): value is `data-${string}` =>\n            typeof value === 'string' && value.startsWith('data-'),\n          { message: 'Type must start with \"data-\"' },\n        ),\n        id: z.string().optional(),\n        data: z.unknown(),\n        transient: z.boolean().optional(),\n      }),\n      z.strictObject({\n        type: z.literal('start-step'),\n      }),\n      z.strictObject({\n        type: z.literal('finish-step'),\n      }),\n      z.strictObject({\n        type: z.literal('start'),\n        messageId: z.string().optional(),\n        messageMetadata: z.unknown().optional(),\n      }),\n      z.strictObject({\n        type: z.literal('finish'),\n        finishReason: z\n          .enum([\n            'stop',\n            'length',\n            'content-filter',\n            'tool-calls',\n            'error',\n            'other',\n            'unknown',\n          ] as const satisfies readonly FinishReason[])\n          .optional(),\n        messageMetadata: z.unknown().optional(),\n      }),\n      z.strictObject({\n        type: z.literal('abort'),\n      }),\n      z.strictObject({\n        type: z.literal('message-metadata'),\n        messageMetadata: z.unknown(),\n      }),\n    ]),\n  ),\n);\n\nexport type DataUIMessageChunk<DATA_TYPES extends UIDataTypes> = ValueOf<{\n  [NAME in keyof DATA_TYPES & string]: {\n    type: `data-${NAME}`;\n    id?: string;\n    data: DATA_TYPES[NAME];\n    transient?: boolean;\n  };\n}>;\n\nexport type UIMessageChunk<\n  METADATA = unknown,\n  DATA_TYPES extends UIDataTypes = UIDataTypes,\n> =\n  | {\n      type: 'text-start';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'text-delta';\n      delta: string;\n      id: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'text-end';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'reasoning-start';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'reasoning-delta';\n      id: string;\n      delta: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'reasoning-end';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'error';\n      errorText: string;\n    }\n  | {\n      type: 'tool-input-available';\n      toolCallId: string;\n      toolName: string;\n      input: unknown;\n      providerExecuted?: boolean;\n      providerMetadata?: ProviderMetadata;\n      dynamic?: boolean;\n    }\n  | {\n      type: 'tool-input-error';\n      toolCallId: string;\n      toolName: string;\n      input: unknown;\n      providerExecuted?: boolean;\n      providerMetadata?: ProviderMetadata;\n      dynamic?: boolean;\n      errorText: string;\n    }\n  | {\n      type: 'tool-output-available';\n      toolCallId: string;\n      output: unknown;\n      providerExecuted?: boolean;\n      dynamic?: boolean;\n      preliminary?: boolean;\n    }\n  | {\n      type: 'tool-output-error';\n      toolCallId: string;\n      errorText: string;\n      providerExecuted?: boolean;\n      dynamic?: boolean;\n    }\n  | {\n      type: 'tool-input-start';\n      toolCallId: string;\n      toolName: string;\n      providerExecuted?: boolean;\n      dynamic?: boolean;\n    }\n  | {\n      type: 'tool-input-delta';\n      toolCallId: string;\n      inputTextDelta: string;\n    }\n  | {\n      type: 'source-url';\n      sourceId: string;\n      url: string;\n      title?: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'source-document';\n      sourceId: string;\n      mediaType: string;\n      title: string;\n      filename?: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'file';\n      url: string;\n      mediaType: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | DataUIMessageChunk<DATA_TYPES>\n  | {\n      type: 'start-step';\n    }\n  | {\n      type: 'finish-step';\n    }\n  | {\n      type: 'start';\n      messageId?: string;\n      messageMetadata?: METADATA;\n    }\n  | {\n      type: 'finish';\n      finishReason?: FinishReason;\n      messageMetadata?: METADATA;\n    }\n  | {\n      type: 'abort';\n    }\n  | {\n      type: 'message-metadata';\n      messageMetadata: METADATA;\n    };\n\nexport function isDataUIMessageChunk(\n  chunk: UIMessageChunk,\n): chunk is DataUIMessageChunk<UIDataTypes> {\n  return chunk.type.startsWith('data-');\n}\n\nexport type InferUIMessageChunk<T extends UIMessage> = UIMessageChunk<\n  InferUIMessageMetadata<T>,\n  InferUIMessageData<T>\n>;\n","/**\n * Deeply merges two objects together.\n * - Properties from the `overrides` object override those in the `base` object with the same key.\n * - For nested objects, the merge is performed recursively (deep merge).\n * - Arrays are replaced, not merged.\n * - Primitive values are replaced.\n * - If both `base` and `overrides` are undefined, returns undefined.\n * - If one of `base` or `overrides` is undefined, returns the other.\n *\n * @param base The target object to merge into\n * @param overrides The source object to merge from\n * @returns A new object with the merged properties, or undefined if both inputs are undefined\n */\nexport function mergeObjects<T extends object, U extends object>(\n  base: T | undefined,\n  overrides: U | undefined,\n): (T & U) | T | U | undefined {\n  // If both inputs are undefined, return undefined\n  if (base === undefined && overrides === undefined) {\n    return undefined;\n  }\n\n  // If target is undefined, return source\n  if (base === undefined) {\n    return overrides;\n  }\n\n  // If source is undefined, return target\n  if (overrides === undefined) {\n    return base;\n  }\n\n  // Create a new object to avoid mutating the inputs\n  const result = { ...base } as T & U;\n\n  // Iterate through all keys in the source object\n  for (const key in overrides) {\n    if (Object.prototype.hasOwnProperty.call(overrides, key)) {\n      const overridesValue = overrides[key];\n\n      // Skip if the overrides value is undefined\n      if (overridesValue === undefined) continue;\n\n      // Get the base value if it exists\n      const baseValue =\n        key in base ? base[key as unknown as keyof T] : undefined;\n\n      // Check if both values are objects that can be deeply merged\n      const isSourceObject =\n        overridesValue !== null &&\n        typeof overridesValue === 'object' &&\n        !Array.isArray(overridesValue) &&\n        !(overridesValue instanceof Date) &&\n        !(overridesValue instanceof RegExp);\n\n      const isTargetObject =\n        baseValue !== null &&\n        baseValue !== undefined &&\n        typeof baseValue === 'object' &&\n        !Array.isArray(baseValue) &&\n        !(baseValue instanceof Date) &&\n        !(baseValue instanceof RegExp);\n\n      // If both values are mergeable objects, merge them recursively\n      if (isSourceObject && isTargetObject) {\n        result[key as keyof (T & U)] = mergeObjects(\n          baseValue as object,\n          overridesValue as object,\n        ) as any;\n      } else {\n        // For primitives, arrays, or when one value is not a mergeable object,\n        // simply override with the source value\n        result[key as keyof (T & U)] = overridesValue as any;\n      }\n    }\n  }\n\n  return result;\n}\n","import { JSONValue } from '@ai-sdk/provider';\nimport { safeParseJSON } from '@ai-sdk/provider-utils';\nimport { fixJson } from './fix-json';\n\nexport async function parsePartialJson(jsonText: string | undefined): Promise<{\n  value: JSONValue | undefined;\n  state:\n    | 'undefined-input'\n    | 'successful-parse'\n    | 'repaired-parse'\n    | 'failed-parse';\n}> {\n  if (jsonText === undefined) {\n    return { value: undefined, state: 'undefined-input' };\n  }\n\n  let result = await safeParseJSON({ text: jsonText });\n\n  if (result.success) {\n    return { value: result.value, state: 'successful-parse' };\n  }\n\n  result = await safeParseJSON({ text: fixJson(jsonText) });\n\n  if (result.success) {\n    return { value: result.value, state: 'repaired-parse' };\n  }\n\n  return { value: undefined, state: 'failed-parse' };\n}\n","type State =\n  | 'ROOT'\n  | 'FINISH'\n  | 'INSIDE_STRING'\n  | 'INSIDE_STRING_ESCAPE'\n  | 'INSIDE_LITERAL'\n  | 'INSIDE_NUMBER'\n  | 'INSIDE_OBJECT_START'\n  | 'INSIDE_OBJECT_KEY'\n  | 'INSIDE_OBJECT_AFTER_KEY'\n  | 'INSIDE_OBJECT_BEFORE_VALUE'\n  | 'INSIDE_OBJECT_AFTER_VALUE'\n  | 'INSIDE_OBJECT_AFTER_COMMA'\n  | 'INSIDE_ARRAY_START'\n  | 'INSIDE_ARRAY_AFTER_VALUE'\n  | 'INSIDE_ARRAY_AFTER_COMMA';\n\n// Implemented as a scanner with additional fixing\n// that performs a single linear time scan pass over the partial JSON.\n//\n// The states should ideally match relevant states from the JSON spec:\n// https://www.json.org/json-en.html\n//\n// Please note that invalid JSON is not considered/covered, because it\n// is assumed that the resulting JSON will be processed by a standard\n// JSON parser that will detect any invalid JSON.\nexport function fixJson(input: string): string {\n  const stack: State[] = ['ROOT'];\n  let lastValidIndex = -1;\n  let literalStart: number | null = null;\n\n  function processValueStart(char: string, i: number, swapState: State) {\n    {\n      switch (char) {\n        case '\"': {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push('INSIDE_STRING');\n          break;\n        }\n\n        case 'f':\n        case 't':\n        case 'n': {\n          lastValidIndex = i;\n          literalStart = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push('INSIDE_LITERAL');\n          break;\n        }\n\n        case '-': {\n          stack.pop();\n          stack.push(swapState);\n          stack.push('INSIDE_NUMBER');\n          break;\n        }\n        case '0':\n        case '1':\n        case '2':\n        case '3':\n        case '4':\n        case '5':\n        case '6':\n        case '7':\n        case '8':\n        case '9': {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push('INSIDE_NUMBER');\n          break;\n        }\n\n        case '{': {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push('INSIDE_OBJECT_START');\n          break;\n        }\n\n        case '[': {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push('INSIDE_ARRAY_START');\n          break;\n        }\n      }\n    }\n  }\n\n  function processAfterObjectValue(char: string, i: number) {\n    switch (char) {\n      case ',': {\n        stack.pop();\n        stack.push('INSIDE_OBJECT_AFTER_COMMA');\n        break;\n      }\n      case '}': {\n        lastValidIndex = i;\n        stack.pop();\n        break;\n      }\n    }\n  }\n\n  function processAfterArrayValue(char: string, i: number) {\n    switch (char) {\n      case ',': {\n        stack.pop();\n        stack.push('INSIDE_ARRAY_AFTER_COMMA');\n        break;\n      }\n      case ']': {\n        lastValidIndex = i;\n        stack.pop();\n        break;\n      }\n    }\n  }\n\n  for (let i = 0; i < input.length; i++) {\n    const char = input[i];\n    const currentState = stack[stack.length - 1];\n\n    switch (currentState) {\n      case 'ROOT':\n        processValueStart(char, i, 'FINISH');\n        break;\n\n      case 'INSIDE_OBJECT_START': {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push('INSIDE_OBJECT_KEY');\n            break;\n          }\n          case '}': {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n        }\n        break;\n      }\n\n      case 'INSIDE_OBJECT_AFTER_COMMA': {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push('INSIDE_OBJECT_KEY');\n            break;\n          }\n        }\n        break;\n      }\n\n      case 'INSIDE_OBJECT_KEY': {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push('INSIDE_OBJECT_AFTER_KEY');\n            break;\n          }\n        }\n        break;\n      }\n\n      case 'INSIDE_OBJECT_AFTER_KEY': {\n        switch (char) {\n          case ':': {\n            stack.pop();\n            stack.push('INSIDE_OBJECT_BEFORE_VALUE');\n\n            break;\n          }\n        }\n        break;\n      }\n\n      case 'INSIDE_OBJECT_BEFORE_VALUE': {\n        processValueStart(char, i, 'INSIDE_OBJECT_AFTER_VALUE');\n        break;\n      }\n\n      case 'INSIDE_OBJECT_AFTER_VALUE': {\n        processAfterObjectValue(char, i);\n        break;\n      }\n\n      case 'INSIDE_STRING': {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            lastValidIndex = i;\n            break;\n          }\n\n          case '\\\\': {\n            stack.push('INSIDE_STRING_ESCAPE');\n            break;\n          }\n\n          default: {\n            lastValidIndex = i;\n          }\n        }\n\n        break;\n      }\n\n      case 'INSIDE_ARRAY_START': {\n        switch (char) {\n          case ']': {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n\n          default: {\n            lastValidIndex = i;\n            processValueStart(char, i, 'INSIDE_ARRAY_AFTER_VALUE');\n            break;\n          }\n        }\n        break;\n      }\n\n      case 'INSIDE_ARRAY_AFTER_VALUE': {\n        switch (char) {\n          case ',': {\n            stack.pop();\n            stack.push('INSIDE_ARRAY_AFTER_COMMA');\n            break;\n          }\n\n          case ']': {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n\n          default: {\n            lastValidIndex = i;\n            break;\n          }\n        }\n\n        break;\n      }\n\n      case 'INSIDE_ARRAY_AFTER_COMMA': {\n        processValueStart(char, i, 'INSIDE_ARRAY_AFTER_VALUE');\n        break;\n      }\n\n      case 'INSIDE_STRING_ESCAPE': {\n        stack.pop();\n        lastValidIndex = i;\n\n        break;\n      }\n\n      case 'INSIDE_NUMBER': {\n        switch (char) {\n          case '0':\n          case '1':\n          case '2':\n          case '3':\n          case '4':\n          case '5':\n          case '6':\n          case '7':\n          case '8':\n          case '9': {\n            lastValidIndex = i;\n            break;\n          }\n\n          case 'e':\n          case 'E':\n          case '-':\n          case '.': {\n            break;\n          }\n\n          case ',': {\n            stack.pop();\n\n            if (stack[stack.length - 1] === 'INSIDE_ARRAY_AFTER_VALUE') {\n              processAfterArrayValue(char, i);\n            }\n\n            if (stack[stack.length - 1] === 'INSIDE_OBJECT_AFTER_VALUE') {\n              processAfterObjectValue(char, i);\n            }\n\n            break;\n          }\n\n          case '}': {\n            stack.pop();\n\n            if (stack[stack.length - 1] === 'INSIDE_OBJECT_AFTER_VALUE') {\n              processAfterObjectValue(char, i);\n            }\n\n            break;\n          }\n\n          case ']': {\n            stack.pop();\n\n            if (stack[stack.length - 1] === 'INSIDE_ARRAY_AFTER_VALUE') {\n              processAfterArrayValue(char, i);\n            }\n\n            break;\n          }\n\n          default: {\n            stack.pop();\n            break;\n          }\n        }\n\n        break;\n      }\n\n      case 'INSIDE_LITERAL': {\n        const partialLiteral = input.substring(literalStart!, i + 1);\n\n        if (\n          !'false'.startsWith(partialLiteral) &&\n          !'true'.startsWith(partialLiteral) &&\n          !'null'.startsWith(partialLiteral)\n        ) {\n          stack.pop();\n\n          if (stack[stack.length - 1] === 'INSIDE_OBJECT_AFTER_VALUE') {\n            processAfterObjectValue(char, i);\n          } else if (stack[stack.length - 1] === 'INSIDE_ARRAY_AFTER_VALUE') {\n            processAfterArrayValue(char, i);\n          }\n        } else {\n          lastValidIndex = i;\n        }\n\n        break;\n      }\n    }\n  }\n\n  let result = input.slice(0, lastValidIndex + 1);\n\n  for (let i = stack.length - 1; i >= 0; i--) {\n    const state = stack[i];\n\n    switch (state) {\n      case 'INSIDE_STRING': {\n        result += '\"';\n        break;\n      }\n\n      case 'INSIDE_OBJECT_KEY':\n      case 'INSIDE_OBJECT_AFTER_KEY':\n      case 'INSIDE_OBJECT_AFTER_COMMA':\n      case 'INSIDE_OBJECT_START':\n      case 'INSIDE_OBJECT_BEFORE_VALUE':\n      case 'INSIDE_OBJECT_AFTER_VALUE': {\n        result += '}';\n        break;\n      }\n\n      case 'INSIDE_ARRAY_START':\n      case 'INSIDE_ARRAY_AFTER_COMMA':\n      case 'INSIDE_ARRAY_AFTER_VALUE': {\n        result += ']';\n        break;\n      }\n\n      case 'INSIDE_LITERAL': {\n        const partialLiteral = input.substring(literalStart!, input.length);\n\n        if ('true'.startsWith(partialLiteral)) {\n          result += 'true'.slice(partialLiteral.length);\n        } else if ('false'.startsWith(partialLiteral)) {\n          result += 'false'.slice(partialLiteral.length);\n        } else if ('null'.startsWith(partialLiteral)) {\n          result += 'null'.slice(partialLiteral.length);\n        }\n      }\n    }\n  }\n\n  return result;\n}\n","import {\n  InferToolInput,\n  InferToolOutput,\n  Tool,\n  ToolCall,\n} from '@ai-sdk/provider-utils';\nimport { ToolSet } from '../generate-text';\nimport { ProviderMetadata } from '../types/provider-metadata';\nimport { DeepPartial } from '../util/deep-partial';\nimport { ValueOf } from '../util/value-of';\n\n/**\nThe data types that can be used in the UI message for the UI message data parts.\n */\nexport type UIDataTypes = Record<string, unknown>;\n\nexport type UITool = {\n  input: unknown;\n  output: unknown | undefined;\n};\n\n/**\n * Infer the input and output types of a tool so it can be used as a UI tool.\n */\nexport type InferUITool<TOOL extends Tool> = {\n  input: InferToolInput<TOOL>;\n  output: InferToolOutput<TOOL>;\n};\n\n/**\n * Infer the input and output types of a tool set so it can be used as a UI tool set.\n */\nexport type InferUITools<TOOLS extends ToolSet> = {\n  [NAME in keyof TOOLS & string]: InferUITool<TOOLS[NAME]>;\n};\n\nexport type UITools = Record<string, UITool>;\n\n/**\nAI SDK UI Messages. They are used in the client and to communicate between the frontend and the API routes.\n */\nexport interface UIMessage<\n  METADATA = unknown,\n  DATA_PARTS extends UIDataTypes = UIDataTypes,\n  TOOLS extends UITools = UITools,\n> {\n  /**\nA unique identifier for the message.\n   */\n  id: string;\n\n  /**\nThe role of the message.\n   */\n  role: 'system' | 'user' | 'assistant';\n\n  /**\nThe metadata of the message.\n   */\n  metadata?: METADATA;\n\n  /**\nThe parts of the message. Use this for rendering the message in the UI.\n\nSystem messages should be avoided (set the system prompt on the server instead).\nThey can have text parts.\n\nUser messages can have text parts and file parts.\n\nAssistant messages can have text, reasoning, tool invocation, and file parts.\n   */\n  parts: Array<UIMessagePart<DATA_PARTS, TOOLS>>;\n}\n\nexport type UIMessagePart<\n  DATA_TYPES extends UIDataTypes,\n  TOOLS extends UITools,\n> =\n  | TextUIPart\n  | ReasoningUIPart\n  | ToolUIPart<TOOLS>\n  | DynamicToolUIPart\n  | SourceUrlUIPart\n  | SourceDocumentUIPart\n  | FileUIPart\n  | DataUIPart<DATA_TYPES>\n  | StepStartUIPart;\n\n/**\n * A text part of a message.\n */\nexport type TextUIPart = {\n  type: 'text';\n\n  /**\n   * The text content.\n   */\n  text: string;\n\n  /**\n   * The state of the text part.\n   */\n  state?: 'streaming' | 'done';\n\n  /**\n   * The provider metadata.\n   */\n  providerMetadata?: ProviderMetadata;\n};\n\n/**\n * A reasoning part of a message.\n */\nexport type ReasoningUIPart = {\n  type: 'reasoning';\n\n  /**\n   * The reasoning text.\n   */\n  text: string;\n\n  /**\n   * The state of the reasoning part.\n   */\n  state?: 'streaming' | 'done';\n\n  /**\n   * The provider metadata.\n   */\n  providerMetadata?: ProviderMetadata;\n};\n\n/**\n * A source part of a message.\n */\nexport type SourceUrlUIPart = {\n  type: 'source-url';\n  sourceId: string;\n  url: string;\n  title?: string;\n  providerMetadata?: ProviderMetadata;\n};\n\n/**\n * A document source part of a message.\n */\nexport type SourceDocumentUIPart = {\n  type: 'source-document';\n  sourceId: string;\n  mediaType: string;\n  title: string;\n  filename?: string;\n  providerMetadata?: ProviderMetadata;\n};\n\n/**\n * A file part of a message.\n */\nexport type FileUIPart = {\n  type: 'file';\n\n  /**\n   * IANA media type of the file.\n   *\n   * @see https://www.iana.org/assignments/media-types/media-types.xhtml\n   */\n  mediaType: string;\n\n  /**\n   * Optional filename of the file.\n   */\n  filename?: string;\n\n  /**\n   * The URL of the file.\n   * It can either be a URL to a hosted file or a [Data URL](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs).\n   */\n  url: string;\n\n  /**\n   * The provider metadata.\n   */\n  providerMetadata?: ProviderMetadata;\n};\n\n/**\n * A step boundary part of a message.\n */\nexport type StepStartUIPart = {\n  type: 'step-start';\n};\n\nexport type DataUIPart<DATA_TYPES extends UIDataTypes> = ValueOf<{\n  [NAME in keyof DATA_TYPES & string]: {\n    type: `data-${NAME}`;\n    id?: string;\n    data: DATA_TYPES[NAME];\n  };\n}>;\n\ntype asUITool<TOOL extends UITool | Tool> = TOOL extends Tool\n  ? InferUITool<TOOL>\n  : TOOL;\n\n/**\n * Check if a message part is a data part.\n */\nexport function isDataUIPart<DATA_TYPES extends UIDataTypes>(\n  part: UIMessagePart<DATA_TYPES, UITools>,\n): part is DataUIPart<DATA_TYPES> {\n  return part.type.startsWith('data-');\n}\n\n/**\n * A UI tool invocation contains all the information needed to render a tool invocation in the UI.\n * It can be derived from a tool without knowing the tool name, and can be used to define\n * UI components for the tool.\n */\nexport type UIToolInvocation<TOOL extends UITool | Tool> = {\n  /**\n   * ID of the tool call.\n   */\n  toolCallId: string;\n\n  /**\n   * Whether the tool call was executed by the provider.\n   */\n  providerExecuted?: boolean;\n} & (\n  | {\n      state: 'input-streaming';\n      input: DeepPartial<asUITool<TOOL>['input']> | undefined;\n      providerExecuted?: boolean;\n      output?: never;\n      errorText?: never;\n    }\n  | {\n      state: 'input-available';\n      input: asUITool<TOOL>['input'];\n      providerExecuted?: boolean;\n      output?: never;\n      errorText?: never;\n      callProviderMetadata?: ProviderMetadata;\n    }\n  | {\n      state: 'output-available';\n      input: asUITool<TOOL>['input'];\n      output: asUITool<TOOL>['output'];\n      errorText?: never;\n      providerExecuted?: boolean;\n      callProviderMetadata?: ProviderMetadata;\n      preliminary?: boolean;\n    }\n  | {\n      state: 'output-error'; // TODO AI SDK 6: change to 'error' state\n      input: asUITool<TOOL>['input'] | undefined;\n      rawInput?: unknown; // TODO AI SDK 6: remove this field, input should be unknown\n      output?: never;\n      errorText: string;\n      providerExecuted?: boolean;\n      callProviderMetadata?: ProviderMetadata;\n    }\n);\n\nexport type ToolUIPart<TOOLS extends UITools = UITools> = ValueOf<{\n  [NAME in keyof TOOLS & string]: {\n    type: `tool-${NAME}`;\n  } & UIToolInvocation<TOOLS[NAME]>;\n}>;\n\nexport type DynamicToolUIPart = {\n  type: 'dynamic-tool';\n\n  /**\n   * Name of the tool that is being called.\n   */\n  toolName: string;\n\n  /**\n   * ID of the tool call.\n   */\n  toolCallId: string;\n  title?: string;\n\n  /**\n   * Whether the tool call was executed by the provider.\n   */\n  providerExecuted?: boolean;\n} & (\n  | {\n      state: 'input-streaming';\n      input: unknown | undefined;\n      output?: never;\n      errorText?: never;\n    }\n  | {\n      state: 'input-available';\n      input: unknown;\n      output?: never;\n      errorText?: never;\n      callProviderMetadata?: ProviderMetadata;\n    }\n  | {\n      state: 'output-available';\n      input: unknown;\n      output: unknown;\n      errorText?: never;\n      callProviderMetadata?: ProviderMetadata;\n      preliminary?: boolean;\n    }\n  | {\n      state: 'output-error'; // TODO AI SDK 6: change to 'error' state\n      input: unknown;\n      output?: never;\n      errorText: string;\n      callProviderMetadata?: ProviderMetadata;\n    }\n);\n\n/**\n * Type guard to check if a message part is a text part.\n */\nexport function isTextUIPart(\n  part: UIMessagePart<UIDataTypes, UITools>,\n): part is TextUIPart {\n  return part.type === 'text';\n}\n\n/**\n * Type guard to check if a message part is a file part.\n */\nexport function isFileUIPart(\n  part: UIMessagePart<UIDataTypes, UITools>,\n): part is FileUIPart {\n  return part.type === 'file';\n}\n\n/**\n * Type guard to check if a message part is a reasoning part.\n */\nexport function isReasoningUIPart(\n  part: UIMessagePart<UIDataTypes, UITools>,\n): part is ReasoningUIPart {\n  return part.type === 'reasoning';\n}\n\n// TODO AI SDK 6: rename to isStaticToolUIPart\nexport function isToolUIPart<TOOLS extends UITools>(\n  part: UIMessagePart<UIDataTypes, TOOLS>,\n): part is ToolUIPart<TOOLS> {\n  return part.type.startsWith('tool-');\n}\n\nexport function isDynamicToolUIPart(\n  part: UIMessagePart<UIDataTypes, UITools>,\n): part is DynamicToolUIPart {\n  return part.type === 'dynamic-tool';\n}\n\nexport function isToolOrDynamicToolUIPart<TOOLS extends UITools>(\n  part: UIMessagePart<UIDataTypes, TOOLS>,\n): part is ToolUIPart<TOOLS> | DynamicToolUIPart {\n  return isToolUIPart(part) || isDynamicToolUIPart(part);\n}\n\nexport function getToolName<TOOLS extends UITools>(\n  part: ToolUIPart<TOOLS>,\n): keyof TOOLS {\n  return part.type.split('-').slice(1).join('-') as keyof TOOLS;\n}\n\nexport function getToolOrDynamicToolName(\n  part: ToolUIPart<UITools> | DynamicToolUIPart,\n): string {\n  return isDynamicToolUIPart(part) ? part.toolName : getToolName(part);\n}\n\nexport type InferUIMessageMetadata<T extends UIMessage> =\n  T extends UIMessage<infer METADATA> ? METADATA : unknown;\n\nexport type InferUIMessageData<T extends UIMessage> =\n  T extends UIMessage<unknown, infer DATA_TYPES> ? DATA_TYPES : UIDataTypes;\n\nexport type InferUIMessageTools<T extends UIMessage> =\n  T extends UIMessage<unknown, UIDataTypes, infer TOOLS> ? TOOLS : UITools;\n\nexport type InferUIMessageToolOutputs<UI_MESSAGE extends UIMessage> =\n  InferUIMessageTools<UI_MESSAGE>[keyof InferUIMessageTools<UI_MESSAGE>]['output'];\n\nexport type InferUIMessageToolCall<UI_MESSAGE extends UIMessage> =\n  | ValueOf<{\n      [NAME in keyof InferUIMessageTools<UI_MESSAGE>]: ToolCall<\n        NAME & string,\n        InferUIMessageTools<UI_MESSAGE>[NAME] extends { input: infer INPUT }\n          ? INPUT\n          : never\n      > & { dynamic?: false };\n    }>\n  | (ToolCall<string, unknown> & { dynamic: true });\n\nexport type InferUIMessagePart<UI_MESSAGE extends UIMessage> = UIMessagePart<\n  InferUIMessageData<UI_MESSAGE>,\n  InferUIMessageTools<UI_MESSAGE>\n>;\n","import {\n  createStreamingUIMessageState,\n  processUIMessageStream,\n  StreamingUIMessageState,\n} from '../ui/process-ui-message-stream';\nimport { UIMessage } from '../ui/ui-messages';\nimport { ErrorHandler } from '../util/error-handler';\nimport { InferUIMessageChunk, UIMessageChunk } from './ui-message-chunks';\nimport { UIMessageStreamOnFinishCallback } from './ui-message-stream-on-finish-callback';\n\nexport function handleUIMessageStreamFinish<UI_MESSAGE extends UIMessage>({\n  messageId,\n  originalMessages = [],\n  onFinish,\n  onError,\n  stream,\n}: {\n  stream: ReadableStream<InferUIMessageChunk<UI_MESSAGE>>;\n\n  /**\n   * The message ID to use for the response message.\n   * If not provided, no id will be set for the response message.\n   */\n  messageId?: string;\n\n  /**\n   * The original messages.\n   */\n  originalMessages?: UI_MESSAGE[];\n\n  onError: ErrorHandler;\n\n  onFinish?: UIMessageStreamOnFinishCallback<UI_MESSAGE>;\n}): ReadableStream<InferUIMessageChunk<UI_MESSAGE>> {\n  // last message is only relevant for assistant messages\n  let lastMessage: UI_MESSAGE | undefined =\n    originalMessages?.[originalMessages.length - 1];\n  if (lastMessage?.role !== 'assistant') {\n    lastMessage = undefined;\n  } else {\n    // appending to the last message, so we need to use the same id\n    messageId = lastMessage.id;\n  }\n\n  let isAborted = false;\n\n  const idInjectedStream = stream.pipeThrough(\n    new TransformStream<\n      InferUIMessageChunk<UI_MESSAGE>,\n      InferUIMessageChunk<UI_MESSAGE>\n    >({\n      transform(chunk, controller) {\n        // when there is no messageId in the start chunk,\n        // but the user checked for persistence,\n        // inject the messageId into the chunk\n        if (chunk.type === 'start') {\n          const startChunk = chunk as UIMessageChunk & { type: 'start' };\n          if (startChunk.messageId == null && messageId != null) {\n            startChunk.messageId = messageId;\n          }\n        }\n\n        if (chunk.type === 'abort') {\n          isAborted = true;\n        }\n\n        controller.enqueue(chunk);\n      },\n    }),\n  );\n\n  if (onFinish == null) {\n    return idInjectedStream;\n  }\n\n  const state = createStreamingUIMessageState<UI_MESSAGE>({\n    lastMessage: lastMessage\n      ? (structuredClone(lastMessage) as UI_MESSAGE)\n      : undefined,\n    messageId: messageId ?? '', // will be overridden by the stream\n  });\n\n  const runUpdateMessageJob = async (\n    job: (options: {\n      state: StreamingUIMessageState<UI_MESSAGE>;\n      write: () => void;\n    }) => Promise<void>,\n  ) => {\n    await job({ state, write: () => {} });\n  };\n\n  let finishCalled = false;\n\n  const callOnFinish = async () => {\n    if (finishCalled || !onFinish) {\n      return;\n    }\n    finishCalled = true;\n\n    const isContinuation = state.message.id === lastMessage?.id;\n    await onFinish({\n      isAborted,\n      isContinuation,\n      responseMessage: state.message as UI_MESSAGE,\n      messages: [\n        ...(isContinuation ? originalMessages.slice(0, -1) : originalMessages),\n        state.message,\n      ] as UI_MESSAGE[],\n      finishReason: state.finishReason,\n    });\n  };\n\n  return processUIMessageStream<UI_MESSAGE>({\n    stream: idInjectedStream,\n    runUpdateMessageJob,\n    onError,\n  }).pipeThrough(\n    new TransformStream<\n      InferUIMessageChunk<UI_MESSAGE>,\n      InferUIMessageChunk<UI_MESSAGE>\n    >({\n      transform(chunk, controller) {\n        controller.enqueue(chunk);\n      },\n      // @ts-expect-error cancel is still new and missing from types https://developer.mozilla.org/en-US/docs/Web/API/TransformStream#browser_compatibility\n      async cancel() {\n        await callOnFinish();\n      },\n\n      async flush() {\n        await callOnFinish();\n      },\n    }),\n  );\n}\n","import { ServerResponse } from 'node:http';\nimport { prepareHeaders } from '../util/prepare-headers';\nimport { writeToServerResponse } from '../util/write-to-server-response';\nimport { JsonToSseTransformStream } from './json-to-sse-transform-stream';\nimport { UI_MESSAGE_STREAM_HEADERS } from './ui-message-stream-headers';\nimport { UIMessageChunk } from './ui-message-chunks';\nimport { UIMessageStreamResponseInit } from './ui-message-stream-response-init';\n\nexport function pipeUIMessageStreamToResponse({\n  response,\n  status,\n  statusText,\n  headers,\n  stream,\n  consumeSseStream,\n}: {\n  response: ServerResponse;\n  stream: ReadableStream<UIMessageChunk>;\n} & UIMessageStreamResponseInit): void {\n  let sseStream = stream.pipeThrough(new JsonToSseTransformStream());\n\n  // when the consumeSseStream is provided, we need to tee the stream\n  // and send the second part to the consumeSseStream function\n  // so that it can be consumed by the client independently\n  if (consumeSseStream) {\n    const [stream1, stream2] = sseStream.tee();\n    sseStream = stream1;\n    consumeSseStream({ stream: stream2 }); // no await (do not block the response)\n  }\n\n  writeToServerResponse({\n    response,\n    status,\n    statusText,\n    headers: Object.fromEntries(\n      prepareHeaders(headers, UI_MESSAGE_STREAM_HEADERS).entries(),\n    ),\n    stream: sseStream.pipeThrough(new TextEncoderStream()),\n  });\n}\n","/**\n * A type that combines AsyncIterable and ReadableStream.\n * This allows a ReadableStream to be consumed using for-await-of syntax.\n */\nexport type AsyncIterableStream<T> = AsyncIterable<T> & ReadableStream<T>;\n\n/**\n * Wraps a ReadableStream and returns an object that is both a ReadableStream and an AsyncIterable.\n * This enables consumption of the stream using for-await-of, with proper resource cleanup on early exit or error.\n *\n * @template T The type of the stream's chunks.\n * @param source The source ReadableStream to wrap.\n * @returns An AsyncIterableStream that can be used as both a ReadableStream and an AsyncIterable.\n */\nexport function createAsyncIterableStream<T>(\n  source: ReadableStream<T>,\n): AsyncIterableStream<T> {\n  // Pipe through a TransformStream to ensure a fresh, unlocked stream.\n  const stream = source.pipeThrough(new TransformStream<T, T>());\n\n  /**\n   * Implements the async iterator protocol for the stream.\n   * Ensures proper cleanup (cancelling and releasing the reader) on completion, early exit, or error.\n   */\n  (stream as AsyncIterableStream<T>)[Symbol.asyncIterator] = function (\n    this: ReadableStream<T>,\n  ): AsyncIterator<T> {\n    const reader = this.getReader();\n\n    let finished = false;\n\n    /**\n     * Cleans up the reader by cancelling and releasing the lock.\n     */\n    async function cleanup(cancelStream: boolean) {\n      finished = true;\n      try {\n        if (cancelStream) {\n          await reader.cancel?.();\n        }\n      } finally {\n        try {\n          reader.releaseLock();\n        } catch {}\n      }\n    }\n\n    return {\n      /**\n       * Reads the next chunk from the stream.\n       * @returns A promise resolving to the next IteratorResult.\n       */\n      async next(): Promise<IteratorResult<T>> {\n        if (finished) {\n          return { done: true, value: undefined };\n        }\n\n        const { done, value } = await reader.read();\n\n        if (done) {\n          await cleanup(true);\n          return { done: true, value: undefined };\n        }\n\n        return { done: false, value };\n      },\n\n      /**\n       * Called on early exit (e.g., break from for-await).\n       * Ensures the stream is cancelled and resources are released.\n       * @returns A promise resolving to a completed IteratorResult.\n       */\n      async return(): Promise<IteratorResult<T>> {\n        await cleanup(true);\n        return { done: true, value: undefined };\n      },\n\n      /**\n       * Called on early exit with error.\n       * Ensures the stream is cancelled and resources are released, then rethrows the error.\n       * @param err The error to throw.\n       * @returns A promise that rejects with the provided error.\n       */\n      async throw(err: unknown): Promise<IteratorResult<T>> {\n        await cleanup(true);\n        throw err;\n      },\n    };\n  };\n\n  return stream as AsyncIterableStream<T>;\n}\n","/**\n * Consumes a ReadableStream until it's fully read.\n *\n * This function reads the stream chunk by chunk until the stream is exhausted.\n * It doesn't process or return the data from the stream; it simply ensures\n * that the entire stream is read.\n *\n * @param {ReadableStream} stream - The ReadableStream to be consumed.\n * @returns {Promise<void>} A promise that resolves when the stream is fully consumed.\n */\nexport async function consumeStream({\n  stream,\n  onError,\n}: {\n  stream: ReadableStream;\n  onError?: (error: unknown) => void;\n}): Promise<void> {\n  const reader = stream.getReader();\n  try {\n    while (true) {\n      const { done } = await reader.read();\n      if (done) break;\n    }\n  } catch (error) {\n    onError?.(error);\n  } finally {\n    reader.releaseLock();\n  }\n}\n","import { ErrorHandler } from './error-handler';\n\n/**\n * Creates a Promise with externally accessible resolve and reject functions.\n *\n * @template T - The type of the value that the Promise will resolve to.\n * @returns An object containing:\n *   - promise: A Promise that can be resolved or rejected externally.\n *   - resolve: A function to resolve the Promise with a value of type T.\n *   - reject: A function to reject the Promise with an error.\n */\nexport function createResolvablePromise<T = any>(): {\n  promise: Promise<T>;\n  resolve: (value: T) => void;\n  reject: ErrorHandler;\n} {\n  let resolve: (value: T) => void;\n  let reject: ErrorHandler;\n\n  const promise = new Promise<T>((res, rej) => {\n    resolve = res;\n    reject = rej;\n  });\n\n  return {\n    promise,\n    resolve: resolve!,\n    reject: reject!,\n  };\n}\n","import { createResolvablePromise } from './create-resolvable-promise';\n\n/**\n * Creates a stitchable stream that can pipe one stream at a time.\n *\n * @template T - The type of values emitted by the streams.\n * @returns {Object} An object containing the stitchable stream and control methods.\n */\nexport function createStitchableStream<T>(): {\n  stream: ReadableStream<T>;\n  addStream: (innerStream: ReadableStream<T>) => void;\n  close: () => void;\n  terminate: () => void;\n} {\n  let innerStreamReaders: ReadableStreamDefaultReader<T>[] = [];\n  let controller: ReadableStreamDefaultController<T> | null = null;\n  let isClosed = false;\n  let waitForNewStream = createResolvablePromise<void>();\n\n  const terminate = () => {\n    isClosed = true;\n    waitForNewStream.resolve();\n\n    innerStreamReaders.forEach(reader => reader.cancel());\n    innerStreamReaders = [];\n    controller?.close();\n  };\n\n  const processPull = async () => {\n    // Case 1: Outer stream is closed and no more inner streams\n    if (isClosed && innerStreamReaders.length === 0) {\n      controller?.close();\n      return;\n    }\n\n    // Case 2: No inner streams available, but outer stream is open\n    // wait for a new inner stream to be added or the outer stream to close\n    if (innerStreamReaders.length === 0) {\n      waitForNewStream = createResolvablePromise<void>();\n      await waitForNewStream.promise;\n      return processPull();\n    }\n\n    try {\n      const { value, done } = await innerStreamReaders[0].read();\n\n      if (done) {\n        // Case 3: Current inner stream is done\n        innerStreamReaders.shift(); // Remove the finished stream\n\n        // Continue pulling from the next stream if available\n        if (innerStreamReaders.length > 0) {\n          await processPull();\n        } else if (isClosed) {\n          controller?.close();\n        }\n      } else {\n        // Case 4: Current inner stream returns an item\n        controller?.enqueue(value);\n      }\n    } catch (error) {\n      // Case 5: Current inner stream throws an error\n      controller?.error(error);\n      innerStreamReaders.shift(); // Remove the errored stream\n      terminate(); // we have errored, terminate all streams\n    }\n  };\n\n  return {\n    stream: new ReadableStream<T>({\n      start(controllerParam) {\n        controller = controllerParam;\n      },\n      pull: processPull,\n      async cancel() {\n        for (const reader of innerStreamReaders) {\n          await reader.cancel();\n        }\n        innerStreamReaders = [];\n        isClosed = true;\n      },\n    }),\n    addStream: (innerStream: ReadableStream<T>) => {\n      if (isClosed) {\n        throw new Error('Cannot add inner stream: outer stream is closed');\n      }\n\n      innerStreamReaders.push(innerStream.getReader());\n      waitForNewStream.resolve();\n    },\n\n    /**\n     * Gracefully close the outer stream. This will let the inner streams\n     * finish processing and then close the outer stream.\n     */\n    close: () => {\n      isClosed = true;\n      waitForNewStream.resolve();\n\n      if (innerStreamReaders.length === 0) {\n        controller?.close();\n      }\n    },\n\n    /**\n     * Immediately close the outer stream. This will cancel all inner streams\n     * and close the outer stream.\n     */\n    terminate,\n  };\n}\n","// Shim for performance.now() to support environments that don't have it:\nexport function now(): number {\n  return globalThis?.performance?.now() ?? Date.now();\n}\n","import {\n  LanguageModelV2CallWarning,\n  LanguageModelV2StreamPart,\n} from '@ai-sdk/provider';\nimport {\n  executeTool,\n  generateId,\n  getErrorMessage,\n  ModelMessage,\n} from '@ai-sdk/provider-utils';\nimport { Tracer } from '@opentelemetry/api';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { recordErrorOnSpan, recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { FinishReason, LanguageModelUsage, ProviderMetadata } from '../types';\nimport { Source } from '../types/language-model';\nimport { DefaultGeneratedFileWithType, GeneratedFile } from './generated-file';\nimport { parseToolCall } from './parse-tool-call';\nimport { TypedToolCall } from './tool-call';\nimport { ToolCallRepairFunction } from './tool-call-repair-function';\nimport { TypedToolError } from './tool-error';\nimport { TypedToolResult } from './tool-result';\nimport { ToolSet } from './tool-set';\n\nexport type SingleRequestTextStreamPart<TOOLS extends ToolSet> =\n  // Text blocks:\n  | {\n      type: 'text-start';\n      providerMetadata?: ProviderMetadata;\n      id: string;\n    }\n  | {\n      type: 'text-delta';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n      delta: string;\n    }\n  | {\n      type: 'text-end';\n      providerMetadata?: ProviderMetadata;\n      id: string;\n    }\n\n  // Reasoning blocks:\n  | {\n      type: 'reasoning-start';\n      providerMetadata?: ProviderMetadata;\n      id: string;\n    }\n  | {\n      type: 'reasoning-delta';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n      delta: string;\n    }\n  | {\n      type: 'reasoning-end';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n    }\n\n  // Tool calls:\n  | {\n      type: 'tool-input-start';\n      id: string;\n      toolName: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'tool-input-delta';\n      id: string;\n      delta: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'tool-input-end';\n      id: string;\n      providerMetadata?: ProviderMetadata;\n    }\n  | ({ type: 'source' } & Source)\n  | { type: 'file'; file: GeneratedFile } // different because of GeneratedFile object\n  | ({ type: 'tool-call' } & TypedToolCall<TOOLS>)\n  | ({ type: 'tool-result' } & TypedToolResult<TOOLS>)\n  | ({ type: 'tool-error' } & TypedToolError<TOOLS>)\n  | { type: 'file'; file: GeneratedFile } // different because of GeneratedFile object\n  | { type: 'stream-start'; warnings: LanguageModelV2CallWarning[] }\n  | {\n      type: 'response-metadata';\n      id?: string;\n      timestamp?: Date;\n      modelId?: string;\n    }\n  | {\n      type: 'finish';\n      finishReason: FinishReason;\n      usage: LanguageModelUsage;\n      providerMetadata?: ProviderMetadata;\n    }\n  | { type: 'error'; error: unknown }\n  | { type: 'raw'; rawValue: unknown };\n\nexport function runToolsTransformation<TOOLS extends ToolSet>({\n  tools,\n  generatorStream,\n  tracer,\n  telemetry,\n  system,\n  messages,\n  abortSignal,\n  repairToolCall,\n  experimental_context,\n}: {\n  tools: TOOLS | undefined;\n  generatorStream: ReadableStream<LanguageModelV2StreamPart>;\n  tracer: Tracer;\n  telemetry: TelemetrySettings | undefined;\n  system: string | undefined;\n  messages: ModelMessage[];\n  abortSignal: AbortSignal | undefined;\n  repairToolCall: ToolCallRepairFunction<TOOLS> | undefined;\n  experimental_context: unknown;\n}): ReadableStream<SingleRequestTextStreamPart<TOOLS>> {\n  // tool results stream\n  let toolResultsStreamController: ReadableStreamDefaultController<\n    SingleRequestTextStreamPart<TOOLS>\n  > | null = null;\n  const toolResultsStream = new ReadableStream<\n    SingleRequestTextStreamPart<TOOLS>\n  >({\n    start(controller) {\n      toolResultsStreamController = controller;\n    },\n  });\n\n  // keep track of outstanding tool results for stream closing:\n  const outstandingToolResults = new Set<string>();\n\n  // keep track of tool inputs for provider-side tool results\n  const toolInputs = new Map<string, unknown>();\n\n  let canClose = false;\n  let finishChunk:\n    | (SingleRequestTextStreamPart<TOOLS> & { type: 'finish' })\n    | undefined = undefined;\n\n  function attemptClose() {\n    // close the tool results controller if no more outstanding tool calls\n    if (canClose && outstandingToolResults.size === 0) {\n      // we delay sending the finish chunk until all tool results (incl. delayed ones)\n      // are received to ensure that the frontend receives tool results before a message\n      // finish event arrives.\n      if (finishChunk != null) {\n        toolResultsStreamController!.enqueue(finishChunk);\n      }\n\n      toolResultsStreamController!.close();\n    }\n  }\n\n  // forward stream\n  const forwardStream = new TransformStream<\n    LanguageModelV2StreamPart,\n    SingleRequestTextStreamPart<TOOLS>\n  >({\n    async transform(\n      chunk: LanguageModelV2StreamPart,\n      controller: TransformStreamDefaultController<\n        SingleRequestTextStreamPart<TOOLS>\n      >,\n    ) {\n      const chunkType = chunk.type;\n\n      switch (chunkType) {\n        // forward:\n        case 'stream-start':\n        case 'text-start':\n        case 'text-delta':\n        case 'text-end':\n        case 'reasoning-start':\n        case 'reasoning-delta':\n        case 'reasoning-end':\n        case 'tool-input-start':\n        case 'tool-input-delta':\n        case 'tool-input-end':\n        case 'source':\n        case 'response-metadata':\n        case 'error':\n        case 'raw': {\n          controller.enqueue(chunk);\n          break;\n        }\n\n        case 'file': {\n          controller.enqueue({\n            type: 'file',\n            file: new DefaultGeneratedFileWithType({\n              data: chunk.data,\n              mediaType: chunk.mediaType,\n            }),\n          });\n          break;\n        }\n\n        case 'finish': {\n          finishChunk = {\n            type: 'finish',\n            finishReason: chunk.finishReason,\n            usage: chunk.usage,\n            providerMetadata: chunk.providerMetadata,\n          };\n          break;\n        }\n\n        // process tool call:\n        case 'tool-call': {\n          try {\n            const toolCall = await parseToolCall({\n              toolCall: chunk,\n              tools,\n              repairToolCall,\n              system,\n              messages,\n            });\n\n            controller.enqueue(toolCall);\n\n            // handle invalid tool calls:\n            if (toolCall.invalid) {\n              toolResultsStreamController!.enqueue({\n                type: 'tool-error',\n                toolCallId: toolCall.toolCallId,\n                toolName: toolCall.toolName,\n                input: toolCall.input,\n                error: getErrorMessage(toolCall.error!),\n                dynamic: true,\n              });\n\n              break;\n            }\n\n            const tool = tools![toolCall.toolName];\n\n            toolInputs.set(toolCall.toolCallId, toolCall.input);\n\n            if (tool.onInputAvailable != null) {\n              await tool.onInputAvailable({\n                input: toolCall.input,\n                toolCallId: toolCall.toolCallId,\n                messages,\n                abortSignal,\n                experimental_context,\n              });\n            }\n\n            // Only execute tools that are not provider-executed:\n            if (tool.execute != null && toolCall.providerExecuted !== true) {\n              const toolExecutionId = generateId(); // use our own id to guarantee uniqueness\n              outstandingToolResults.add(toolExecutionId);\n\n              // Note: we don't await the tool execution here (by leaving out 'await' on recordSpan),\n              // because we want to process the next chunk as soon as possible.\n              // This is important for the case where the tool execution takes a long time.\n              recordSpan({\n                name: 'ai.toolCall',\n                attributes: selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    ...assembleOperationName({\n                      operationId: 'ai.toolCall',\n                      telemetry,\n                    }),\n                    'ai.toolCall.name': toolCall.toolName,\n                    'ai.toolCall.id': toolCall.toolCallId,\n                    'ai.toolCall.args': {\n                      output: () => JSON.stringify(toolCall.input),\n                    },\n                  },\n                }),\n                tracer,\n                fn: async span => {\n                  let output: unknown;\n\n                  try {\n                    const stream = executeTool({\n                      execute: tool.execute!.bind(tool),\n                      input: toolCall.input,\n                      options: {\n                        toolCallId: toolCall.toolCallId,\n                        messages,\n                        abortSignal,\n                        experimental_context,\n                      },\n                    });\n\n                    for await (const part of stream) {\n                      toolResultsStreamController!.enqueue({\n                        ...toolCall,\n                        type: 'tool-result',\n                        output: part.output,\n                        ...(part.type === 'preliminary' && {\n                          preliminary: true,\n                        }),\n                      });\n\n                      if (part.type === 'final') {\n                        output = part.output;\n                      }\n                    }\n                  } catch (error) {\n                    recordErrorOnSpan(span, error);\n                    toolResultsStreamController!.enqueue({\n                      ...toolCall,\n                      type: 'tool-error',\n                      error,\n                    } satisfies TypedToolError<TOOLS>);\n\n                    outstandingToolResults.delete(toolExecutionId);\n                    attemptClose();\n                    return;\n                  }\n\n                  outstandingToolResults.delete(toolExecutionId);\n                  attemptClose();\n\n                  // record telemetry\n                  try {\n                    span.setAttributes(\n                      selectTelemetryAttributes({\n                        telemetry,\n                        attributes: {\n                          'ai.toolCall.result': {\n                            output: () => JSON.stringify(output),\n                          },\n                        },\n                      }),\n                    );\n                  } catch (ignored) {\n                    // JSON stringify might fail if the result is not serializable,\n                    // in which case we just ignore it. In the future we might want to\n                    // add an optional serialize method to the tool interface and warn\n                    // if the result is not serializable.\n                  }\n                },\n              });\n            }\n          } catch (error) {\n            toolResultsStreamController!.enqueue({ type: 'error', error });\n          }\n\n          break;\n        }\n\n        case 'tool-result': {\n          const toolName = chunk.toolName as keyof TOOLS & string;\n\n          if (chunk.isError) {\n            toolResultsStreamController!.enqueue({\n              type: 'tool-error',\n              toolCallId: chunk.toolCallId,\n              toolName,\n              input: toolInputs.get(chunk.toolCallId),\n              providerExecuted: chunk.providerExecuted,\n              error: chunk.result,\n            } as TypedToolError<TOOLS>);\n          } else {\n            controller.enqueue({\n              type: 'tool-result',\n              toolCallId: chunk.toolCallId,\n              toolName,\n              input: toolInputs.get(chunk.toolCallId),\n              output: chunk.result,\n              providerExecuted: chunk.providerExecuted,\n            } as TypedToolResult<TOOLS>);\n          }\n          break;\n        }\n\n        default: {\n          const _exhaustiveCheck: never = chunkType;\n          throw new Error(`Unhandled chunk type: ${_exhaustiveCheck}`);\n        }\n      }\n    },\n\n    flush() {\n      canClose = true;\n      attemptClose();\n    },\n  });\n\n  // combine the generator stream and the tool results stream\n  return new ReadableStream<SingleRequestTextStreamPart<TOOLS>>({\n    async start(controller) {\n      // need to wait for both pipes so there are no dangling promises that\n      // can cause uncaught promise rejections when the stream is aborted\n      return Promise.all([\n        generatorStream.pipeThrough(forwardStream).pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n              // the generator stream controller is automatically closed when it's consumed\n            },\n          }),\n        ),\n        toolResultsStream.pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n              controller.close();\n            },\n          }),\n        ),\n      ]);\n    },\n  });\n}\n","import {\n  AssistantContent,\n  FilePart,\n  ModelMessage,\n  TextPart,\n  ToolResultPart,\n} from '@ai-sdk/provider-utils';\nimport { ToolSet } from '../generate-text/tool-set';\nimport { createToolModelOutput } from '../prompt/create-tool-model-output';\nimport { MessageConversionError } from '../prompt/message-conversion-error';\nimport {\n  DataUIPart,\n  DynamicToolUIPart,\n  FileUIPart,\n  getToolName,\n  getToolOrDynamicToolName,\n  InferUIMessageData,\n  InferUIMessageTools,\n  isDataUIPart,\n  isDynamicToolUIPart,\n  isFileUIPart,\n  isReasoningUIPart,\n  isTextUIPart,\n  isToolOrDynamicToolUIPart,\n  isToolUIPart,\n  ReasoningUIPart,\n  TextUIPart,\n  ToolUIPart,\n  UIMessage,\n} from './ui-messages';\n\n/**\nConverts an array of UI messages from useChat into an array of ModelMessages that can be used\nwith the AI functions (e.g. `streamText`, `generateText`).\n\n@param messages - The UI messages to convert.\n@param options.tools - The tools to use.\n@param options.ignoreIncompleteToolCalls - Whether to ignore incomplete tool calls. Default is `false`.\n@param options.convertDataPart - Optional function to convert data parts to text or file model message parts. Returns `undefined` if the part should be ignored.\n\n@returns An array of ModelMessages.\n */\nexport function convertToModelMessages<UI_MESSAGE extends UIMessage>(\n  messages: Array<Omit<UI_MESSAGE, 'id'>>,\n  options?: {\n    tools?: ToolSet;\n    ignoreIncompleteToolCalls?: boolean;\n    convertDataPart?: (\n      part: DataUIPart<InferUIMessageData<UI_MESSAGE>>,\n    ) => TextPart | FilePart | undefined;\n  },\n): ModelMessage[] {\n  const modelMessages: ModelMessage[] = [];\n\n  if (options?.ignoreIncompleteToolCalls) {\n    messages = messages.map(message => ({\n      ...message,\n      parts: message.parts.filter(\n        part =>\n          !isToolOrDynamicToolUIPart(part) ||\n          (part.state !== 'input-streaming' &&\n            part.state !== 'input-available'),\n      ),\n    }));\n  }\n\n  for (const message of messages) {\n    switch (message.role) {\n      case 'system': {\n        const textParts = message.parts.filter(\n          (part): part is TextUIPart => part.type === 'text',\n        );\n\n        const providerMetadata = textParts.reduce((acc, part) => {\n          if (part.providerMetadata != null) {\n            return { ...acc, ...part.providerMetadata };\n          }\n          return acc;\n        }, {});\n\n        modelMessages.push({\n          role: 'system',\n          content: textParts.map(part => part.text).join(''),\n          ...(Object.keys(providerMetadata).length > 0\n            ? { providerOptions: providerMetadata }\n            : {}),\n        });\n        break;\n      }\n\n      case 'user': {\n        modelMessages.push({\n          role: 'user',\n          content: message.parts\n            .map((part): TextPart | FilePart | undefined => {\n              // Process text parts\n              if (isTextUIPart(part)) {\n                return {\n                  type: 't